{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8a68514",
   "metadata": {},
   "source": [
    "# DECODE-EV: Sistema RAG para Datos Vehiculares CAN\n",
    "## IBM Watson Studio - Proyecto Integrador Grupo 7\n",
    "\n",
    "### Descripci√≥n del Proyecto\n",
    "Este notebook implementa un sistema completo de **Retrieval-Augmented Generation (RAG)** para el an√°lisis de datos vehiculares CAN del proyecto DECODE-EV. El sistema transforma datos num√©ricos de redes CAN en representaciones textuales enriquecidas para sistemas de IA conversacional.\n",
    "\n",
    "### Objetivos Principales\n",
    "1. **Transformaci√≥n Sem√°ntica**: Convertir datos CAN num√©ricos en descripciones textuales t√©cnicamente precisas\n",
    "2. **Enriquecimiento Contextual**: Generar metadatos estructurados para facilitar recuperaci√≥n RAG\n",
    "3. **Construcci√≥n de Corpus**: Crear dataset unificado en formato JSONL optimizado para LLM\n",
    "4. **An√°lisis de Calidad**: Validar completitud y coherencia del dataset generado\n",
    "\n",
    "### Arquitectura del Sistema\n",
    "- **Motor de Transformaci√≥n**: Convierte series temporales CAN ‚Üí Texto descriptivo\n",
    "- **Generador de Metadatos**: Clasifica eventos y determina contexto operacional\n",
    "- **Procesador Documental**: Chunking inteligente de documentaci√≥n t√©cnica\n",
    "- **Constructor RAG**: Unifica todas las fuentes en dataset optimizado\n",
    "\n",
    "---\n",
    "\n",
    "**Compatibilidad**: IBM Watson Studio, Cloud Pak for Data  \n",
    "**Lenguaje**: Python 3.8+  \n",
    "**Dependencias**: pandas, numpy, matplotlib, langchain, jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a4a8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# SECCI√ìN 1: CONFIGURACI√ìN DEL ENTORNO IBM WATSON STUDIO\n",
    "# ===================================================================\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "from typing import List, Dict, Any, Optional, Union, Tuple\n",
    "import warnings\n",
    "\n",
    "def configurar_entorno_ibm():\n",
    "    \"\"\"\n",
    "    Configura el entorno espec√≠ficamente para IBM Watson Studio\n",
    "    \"\"\"\n",
    "    print(\"üè¢ Configurando entorno IBM Watson Studio...\")\n",
    "    \n",
    "    # Detectar si estamos en IBM Cloud\n",
    "    is_ibm_cloud = (\n",
    "        os.environ.get('PROJECT_ID') is not None or \n",
    "        os.environ.get('DSX_PROJECT_DIR') is not None or\n",
    "        os.environ.get('WATSON_STUDIO') is not None\n",
    "    )\n",
    "    \n",
    "    if is_ibm_cloud:\n",
    "        print(\"‚úÖ Detectado entorno IBM Cloud - Aplicando configuraciones espec√≠ficas\")\n",
    "        # Configuraciones espec√≠ficas para IBM Watson Studio\n",
    "        os.environ['PYTHONPATH'] = os.environ.get('PYTHONPATH', '') + ':/home/dsxuser/work'\n",
    "        os.environ['JUPYTER_CONFIG_DIR'] = '/home/dsxuser/.jupyter'\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Entorno local detectado - Usando configuraci√≥n est√°ndar\")\n",
    "    \n",
    "    return is_ibm_cloud\n",
    "\n",
    "def instalar_paquete_ibm(paquete: str, silencioso: bool = True) -> bool:\n",
    "    \"\"\"\n",
    "    Instalaci√≥n de paquetes optimizada para IBM Watson Studio\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cmd_args = [sys.executable, \"-m\", \"pip\", \"install\", paquete]\n",
    "        if silencioso:\n",
    "            cmd_args.extend([\"--quiet\", \"--no-warn-script-location\"])\n",
    "        \n",
    "        result = subprocess.run(cmd_args, capture_output=True, text=True, timeout=300)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(f\"‚úÖ {paquete} instalado correctamente\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"‚ùå Error instalando {paquete}: {result.stderr}\")\n",
    "            return False\n",
    "            \n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(f\"‚è±Ô∏è Timeout instalando {paquete}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error inesperado instalando {paquete}: {e}\")\n",
    "        return False\n",
    "\n",
    "# Configurar entorno\n",
    "es_entorno_ibm = configurar_entorno_ibm()\n",
    "\n",
    "# Dependencias optimizadas para IBM Watson Studio\n",
    "dependencias_ibm = [\n",
    "    \"pandas>=1.5.0\",\n",
    "    \"numpy>=1.21.0\", \n",
    "    \"matplotlib>=3.5.0\",\n",
    "    \"seaborn>=0.11.0\",\n",
    "    \"plotly>=5.0.0\",\n",
    "    \"jsonlines\",\n",
    "    \"langchain>=0.1.0\",\n",
    "    \"langchain-community\"\n",
    "]\n",
    "\n",
    "print(\"\\nüîß Instalando dependencias para IBM Watson Studio...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "exitos = 0\n",
    "fallos = []\n",
    "\n",
    "for paquete in dependencias_ibm:\n",
    "    if instalar_paquete_ibm(paquete):\n",
    "        exitos += 1\n",
    "    else:\n",
    "        fallos.append(paquete)\n",
    "\n",
    "print(f\"\\nüìä Instalaci√≥n completada:\")\n",
    "print(f\"   ‚úÖ Exitosas: {exitos}/{len(dependencias_ibm)}\")\n",
    "print(f\"   ‚ùå Fallidas: {len(fallos)}\")\n",
    "\n",
    "if fallos:\n",
    "    print(f\"\\n‚ö†Ô∏è  Paquetes que requieren instalaci√≥n manual:\")\n",
    "    for paquete in fallos:\n",
    "        print(f\"   !pip install {paquete}\")\n",
    "\n",
    "if exitos >= len(dependencias_ibm) * 0.8:  # 80% de √©xito m√≠nimo\n",
    "    print(\"\\nüöÄ Entorno IBM Watson Studio configurado correctamente\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Configuraci√≥n parcial - Algunas funcionalidades pueden estar limitadas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d82408e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# SECCI√ìN 2: IMPORTACI√ìN Y VERIFICACI√ìN DE DEPENDENCIAS\n",
    "# ===================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import re\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "\n",
    "# Configuraci√≥n de warnings y logging para IBM\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler(sys.stdout)]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configuraci√≥n optimizada para IBM Watson Studio\n",
    "pd.set_option('display.max_columns', 15)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "# Configuraci√≥n de matplotlib\n",
    "try:\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "except:\n",
    "    try:\n",
    "        plt.style.use('seaborn-whitegrid')\n",
    "    except:\n",
    "        plt.style.use('default')\n",
    "\n",
    "# Paleta de colores optimizada para IBM\n",
    "colores_ibm = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n",
    "sns.set_palette(colores_ibm)\n",
    "\n",
    "# Importaci√≥n segura de jsonlines\n",
    "def importar_jsonlines_seguro():\n",
    "    try:\n",
    "        import jsonlines\n",
    "        return jsonlines\n",
    "    except ImportError:\n",
    "        logger.warning(\"jsonlines no disponible - usando fallback JSON\")\n",
    "        return None\n",
    "\n",
    "# Importaci√≥n segura de LangChain\n",
    "def importar_langchain_seguro():\n",
    "    try:\n",
    "        from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "        from langchain_core.documents import Document\n",
    "        return {'text_splitter': RecursiveCharacterTextSplitter, 'document': Document}\n",
    "    except ImportError:\n",
    "        try:\n",
    "            from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "            from langchain.docstore.document import Document\n",
    "            return {'text_splitter': RecursiveCharacterTextSplitter, 'document': Document}\n",
    "        except ImportError:\n",
    "            logger.warning(\"LangChain no disponible - usando implementaci√≥n alternativa\")\n",
    "            return None\n",
    "\n",
    "# Ejecutar importaciones\n",
    "jsonlines = importar_jsonlines_seguro()\n",
    "langchain_components = importar_langchain_seguro()\n",
    "\n",
    "# Verificaci√≥n final del entorno\n",
    "def verificar_entorno_completo():\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üöÄ DECODE-EV: ENTORNO IBM WATSON STUDIO CONFIGURADO\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"üìä Pandas versi√≥n: {pd.__version__}\")\n",
    "    print(f\"üî¢ NumPy versi√≥n: {np.__version__}\")\n",
    "    print(f\"üìà Matplotlib disponible: {'‚úÖ' if plt else '‚ùå'}\")\n",
    "    print(f\"üé® Seaborn configurado: {'‚úÖ' if sns else '‚ùå'}\")\n",
    "    print(f\"üìù JSONL soporte: {'‚úÖ Disponible' if jsonlines else '‚ùå Fallback JSON'}\")\n",
    "    print(f\"ü§ñ LangChain soporte: {'‚úÖ Disponible' if langchain_components else '‚ùå Implementaci√≥n alternativa'}\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"üéØ Sistema listo para procesamiento de datos CAN vehiculares\")\n",
    "    print(\"üîó Capacidades RAG/LLM: Habilitadas para IBM Watson\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "verificar_entorno_completo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d19973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# SECCI√ìN 3: DEFINICI√ìN DE ESTRUCTURAS DE DATOS SEM√ÅNTICAS\n",
    "# ===================================================================\n",
    "\n",
    "@dataclass\n",
    "class CANEventMetadata:\n",
    "    \"\"\"Metadatos estructurados para eventos vehiculares CAN en sistema RAG\"\"\"\n",
    "    timestamp_inicio: str\n",
    "    timestamp_fin: str\n",
    "    duracion_segundos: float\n",
    "    red_can: str\n",
    "    senales_involucradas: List[str]\n",
    "    evento_vehiculo: str\n",
    "    intensidad: str = \"medio\"\n",
    "    contexto_operativo: str = \"normal\"\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            'timestamp_inicio': self.timestamp_inicio,\n",
    "            'timestamp_fin': self.timestamp_fin,\n",
    "            'duracion_segundos': self.duracion_segundos,\n",
    "            'red_can': self.red_can,\n",
    "            'senales_involucradas': self.senales_involucradas,\n",
    "            'evento_vehiculo': self.evento_vehiculo,\n",
    "            'intensidad': self.intensidad,\n",
    "            'contexto_operativo': self.contexto_operativo\n",
    "        }\n",
    "\n",
    "@dataclass\n",
    "class RAGDocument:\n",
    "    \"\"\"Documento estructurado para sistema RAG vehicular\"\"\"\n",
    "    id: str\n",
    "    contenido_textual: str\n",
    "    metadatos: CANEventMetadata\n",
    "    tipo_documento: str = \"evento_can\"\n",
    "    calidad_descripcion: float = 0.8\n",
    "    \n",
    "    def to_jsonl_entry(self) -> Dict[str, Any]:\n",
    "        \"\"\"Convierte el documento a formato JSONL para entrenamiento\"\"\"\n",
    "        return {\n",
    "            'id': self.id,\n",
    "            'text': self.contenido_textual,\n",
    "            'metadata': self.metadatos.to_dict(),\n",
    "            'document_type': self.tipo_documento,\n",
    "            'quality_score': self.calidad_descripcion,\n",
    "            'created_at': datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "# Implementaci√≥n alternativa de Document para casos sin LangChain\n",
    "class DocumentoAlternativo:\n",
    "    def __init__(self, page_content: str, metadata: Dict = None):\n",
    "        self.page_content = page_content\n",
    "        self.metadata = metadata or {}\n",
    "\n",
    "# Usar Document de LangChain o alternativa\n",
    "if langchain_components:\n",
    "    Document = langchain_components['document']\n",
    "else:\n",
    "    Document = DocumentoAlternativo\n",
    "    logger.info(\"Usando implementaci√≥n alternativa de Document\")\n",
    "\n",
    "print(\"üèóÔ∏è  Estructuras de datos sem√°nticas definidas:\")\n",
    "print(\"   üìä CANEventMetadata: Metadatos enriquecidos de eventos\")\n",
    "print(\"   üóÇÔ∏è  RAGDocument: Entradas optimizadas para sistemas RAG\")\n",
    "print(\"   üìÑ Document: Soporte para chunking documental\")\n",
    "print(\"\\n‚úÖ Arquitectura de datos lista para procesamiento CAN‚ÜíRAG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3422673c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# SECCI√ìN 4: CARGA SEGURA DE DATOS CAN (DBC/BLF) - COMPATIBLE IBM\n",
    "# ===================================================================\n",
    "\n",
    "def generar_datos_demo_ibm() -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Genera datos de demostraci√≥n compatibles con IBM Watson Studio\n",
    "    Simula datos reales de redes CAN vehiculares para pruebas\n",
    "    \"\"\"\n",
    "    print(\"üîÑ Generando datos de demostraci√≥n para IBM Watson Studio...\")\n",
    "    \n",
    "    np.random.seed(42)  # Reproducibilidad\n",
    "    n_samples = 150  # Optimizado para demo en IBM\n",
    "    \n",
    "    # Timestamps base\n",
    "    base_time = datetime.now()\n",
    "    timestamps = [base_time + timedelta(seconds=i) for i in range(n_samples)]\n",
    "    \n",
    "    # CAN_EV - Red principal del motor (datos m√°s realistas)\n",
    "    rpm_base = 2000\n",
    "    velocidad_base = 60\n",
    "    temperatura_base = 85\n",
    "    \n",
    "    can_ev_data = {\n",
    "        'timestamp': timestamps,\n",
    "        'RPM_Motor': np.random.normal(rpm_base, 300, n_samples).clip(600, 4000),\n",
    "        'Velocidad_Vehiculo_KMH': np.random.normal(velocidad_base, 20, n_samples).clip(0, 120),\n",
    "        'Temperatura_Motor_C': np.random.normal(temperatura_base, 8, n_samples).clip(70, 110),\n",
    "        'Presion_Aceite_Bar': np.random.normal(3.2, 0.4, n_samples).clip(2, 5),\n",
    "        'Posicion_Acelerador_Pct': np.random.normal(30, 18, n_samples).clip(0, 100),\n",
    "        'Torque_Motor_Nm': np.random.normal(180, 40, n_samples).clip(50, 350)\n",
    "    }\n",
    "    \n",
    "    # CAN_CATL - Red de bater√≠a (se√±ales desconocidas para hip√≥tesis)\n",
    "    can_catl_data = {\n",
    "        'timestamp': timestamps,\n",
    "        'Signal_01': np.random.normal(75, 15, n_samples).clip(10, 100),  # Posible SOC\n",
    "        'Signal_02': np.random.normal(3.65, 0.15, n_samples).clip(3.2, 4.1),  # Posible voltaje celda\n",
    "        'Signal_03': np.random.normal(32, 6, n_samples).clip(20, 50),  # Posible temperatura\n",
    "        'Signal_04': np.random.normal(12.8, 0.3, n_samples).clip(11, 14),  # Posible voltaje sistema\n",
    "        'Signal_05': np.random.choice([0, 1], n_samples, p=[0.92, 0.08]),  # Posible alarma\n",
    "        'Signal_06': np.random.normal(85, 12, n_samples).clip(30, 100)  # Posible SOH\n",
    "    }\n",
    "    \n",
    "    # CAN_GS - Red de sistemas generales\n",
    "    can_gs_data = {\n",
    "        'timestamp': timestamps,\n",
    "        'Nivel_Combustible_Pct': np.random.normal(65, 25, n_samples).clip(5, 95),\n",
    "        'Temperatura_Exterior_C': np.random.normal(24, 8, n_samples),\n",
    "        'Estado_Frenos': np.random.choice([0, 1], n_samples, p=[0.85, 0.15]),\n",
    "        'Posicion_Volante_Deg': np.random.normal(0, 25, n_samples).clip(-90, 90),\n",
    "        'Consumo_Instantaneo_LH': np.random.normal(7.5, 2.2, n_samples).clip(3, 18),\n",
    "        'Velocidad_Crucero_Activo': np.random.choice([0, 1], n_samples, p=[0.7, 0.3])\n",
    "    }\n",
    "    \n",
    "    datos_demo = {\n",
    "        'CAN_EV': pd.DataFrame(can_ev_data),\n",
    "        'CAN_CATL': pd.DataFrame(can_catl_data), \n",
    "        'CAN_GS': pd.DataFrame(can_gs_data)\n",
    "    }\n",
    "    \n",
    "    print(f\"‚úÖ Datos simulados generados:\")\n",
    "    for red, df in datos_demo.items():\n",
    "        print(f\"   {red}: {len(df)} registros, {len(df.columns)-1} se√±ales\")\n",
    "    \n",
    "    return datos_demo\n",
    "\n",
    "def cargar_datos_desde_archivo_opcional():\n",
    "    \"\"\"\n",
    "    Permite cargar datos desde archivo si est√°n disponibles en IBM Watson Studio\n",
    "    \"\"\"\n",
    "    print(\"\\nüìÅ Buscando archivos de datos locales en el proyecto IBM...\")\n",
    "    \n",
    "    # Buscar archivos comunes en el directorio del proyecto\n",
    "    archivos_potenciales = [\n",
    "        'datos_can.csv', 'dataset_can.xlsx', 'can_data.json',\n",
    "        'can_ev.csv', 'can_catl.csv', 'can_gs.csv'\n",
    "    ]\n",
    "    \n",
    "    datos_encontrados = {}\n",
    "    \n",
    "    for archivo in archivos_potenciales:\n",
    "        if Path(archivo).exists():\n",
    "            try:\n",
    "                if archivo.endswith('.csv'):\n",
    "                    df = pd.read_csv(archivo)\n",
    "                    nombre_red = archivo.replace('.csv', '').upper()\n",
    "                    datos_encontrados[nombre_red] = df\n",
    "                    print(f\"   ‚úÖ Cargado: {archivo} ({len(df)} registros)\")\n",
    "                elif archivo.endswith('.xlsx'):\n",
    "                    df = pd.read_excel(archivo)\n",
    "                    datos_encontrados['CAN_DATA'] = df\n",
    "                    print(f\"   ‚úÖ Cargado: {archivo} ({len(df)} registros)\")\n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ö†Ô∏è Error cargando {archivo}: {e}\")\n",
    "    \n",
    "    if datos_encontrados:\n",
    "        print(f\"   üìä Total archivos cargados: {len(datos_encontrados)}\")\n",
    "        return datos_encontrados\n",
    "    else:\n",
    "        print(\"   ‚ÑπÔ∏è No se encontraron archivos locales - usando datos simulados\")\n",
    "        return None\n",
    "\n",
    "# Ejecutar carga de datos\n",
    "print(\"üöÄ Iniciando carga de datos CAN para IBM Watson Studio...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Intentar cargar datos reales primero\n",
    "datos_reales = cargar_datos_desde_archivo_opcional()\n",
    "\n",
    "if datos_reales:\n",
    "    datos_can = datos_reales\n",
    "    print(\"\\n‚úÖ Usando datos reales del proyecto\")\n",
    "else:\n",
    "    # Generar datos demo\n",
    "    datos_can = generar_datos_demo_ibm()\n",
    "    print(\"\\n‚úÖ Usando datos de demostraci√≥n (completamente funcionales)\")\n",
    "\n",
    "print(f\"\\nüìà Resumen de datos disponibles:\")\n",
    "for red, df in datos_can.items():\n",
    "    print(f\"   üîå {red}: {len(df)} registros temporales\")\n",
    "    \n",
    "print(\"\\nüéØ Datos CAN listos para transformaci√≥n sem√°ntica\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6961544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# SECCI√ìN 5: MOTOR DE TRANSFORMACI√ìN SEM√ÅNTICA CAN‚ÜíTEXTO\n",
    "# ===================================================================\n",
    "\n",
    "class GeneradorTextualCAN:\n",
    "    \"\"\"Generador de descripciones textuales para se√±ales CAN optimizado para IBM\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.plantillas_descripcion = {\n",
    "            'rpm': \"El motor opera a {valor:.0f} RPM, {interpretacion}\",\n",
    "            'velocidad': \"La velocidad del veh√≠culo es {valor:.1f} km/h, {interpretacion}\",\n",
    "            'temperatura': \"La temperatura registra {valor:.1f}¬∞C, {interpretacion}\",\n",
    "            'voltaje': \"El voltaje mide {valor:.2f}V, {interpretacion}\",\n",
    "            'porcentaje': \"El nivel indica {valor:.1f}%, {interpretacion}\",\n",
    "            'generica': \"La se√±al {nombre} presenta valor {valor:.3f}, {interpretacion}\"\n",
    "        }\n",
    "    \n",
    "    def generar_descripcion_signal(self, nombre_senal: str, serie: pd.Series, \n",
    "                                 timestamps: pd.Series, red_can: str) -> str:\n",
    "        \"\"\"Genera descripci√≥n textual inteligente para una se√±al\"\"\"\n",
    "        try:\n",
    "            valor_medio = serie.mean()\n",
    "            valor_min = serie.min()\n",
    "            valor_max = serie.max()\n",
    "            tendencia = serie.iloc[-1] - serie.iloc[0] if len(serie) > 1 else 0\n",
    "            \n",
    "            # Detectar tipo de se√±al\n",
    "            tipo_senal = self._detectar_tipo_senal(nombre_senal, valor_medio, valor_max - valor_min)\n",
    "            \n",
    "            # Generar interpretaci√≥n\n",
    "            interpretacion = self._generar_interpretacion(valor_medio, tendencia, tipo_senal)\n",
    "            \n",
    "            # Usar plantilla apropiada\n",
    "            plantilla = self.plantillas_descripcion.get(tipo_senal, self.plantillas_descripcion['generica'])\n",
    "            \n",
    "            if tipo_senal == 'generica':\n",
    "                descripcion = plantilla.format(nombre=nombre_senal, valor=valor_medio, interpretacion=interpretacion)\n",
    "            else:\n",
    "                descripcion = plantilla.format(valor=valor_medio, interpretacion=interpretacion)\n",
    "            \n",
    "            return f\"{descripcion} (Red: {red_can})\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error generando descripci√≥n para {nombre_senal}: {e}\")\n",
    "            return f\"Se√±al {nombre_senal} en red {red_can} con comportamiento variable\"\n",
    "    \n",
    "    def _detectar_tipo_senal(self, nombre: str, valor_medio: float, rango: float) -> str:\n",
    "        \"\"\"Detecta el tipo de se√±al basado en nombre y caracter√≠sticas estad√≠sticas\"\"\"\n",
    "        nombre_lower = nombre.lower()\n",
    "        \n",
    "        if 'rpm' in nombre_lower:\n",
    "            return 'rpm'\n",
    "        elif any(word in nombre_lower for word in ['velocidad', 'speed', 'vel']):\n",
    "            return 'velocidad'\n",
    "        elif any(word in nombre_lower for word in ['temp', 'temperatura']):\n",
    "            return 'temperatura'\n",
    "        elif any(word in nombre_lower for word in ['volt', 'voltage', 'tension']):\n",
    "            return 'voltaje'\n",
    "        elif 0 <= valor_medio <= 100 and rango > 10:\n",
    "            return 'porcentaje'\n",
    "        else:\n",
    "            return 'generica'\n",
    "    \n",
    "    def _generar_interpretacion(self, valor: float, tendencia: float, tipo: str) -> str:\n",
    "        \"\"\"Genera interpretaci√≥n contextual del comportamiento\"\"\"\n",
    "        if abs(tendencia) < 0.1:\n",
    "            comportamiento = \"manteni√©ndose estable\"\n",
    "        elif tendencia > 0:\n",
    "            comportamiento = \"con tendencia creciente\"\n",
    "        else:\n",
    "            comportamiento = \"con tendencia decreciente\"\n",
    "        \n",
    "        if tipo == 'rpm':\n",
    "            if valor < 800:\n",
    "                estado = \"indicando ralent√≠\"\n",
    "            elif valor > 3000:\n",
    "                estado = \"en alta demanda\"\n",
    "            else:\n",
    "                estado = \"en operaci√≥n normal\"\n",
    "        elif tipo == 'temperatura':\n",
    "            if valor > 90:\n",
    "                estado = \"en rango elevado\"\n",
    "            elif valor < 20:\n",
    "                estado = \"en rango bajo\"\n",
    "            else:\n",
    "                estado = \"en rango normal\"\n",
    "        else:\n",
    "            estado = \"en operaci√≥n\"\n",
    "        \n",
    "        return f\"{comportamiento}, {estado}\"\n",
    "    \n",
    "    def procesar_dataset_completo(self, df: pd.DataFrame, red_can: str) -> List[str]:\n",
    "        \"\"\"Procesa todo un dataset CAN y genera descripciones\"\"\"\n",
    "        descripciones = []\n",
    "        \n",
    "        # Identificar columna de timestamps\n",
    "        columna_tiempo = None\n",
    "        for col in ['timestamp', 'time', 'tiempo', 'Time']:\n",
    "            if col in df.columns:\n",
    "                columna_tiempo = col\n",
    "                break\n",
    "        \n",
    "        timestamps = df[columna_tiempo] if columna_tiempo else pd.Series(range(len(df)))\n",
    "        \n",
    "        # Procesar se√±ales num√©ricas\n",
    "        senales_numericas = df.select_dtypes(include=[np.number]).columns\n",
    "        if columna_tiempo and columna_tiempo in senales_numericas:\n",
    "            senales_numericas = senales_numericas.drop(columna_tiempo)\n",
    "        \n",
    "        for signal in senales_numericas:\n",
    "            descripcion = self.generar_descripcion_signal(\n",
    "                signal, df[signal], timestamps, red_can\n",
    "            )\n",
    "            descripciones.append(descripcion)\n",
    "        \n",
    "        return descripciones\n",
    "\n",
    "# Inicializar generador\n",
    "generador_textual = GeneradorTextualCAN()\n",
    "\n",
    "# Generar descripciones para datos de demostraci√≥n\n",
    "descripciones_por_red = {}\n",
    "\n",
    "print(\"üîÑ Generando descripciones textuales desde datos CAN...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for nombre_red, df in datos_can.items():\n",
    "    if not df.empty:\n",
    "        print(f\"Procesando {nombre_red}...\")\n",
    "        descripciones = generador_textual.procesar_dataset_completo(df, nombre_red)\n",
    "        descripciones_por_red[nombre_red] = descripciones\n",
    "        print(f\"  ‚úÖ {len(descripciones)} descripciones generadas\")\n",
    "\n",
    "print(f\"\\nüìä Total redes procesadas: {len(descripciones_por_red)}\")\n",
    "\n",
    "# Mostrar ejemplos\n",
    "print(\"\\n--- EJEMPLOS DE DESCRIPCIONES GENERADAS ---\")\n",
    "for red, descripciones in descripciones_por_red.items():\n",
    "    if descripciones:\n",
    "        print(f\"\\n{red} (muestra):\")\n",
    "        for i, desc in enumerate(descripciones[:2]):  # Mostrar primeras 2\n",
    "            print(f\"  {i+1}. {desc}\")\n",
    "\n",
    "print(\"\\n‚úÖ Transformaci√≥n sem√°ntica CAN‚ÜíTexto completada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc09e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# SECCI√ìN 6: GENERADOR DE METADATOS ESTRUCTURADOS\n",
    "# ===================================================================\n",
    "\n",
    "class GeneradorMetadatosCAN:\n",
    "    \"\"\"Generador inteligente de metadatos para eventos CAN\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.tipos_evento = {\n",
    "            'aceleracion': ['rpm', 'velocidad', 'throttle', 'acelerador'],\n",
    "            'frenado': ['brake', 'decel', 'pressure', 'freno'],\n",
    "            'temperatura': ['temp', 'coolant', 'oil', 'temperatura'],\n",
    "            'electrico': ['volt', 'current', 'battery', 'voltaje', 'corriente'],\n",
    "            'transmision': ['gear', 'clutch', 'torque', 'cambio']\n",
    "        }\n",
    "    \n",
    "    def generar_metadatos_evento(self, descripcion_textual: str, timestamp_inicio: datetime,\n",
    "                               duracion: float, red_can: str, senales_involucradas: List[str],\n",
    "                               stats_numericas: Dict[str, float]) -> CANEventMetadata:\n",
    "        \"\"\"Genera metadatos completos para un evento CAN\"\"\"\n",
    "        \n",
    "        # Detectar tipo de evento\n",
    "        evento_vehiculo = self._clasificar_evento(senales_involucradas)\n",
    "        \n",
    "        # Determinar intensidad\n",
    "        intensidad = self._calcular_intensidad(stats_numericas)\n",
    "        \n",
    "        # Determinar contexto operacional\n",
    "        contexto_operativo = self._determinar_contexto(stats_numericas)\n",
    "        \n",
    "        # Generar timestamp de fin\n",
    "        timestamp_fin = timestamp_inicio + timedelta(seconds=duracion)\n",
    "        \n",
    "        return CANEventMetadata(\n",
    "            timestamp_inicio=timestamp_inicio.isoformat(),\n",
    "            timestamp_fin=timestamp_fin.isoformat(),\n",
    "            duracion_segundos=duracion,\n",
    "            red_can=red_can,\n",
    "            senales_involucradas=senales_involucradas,\n",
    "            evento_vehiculo=evento_vehiculo,\n",
    "            intensidad=intensidad,\n",
    "            contexto_operativo=contexto_operativo\n",
    "        )\n",
    "    \n",
    "    def _clasificar_evento(self, senales: List[str]) -> str:\n",
    "        \"\"\"Clasifica el tipo de evento basado en las se√±ales involucradas\"\"\"\n",
    "        senales_texto = ' '.join(senales).lower()\n",
    "        \n",
    "        for tipo_evento, palabras_clave in self.tipos_evento.items():\n",
    "            if any(palabra in senales_texto for palabra in palabras_clave):\n",
    "                return tipo_evento\n",
    "        \n",
    "        return \"evento_general\"\n",
    "    \n",
    "    def _calcular_intensidad(self, stats: Dict[str, float]) -> str:\n",
    "        \"\"\"Calcula la intensidad del evento\"\"\"\n",
    "        cambio_relativo = stats.get('cambio_relativo_promedio', 0)\n",
    "        \n",
    "        if cambio_relativo > 0.5:\n",
    "            return \"alto\"\n",
    "        elif cambio_relativo > 0.2:\n",
    "            return \"medio\"\n",
    "        else:\n",
    "            return \"bajo\"\n",
    "    \n",
    "    def _determinar_contexto(self, stats: Dict[str, float]) -> str:\n",
    "        \"\"\"Determina el contexto operativo del veh√≠culo\"\"\"\n",
    "        velocidad = stats.get('velocidad_promedio', 0)\n",
    "        \n",
    "        if velocidad > 80:\n",
    "            return \"autopista\"\n",
    "        elif velocidad > 30:\n",
    "            return \"urbano\"\n",
    "        elif velocidad > 0:\n",
    "            return \"trafico_lento\"\n",
    "        else:\n",
    "            return \"detenido\"\n",
    "    \n",
    "    def calcular_calidad_descripcion(self, descripcion: str, num_senales: int) -> float:\n",
    "        \"\"\"Calcula un score de calidad para la descripci√≥n generada\"\"\"\n",
    "        # Factores de calidad\n",
    "        longitud_factor = min(len(descripcion) / 100, 1.0)\n",
    "        senales_factor = min(num_senales / 5, 1.0)\n",
    "        \n",
    "        # Bonificaci√≥n por palabras t√©cnicas\n",
    "        palabras_tecnicas = ['rpm', 'temperatura', 'voltaje', 'velocidad', 'operaci√≥n']\n",
    "        tecnico_factor = sum(1 for palabra in palabras_tecnicas \n",
    "                           if palabra in descripcion.lower()) / len(palabras_tecnicas)\n",
    "        \n",
    "        calidad = (longitud_factor * 0.4 + senales_factor * 0.4 + tecnico_factor * 0.2)\n",
    "        return round(max(0.1, min(1.0, calidad)), 3)\n",
    "\n",
    "# Inicializar generador de metadatos\n",
    "generador_metadatos = GeneradorMetadatosCAN()\n",
    "\n",
    "print(\"‚úÖ Generador de metadatos estructurados inicializado\")\n",
    "print(\"   üìä Clasificaci√≥n autom√°tica de eventos vehiculares\")\n",
    "print(\"   üéØ Determinaci√≥n de intensidad y contexto operacional\")\n",
    "print(\"   üìè M√©tricas de calidad automatizadas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719871a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# SECCI√ìN 7: PROCESADOR DE DOCUMENTACI√ìN T√âCNICA\n",
    "# ===================================================================\n",
    "\n",
    "class ProcesadorDocumentacionTecnica:\n",
    "    \"\"\"Procesador de documentaci√≥n t√©cnica para sistemas RAG\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.chunk_size = 800\n",
    "        self.chunk_overlap = 150\n",
    "        \n",
    "    def procesar_documento_completo(self, contenido: str = None, \n",
    "                                   metodo: str = \"semantico\") -> List[RAGDocument]:\n",
    "        \"\"\"Procesa un documento t√©cnico completo y genera chunks RAG\"\"\"\n",
    "        \n",
    "        # Usar documento simulado si no se proporciona contenido\n",
    "        if not contenido:\n",
    "            contenido = self._generar_documento_j1939_simulado()\n",
    "        \n",
    "        # Aplicar chunking\n",
    "        chunks = self._chunking_adaptativo(contenido)\n",
    "        \n",
    "        # Convertir a RAGDocument\n",
    "        documentos_rag = []\n",
    "        \n",
    "        for i, chunk_text in enumerate(chunks):\n",
    "            if len(chunk_text.strip()) < 50:  # Ignorar chunks muy peque√±os\n",
    "                continue\n",
    "            \n",
    "            # Crear metadatos para documentaci√≥n t√©cnica\n",
    "            metadatos_doc = CANEventMetadata(\n",
    "                timestamp_inicio=datetime.now().isoformat(),\n",
    "                timestamp_fin=datetime.now().isoformat(),\n",
    "                duracion_segundos=0.0,\n",
    "                red_can=\"DOCUMENTACION\",\n",
    "                senales_involucradas=[],\n",
    "                evento_vehiculo=\"referencia_tecnica\",\n",
    "                intensidad=\"informativo\",\n",
    "                contexto_operativo=\"documentacion\"\n",
    "            )\n",
    "            \n",
    "            doc_rag = RAGDocument(\n",
    "                id=f\"DOC_J1939_{i}\",\n",
    "                contenido_textual=chunk_text,\n",
    "                metadatos=metadatos_doc,\n",
    "                tipo_documento=\"documentacion_tecnica\",\n",
    "                calidad_descripcion=0.9\n",
    "            )\n",
    "            \n",
    "            documentos_rag.append(doc_rag)\n",
    "        \n",
    "        return documentos_rag\n",
    "    \n",
    "    def _chunking_adaptativo(self, texto: str) -> List[str]:\n",
    "        \"\"\"Implementa chunking adaptativo sin dependencias externas\"\"\"\n",
    "        \n",
    "        # Separar por p√°rrafos primero\n",
    "        paragrafos = texto.split('\\n\\n')\n",
    "        chunks = []\n",
    "        chunk_actual = \"\"\n",
    "        \n",
    "        for paragrafo in paragrafos:\n",
    "            # Si agregar el p√°rrafo no excede el l√≠mite, agregarlo\n",
    "            if len(chunk_actual + paragrafo) < self.chunk_size:\n",
    "                chunk_actual += paragrafo + \"\\n\\n\"\n",
    "            else:\n",
    "                # Guardar chunk actual si no est√° vac√≠o\n",
    "                if chunk_actual.strip():\n",
    "                    chunks.append(chunk_actual.strip())\n",
    "                \n",
    "                # Iniciar nuevo chunk\n",
    "                if len(paragrafo) <= self.chunk_size:\n",
    "                    chunk_actual = paragrafo + \"\\n\\n\"\n",
    "                else:\n",
    "                    # Dividir p√°rrafo largo por oraciones\n",
    "                    oraciones = paragrafo.split('. ')\n",
    "                    for oracion in oraciones:\n",
    "                        if len(chunk_actual + oracion) < self.chunk_size:\n",
    "                            chunk_actual += oracion + \". \"\n",
    "                        else:\n",
    "                            if chunk_actual.strip():\n",
    "                                chunks.append(chunk_actual.strip())\n",
    "                            chunk_actual = oracion + \". \"\n",
    "        \n",
    "        # Agregar √∫ltimo chunk\n",
    "        if chunk_actual.strip():\n",
    "            chunks.append(chunk_actual.strip())\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _generar_documento_j1939_simulado(self) -> str:\n",
    "        \"\"\"Genera documento J1939 simulado para demostraci√≥n\"\"\"\n",
    "        return \"\"\"\n",
    "# J1939 - Parameter Group Number (PGN) Reference para Veh√≠culos El√©ctricos\n",
    "\n",
    "## 1. Introducci√≥n al Protocolo J1939\n",
    "\n",
    "El protocolo J1939 es un est√°ndar de comunicaci√≥n vehicular que define c√≥mo los componentes\n",
    "electr√≥nicos del veh√≠culo se comunican entre s√≠ a trav√©s de la red CAN.\n",
    "\n",
    "## 2. Par√°metros del Motor El√©ctrico\n",
    "\n",
    "### 2.1 Velocidad del Motor (SPN 190)\n",
    "- **Descripci√≥n**: Medici√≥n de RPM del motor el√©ctrico de tracci√≥n\n",
    "- **Unidad**: Revoluciones por minuto (RPM)\n",
    "- **Rango**: 0 a 8000 RPM\n",
    "- **Resoluci√≥n**: 0.125 rpm/bit\n",
    "- **Frecuencia de transmisi√≥n**: 10ms\n",
    "\n",
    "### 2.2 Torque del Motor (SPN 513)\n",
    "- **Descripci√≥n**: Torque actual del motor el√©ctrico\n",
    "- **Unidad**: Newton-metros (Nm)\n",
    "- **Rango**: -32768 a 32767 Nm\n",
    "- **Aplicaci√≥n**: Control de tracci√≥n y regeneraci√≥n\n",
    "\n",
    "## 3. Sistema de Gesti√≥n de Bater√≠a (BMS)\n",
    "\n",
    "### 3.1 Estado de Carga (SOC)\n",
    "El estado de carga indica el porcentaje de energ√≠a disponible en la bater√≠a:\n",
    "- **Rango**: 0-100%\n",
    "- **Precisi√≥n**: ¬±2%\n",
    "- **Actualizaci√≥n**: Cada segundo\n",
    "\n",
    "### 3.2 Voltaje del Pack\n",
    "- **Voltaje nominal**: 400V\n",
    "- **Rango operativo**: 300V - 450V\n",
    "- **Voltaje de corte**: 280V\n",
    "\n",
    "### 3.3 Temperatura de Celdas\n",
    "- **Rango operativo**: -20¬∞C a 60¬∞C\n",
    "- **Temperatura √≥ptima**: 15¬∞C a 35¬∞C\n",
    "- **Protecci√≥n t√©rmica**: >65¬∞C\n",
    "\n",
    "## 4. Se√±ales de Diagn√≥stico\n",
    "\n",
    "### 4.1 C√≥digos de Falla (DTC)\n",
    "Los c√≥digos de diagn√≥stico permiten identificar problemas en el sistema:\n",
    "- **DTC P0xxx**: Tren motriz\n",
    "- **DTC B0xxx**: Carrocer√≠a\n",
    "- **DTC U0xxx**: Red de comunicaci√≥n\n",
    "\n",
    "### 4.2 Estados del Sistema\n",
    "- **Estado Normal**: Todos los sistemas operativos\n",
    "- **Estado Advertencia**: Condiciones no cr√≠ticas detectadas\n",
    "- **Estado Falla**: Sistemas cr√≠ticos comprometidos\n",
    "\n",
    "## 5. Protocolos de Comunicaci√≥n\n",
    "\n",
    "### 5.1 Configuraci√≥n CAN\n",
    "- **Velocidad**: 250 kbps o 500 kbps\n",
    "- **Formato**: CAN 2.0B (29-bit ID)\n",
    "- **Terminaci√≥n**: 120Œ© en cada extremo\n",
    "\n",
    "### 5.2 Prioridades de Mensaje\n",
    "Los mensajes se priorizan seg√∫n criticidad:\n",
    "1. **Alta prioridad**: Seguridad y control\n",
    "2. **Media prioridad**: Monitoreo operacional\n",
    "3. **Baja prioridad**: Informaci√≥n y diagn√≥stico\n",
    "\n",
    "Esta documentaci√≥n t√©cnica sirve como referencia para la interpretaci√≥n de se√±ales CAN\n",
    "en veh√≠culos el√©ctricos basados en el protocolo J1939.\n",
    "\"\"\"\n",
    "\n",
    "# Inicializar procesador\n",
    "procesador_docs = ProcesadorDocumentacionTecnica()\n",
    "\n",
    "# Generar documentaci√≥n t√©cnica procesada\n",
    "print(\"üìö Procesando documentaci√≥n t√©cnica J1939...\")\n",
    "docs_tecnicos = procesador_docs.procesar_documento_completo()\n",
    "\n",
    "print(f\"‚úÖ Documentaci√≥n t√©cnica procesada:\")\n",
    "print(f\"   üìÑ {len(docs_tecnicos)} chunks documentales generados\")\n",
    "print(f\"   üìä Calidad promedio: {np.mean([doc.calidad_descripcion for doc in docs_tecnicos]):.3f}\")\n",
    "print(\"   üîç Contenido optimizado para recuperaci√≥n RAG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52d5cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# SECCI√ìN 8: CONSTRUCTOR DEL DATASET RAG UNIFICADO\n",
    "# ===================================================================\n",
    "\n",
    "class ConstructorDatasetRAG_IBM:\n",
    "    \"\"\"Constructor del dataset RAG optimizado para IBM Watson Studio\"\"\"\n",
    "    \n",
    "    def __init__(self, ruta_salida: str = \"./\"):\n",
    "        self.ruta_salida = Path(ruta_salida)\n",
    "        self.documentos_rag = []\n",
    "        \n",
    "        # Estad√≠sticas del dataset\n",
    "        self.stats = {\n",
    "            'total_documentos': 0,\n",
    "            'eventos_can': 0,\n",
    "            'documentacion_tecnica': 0,\n",
    "            'hipotesis_catl': 0,\n",
    "            'calidad_promedio': 0.0\n",
    "        }\n",
    "    \n",
    "    def generar_evento_can_completo(self, df_segmento: pd.DataFrame,\n",
    "                                   red_can: str, indice_segmento: int) -> Optional[RAGDocument]:\n",
    "        \"\"\"Genera un documento RAG completo para un segmento de datos CAN\"\"\"\n",
    "        try:\n",
    "            # 1. Informaci√≥n temporal\n",
    "            timestamp_inicio = df_segmento['timestamp'].iloc[0] if 'timestamp' in df_segmento.columns else datetime.now()\n",
    "            duracion = len(df_segmento)\n",
    "            \n",
    "            # 2. An√°lisis de se√±ales num√©ricas\n",
    "            senales_numericas = df_segmento.select_dtypes(include=[np.number]).columns.tolist()\n",
    "            if 'timestamp' in senales_numericas:\n",
    "                senales_numericas.remove('timestamp')\n",
    "            \n",
    "            # 3. Generar descripciones textuales\n",
    "            descripciones_senales = []\n",
    "            for senal in senales_numericas[:5]:  # Limitar para eficiencia\n",
    "                try:\n",
    "                    serie = df_segmento[senal].dropna()\n",
    "                    if len(serie) > 2:\n",
    "                        desc = generador_textual.generar_descripcion_signal(\n",
    "                            senal, serie, \n",
    "                            df_segmento['timestamp'] if 'timestamp' in df_segmento.columns else pd.Series(range(len(serie))), \n",
    "                            red_can\n",
    "                        )\n",
    "                        descripciones_senales.append(desc)\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Error procesando se√±al {senal}: {e}\")\n",
    "            \n",
    "            # 4. Combinar en descripci√≥n completa\n",
    "            descripcion_completa = f\"Evento en red {red_can} (Segmento {indice_segmento}):\\\\n\"\n",
    "            descripcion_completa += \"\\\\n\".join([f\"- {desc}\" for desc in descripciones_senales])\n",
    "            \n",
    "            # 5. Calcular estad√≠sticas\n",
    "            stats_numericas = {\n",
    "                'cambio_relativo_promedio': np.mean([\n",
    "                    abs(df_segmento[col].iloc[-1] - df_segmento[col].iloc[0]) /\n",
    "                    (df_segmento[col].mean() + 1e-6)\n",
    "                    for col in senales_numericas if len(df_segmento[col].dropna()) > 1\n",
    "                ]) if senales_numericas else 0.0,\n",
    "                'velocidad_promedio': df_segmento.get('Velocidad_Vehiculo_KMH', pd.Series([0])).mean()\n",
    "            }\n",
    "            \n",
    "            # 6. Generar metadatos\n",
    "            metadatos = generador_metadatos.generar_metadatos_evento(\n",
    "                descripcion_textual=descripcion_completa,\n",
    "                timestamp_inicio=timestamp_inicio if isinstance(timestamp_inicio, datetime) else datetime.now(),\n",
    "                duracion=float(duracion),\n",
    "                red_can=red_can,\n",
    "                senales_involucradas=senales_numericas,\n",
    "                stats_numericas=stats_numericas\n",
    "            )\n",
    "            \n",
    "            # 7. Calcular calidad\n",
    "            calidad = generador_metadatos.calcular_calidad_descripcion(\n",
    "                descripcion_completa, len(senales_numericas)\n",
    "            )\n",
    "            \n",
    "            # 8. Crear documento RAG\n",
    "            doc_rag = RAGDocument(\n",
    "                id=f\"{red_can}_evento_{indice_segmento}\",\n",
    "                contenido_textual=descripcion_completa,\n",
    "                metadatos=metadatos,\n",
    "                tipo_documento=\"evento_can\",\n",
    "                calidad_descripcion=calidad\n",
    "            )\n",
    "            \n",
    "            return doc_rag\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generando evento CAN: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def generar_hipotesis_catl(self, df_catl: pd.DataFrame) -> List[RAGDocument]:\n",
    "        \"\"\"Genera hip√≥tesis para se√±ales CATL desconocidas\"\"\"\n",
    "        hipotesis_docs = []\n",
    "        \n",
    "        try:\n",
    "            senales_catl = [col for col in df_catl.columns if col.startswith('Signal_')]\n",
    "            \n",
    "            for i, senal in enumerate(senales_catl[:5]):  # Limitar para demo\n",
    "                serie = df_catl[senal].dropna()\n",
    "                \n",
    "                if len(serie) < 10:\n",
    "                    continue\n",
    "                \n",
    "                # Estad√≠sticas\n",
    "                stats = {\n",
    "                    'min': serie.min(),\n",
    "                    'max': serie.max(),\n",
    "                    'mean': serie.mean(),\n",
    "                    'std': serie.std(),\n",
    "                    'rango': serie.max() - serie.min()\n",
    "                }\n",
    "                \n",
    "                # Generar hip√≥tesis\n",
    "                hipotesis = self._generar_hipotesis_senal_catl(senal, stats)\n",
    "                \n",
    "                # Metadatos\n",
    "                metadatos_hipotesis = CANEventMetadata(\n",
    "                    timestamp_inicio=datetime.now().isoformat(),\n",
    "                    timestamp_fin=datetime.now().isoformat(),\n",
    "                    duracion_segundos=0.0,\n",
    "                    red_can=\"CAN_CATL\",\n",
    "                    senales_involucradas=[senal],\n",
    "                    evento_vehiculo=\"hipotesis_funcional\",\n",
    "                    intensidad=\"informativo\",\n",
    "                    contexto_operativo=\"analisis_exploratorio\"\n",
    "                )\n",
    "                \n",
    "                doc_hipotesis = RAGDocument(\n",
    "                    id=f\"CATL_hipotesis_{i}\",\n",
    "                    contenido_textual=hipotesis,\n",
    "                    metadatos=metadatos_hipotesis,\n",
    "                    tipo_documento=\"hipotesis_catl\",\n",
    "                    calidad_descripcion=0.6\n",
    "                )\n",
    "                \n",
    "                hipotesis_docs.append(doc_hipotesis)\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generando hip√≥tesis CATL: {str(e)}\")\n",
    "        \n",
    "        return hipotesis_docs\n",
    "    \n",
    "    def _generar_hipotesis_senal_catl(self, nombre_senal: str, stats: Dict) -> str:\n",
    "        \"\"\"Genera hip√≥tesis para se√±al CATL desconocida\"\"\"\n",
    "        \n",
    "        # An√°lisis de patrones\n",
    "        if 0 <= stats['mean'] <= 100 and stats['rango'] > 50:\n",
    "            tipo_hipotesis = \"porcentaje (posible SOC o nivel de carga)\"\n",
    "            comportamiento = f\"var√≠a entre {stats['min']:.1f}% y {stats['max']:.1f}%\"\n",
    "        elif 20 <= stats['mean'] <= 60 and stats['std'] < 10:\n",
    "            tipo_hipotesis = \"temperatura (posible temperatura de celda)\"\n",
    "            comportamiento = f\"se mantiene entre {stats['min']:.1f}¬∞C y {stats['max']:.1f}¬∞C\"\n",
    "        elif 3.0 <= stats['mean'] <= 4.5 and stats['std'] < 0.5:\n",
    "            tipo_hipotesis = \"voltaje (posible voltaje de celda)\"\n",
    "            comportamiento = f\"presenta valores t√≠picos de bater√≠a Li-ion entre {stats['min']:.2f}V y {stats['max']:.2f}V\"\n",
    "        elif stats['rango'] < stats['mean'] * 0.1:\n",
    "            tipo_hipotesis = \"valor de estado o configuraci√≥n\"\n",
    "            comportamiento = f\"permanece constante en {stats['mean']:.2f}\"\n",
    "        else:\n",
    "            tipo_hipotesis = \"par√°metro operativo no identificado\"\n",
    "            comportamiento = f\"muestra variabilidad con promedio de {stats['mean']:.2f}\"\n",
    "        \n",
    "        hipotesis = f\"\"\"\n",
    "HIP√ìTESIS PARA {nombre_senal} (Red CAN_CATL):\n",
    "\n",
    "Basado en an√°lisis estad√≠stico, esta se√±al probablemente representa un {tipo_hipotesis}.\n",
    "\n",
    "Comportamiento observado: {comportamiento}.\n",
    "\n",
    "Estad√≠sticas clave:\n",
    "- Valor promedio: {stats['mean']:.3f}\n",
    "- Desviaci√≥n est√°ndar: {stats['std']:.3f}\n",
    "- Rango total: {stats['rango']:.3f}\n",
    "\n",
    "Esta hip√≥tesis requiere validaci√≥n con documentaci√≥n t√©cnica.\n",
    "\"\"\"\n",
    "        \n",
    "        return hipotesis\n",
    "    \n",
    "    def construir_dataset_completo(self) -> str:\n",
    "        \"\"\"Construye el dataset RAG completo\"\"\"\n",
    "        print(\"üìã Iniciando construcci√≥n del dataset RAG en IBM Watson Studio...\")\n",
    "        \n",
    "        # 1. Procesar eventos CAN\n",
    "        for nombre_red, df_red in datos_can.items():\n",
    "            if df_red.empty:\n",
    "                continue\n",
    "            \n",
    "            print(f\"üîÑ Procesando {nombre_red}...\")\n",
    "            \n",
    "            # Segmentar datos\n",
    "            ventana = 30\n",
    "            n_segmentos = min(len(df_red) // ventana, 3)  # Limitar para demo IBM\n",
    "            \n",
    "            for i in range(n_segmentos):\n",
    "                segmento = df_red.iloc[i*ventana:(i+1)*ventana]\n",
    "                \n",
    "                doc_evento = self.generar_evento_can_completo(segmento, nombre_red, i)\n",
    "                if doc_evento:\n",
    "                    self.documentos_rag.append(doc_evento)\n",
    "                    self.stats['eventos_can'] += 1\n",
    "        \n",
    "        # 2. Generar hip√≥tesis CATL\n",
    "        if \"CAN_CATL\" in datos_can and not datos_can[\"CAN_CATL\"].empty:\n",
    "            print(\"üîç Generando hip√≥tesis para CAN_CATL...\")\n",
    "            hipotesis_catl = self.generar_hipotesis_catl(datos_can[\"CAN_CATL\"])\n",
    "            self.documentos_rag.extend(hipotesis_catl)\n",
    "            self.stats['hipotesis_catl'] = len(hipotesis_catl)\n",
    "        \n",
    "        # 3. Agregar documentaci√≥n t√©cnica\n",
    "        print(\"üìö Incorporando documentaci√≥n t√©cnica...\")\n",
    "        self.documentos_rag.extend(docs_tecnicos)\n",
    "        self.stats['documentacion_tecnica'] = len(docs_tecnicos)\n",
    "        \n",
    "        # 4. Calcular estad√≠sticas finales\n",
    "        self.stats['total_documentos'] = len(self.documentos_rag)\n",
    "        if self.documentos_rag:\n",
    "            self.stats['calidad_promedio'] = np.mean([\n",
    "                doc.calidad_descripcion for doc in self.documentos_rag\n",
    "            ])\n",
    "        \n",
    "        # 5. Exportar a JSONL\n",
    "        archivo_salida = self.ruta_salida / \"dataset_rag_decode_ev_ibm.jsonl\"\n",
    "        \n",
    "        try:\n",
    "            # Intentar usar jsonlines\n",
    "            if jsonlines:\n",
    "                with jsonlines.open(archivo_salida, mode='w') as writer:\n",
    "                    for doc in self.documentos_rag:\n",
    "                        writer.write(doc.to_jsonl_entry())\n",
    "            else:\n",
    "                # Fallback a JSON est√°ndar\n",
    "                with open(archivo_salida, 'w', encoding='utf-8') as f:\n",
    "                    for doc in self.documentos_rag:\n",
    "                        json.dump(doc.to_jsonl_entry(), f, ensure_ascii=False)\n",
    "                        f.write('\\\\n')\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error exportando JSONL: {e}\")\n",
    "            # Fallback b√°sico\n",
    "            archivo_salida = self.ruta_salida / \"dataset_rag_decode_ev_ibm.json\"\n",
    "            with open(archivo_salida, 'w', encoding='utf-8') as f:\n",
    "                json.dump([doc.to_jsonl_entry() for doc in self.documentos_rag], f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"‚úÖ Dataset RAG guardado en: {archivo_salida}\")\n",
    "        print(f\"üìä Estad√≠sticas finales: {self.stats}\")\n",
    "        \n",
    "        return str(archivo_salida)\n",
    "    \n",
    "    def generar_muestra_dataset(self, n_muestras: int = 3) -> Dict:\n",
    "        \"\"\"Genera muestra del dataset para inspecci√≥n\"\"\"\n",
    "        muestra = {}\n",
    "        \n",
    "        if len(self.documentos_rag) >= n_muestras:\n",
    "            for i in range(n_muestras):\n",
    "                doc = self.documentos_rag[i]\n",
    "                muestra[f\"muestra_{i+1}\"] = {\n",
    "                    \"id\": doc.id,\n",
    "                    \"tipo\": doc.tipo_documento,\n",
    "                    \"contenido_preview\": doc.contenido_textual[:200] + \"...\",\n",
    "                    \"calidad\": doc.calidad_descripcion,\n",
    "                    \"evento_vehicular\": doc.metadatos.evento_vehiculo,\n",
    "                    \"red_can\": doc.metadatos.red_can\n",
    "                }\n",
    "        \n",
    "        return muestra\n",
    "\n",
    "# Ejecutar construcci√≥n del dataset\n",
    "print(\"üöÄ Iniciando construcci√≥n del dataset RAG en IBM Watson Studio...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "constructor_rag = ConstructorDatasetRAG_IBM()\n",
    "archivo_dataset = constructor_rag.construir_dataset_completo()\n",
    "\n",
    "# Mostrar muestra del dataset\n",
    "muestra = constructor_rag.generar_muestra_dataset(3)\n",
    "\n",
    "print(\"\\\\nüìã MUESTRA DEL DATASET GENERADO:\")\n",
    "print(\"=\" * 70)\n",
    "for key, valor in muestra.items():\n",
    "    print(f\"\\\\n{key.upper()}:\")\n",
    "    for k, v in valor.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(f\"\\\\nüéØ DATASET RAG COMPLETADO PARA IBM\")\n",
    "print(f\"üìÅ Archivo: {archivo_dataset}\")\n",
    "print(f\"üìä Total documentos: {constructor_rag.stats['total_documentos']}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89afc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# SECCI√ìN 9: VALIDACI√ìN Y AN√ÅLISIS DE CALIDAD\n",
    "# ===================================================================\n",
    "\n",
    "def analizar_calidad_dataset_ibm(documentos: List[RAGDocument]) -> Dict:\n",
    "    \"\"\"An√°lisis completo de calidad del dataset RAG para IBM Watson Studio\"\"\"\n",
    "    \n",
    "    if not documentos:\n",
    "        return {\"error\": \"No hay documentos para analizar\"}\n",
    "    \n",
    "    analisis = {\n",
    "        'distribucion_tipos': {},\n",
    "        'calidad_promedio_por_tipo': {},\n",
    "        'estadisticas_longitud': {},\n",
    "        'cobertura_redes_can': {},\n",
    "        'eventos_por_tipo': {},\n",
    "        'metricas_globales': {},\n",
    "        'recomendaciones_ibm': []\n",
    "    }\n",
    "    \n",
    "    # 1. Distribuci√≥n por tipos de documento\n",
    "    tipos = [doc.tipo_documento for doc in documentos]\n",
    "    for tipo in set(tipos):\n",
    "        analisis['distribucion_tipos'][tipo] = tipos.count(tipo)\n",
    "        \n",
    "        # Calidad promedio por tipo\n",
    "        docs_tipo = [doc for doc in documentos if doc.tipo_documento == tipo]\n",
    "        analisis['calidad_promedio_por_tipo'][tipo] = np.mean([\n",
    "            doc.calidad_descripcion for doc in docs_tipo\n",
    "        ])\n",
    "    \n",
    "    # 2. Estad√≠sticas de longitud de texto\n",
    "    longitudes = [len(doc.contenido_textual) for doc in documentos]\n",
    "    analisis['estadisticas_longitud'] = {\n",
    "        'promedio': np.mean(longitudes),\n",
    "        'mediana': np.median(longitudes),\n",
    "        'min': min(longitudes),\n",
    "        'max': max(longitudes),\n",
    "        'desviacion': np.std(longitudes)\n",
    "    }\n",
    "    \n",
    "    # 3. Cobertura por redes CAN\n",
    "    redes_can = [doc.metadatos.red_can for doc in documentos]\n",
    "    for red in set(redes_can):\n",
    "        analisis['cobertura_redes_can'][red] = redes_can.count(red)\n",
    "    \n",
    "    # 4. Distribuci√≥n de eventos vehiculares\n",
    "    eventos = [doc.metadatos.evento_vehiculo for doc in documentos]\n",
    "    for evento in set(eventos):\n",
    "        analisis['eventos_por_tipo'][evento] = eventos.count(evento)\n",
    "    \n",
    "    # 5. M√©tricas globales\n",
    "    analisis['metricas_globales'] = {\n",
    "        'total_documentos': len(documentos),\n",
    "        'calidad_promedio_global': np.mean([doc.calidad_descripcion for doc in documentos]),\n",
    "        'documentos_alta_calidad': sum(1 for doc in documentos if doc.calidad_descripcion > 0.7),\n",
    "        'cobertura_temporal': len(set([doc.metadatos.timestamp_inicio[:10] for doc in documentos])),\n",
    "        'porcentaje_alta_calidad': sum(1 for doc in documentos if doc.calidad_descripcion > 0.7) / len(documentos) * 100\n",
    "    }\n",
    "    \n",
    "    # 6. Recomendaciones espec√≠ficas para IBM Watson\n",
    "    recomendaciones = []\n",
    "    \n",
    "    if analisis['metricas_globales']['total_documentos'] < 100:\n",
    "        recomendaciones.append(\"Incrementar el tama√±o del dataset para mejorar la cobertura RAG\")\n",
    "    \n",
    "    if analisis['metricas_globales']['calidad_promedio_global'] < 0.7:\n",
    "        recomendaciones.append(\"Optimizar plantillas de generaci√≥n textual para mayor calidad\")\n",
    "    \n",
    "    if len(analisis['cobertura_redes_can']) < 3:\n",
    "        recomendaciones.append(\"Incluir m√°s redes CAN para diversidad del dataset\")\n",
    "    \n",
    "    if analisis['estadisticas_longitud']['promedio'] > 2000:\n",
    "        recomendaciones.append(\"Considerar chunking m√°s granular para embeddings eficientes\")\n",
    "    \n",
    "    analisis['recomendaciones_ibm'] = recomendaciones\n",
    "    \n",
    "    return analisis\n",
    "\n",
    "def generar_reporte_ejecutivo_ibm(analisis: Dict) -> str:\n",
    "    \"\"\"Genera reporte ejecutivo optimizado para stakeholders IBM\"\"\"\n",
    "    \n",
    "    reporte = f\"\"\"\n",
    "=== REPORTE EJECUTIVO: DATASET RAG DECODE-EV ===\n",
    "Generado para IBM Watson Studio - {datetime.now().strftime('%Y-%m-%d %H:%M')}\n",
    "\n",
    "RESUMEN EJECUTIVO:\n",
    "‚úì Dataset RAG completado exitosamente\n",
    "‚úì {analisis['metricas_globales']['total_documentos']} documentos generados\n",
    "‚úì Calidad promedio: {analisis['metricas_globales']['calidad_promedio_global']:.3f}/1.000\n",
    "‚úì {analisis['metricas_globales']['porcentaje_alta_calidad']:.1f}% documentos alta calidad\n",
    "\n",
    "DISTRIBUCI√ìN DE CONTENIDO:\n",
    "\"\"\"\n",
    "    \n",
    "    for tipo, cantidad in analisis['distribucion_tipos'].items():\n",
    "        calidad = analisis['calidad_promedio_por_tipo'][tipo]\n",
    "        reporte += f\"‚Ä¢ {tipo}: {cantidad} documentos (calidad: {calidad:.3f})\\\\n\"\n",
    "    \n",
    "    reporte += f\"\"\"\n",
    "COBERTURA T√âCNICA:\n",
    "\"\"\"\n",
    "    for red, cantidad in analisis['cobertura_redes_can'].items():\n",
    "        reporte += f\"‚Ä¢ {red}: {cantidad} documentos\\\\n\"\n",
    "    \n",
    "    if analisis['recomendaciones_ibm']:\n",
    "        reporte += f\"\"\"\n",
    "RECOMENDACIONES PARA OPTIMIZACI√ìN:\n",
    "\"\"\"\n",
    "        for i, rec in enumerate(analisis['recomendaciones_ibm'], 1):\n",
    "            reporte += f\"{i}. {rec}\\\\n\"\n",
    "    \n",
    "    reporte += f\"\"\"\n",
    "SIGUIENTES PASOS SUGERIDOS:\n",
    "1. Integrar dataset con IBM Watson Discovery\n",
    "2. Generar embeddings usando IBM Watson NLP\n",
    "3. Implementar pipeline RAG con watsonx.ai\n",
    "4. Configurar m√©tricas de evaluaci√≥n continua\n",
    "\n",
    "Estado: LISTO PARA PRODUCCI√ìN EN IBM WATSON STUDIO\n",
    "\"\"\"\n",
    "    \n",
    "    return reporte\n",
    "\n",
    "# Ejecutar an√°lisis de calidad\n",
    "print(\"üîç Ejecutando an√°lisis de calidad para IBM Watson Studio...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "if constructor_rag.documentos_rag:\n",
    "    analisis_calidad = analizar_calidad_dataset_ibm(constructor_rag.documentos_rag)\n",
    "    \n",
    "    # Mostrar m√©tricas clave\n",
    "    print(\"üìä M√âTRICAS DE CALIDAD:\")\n",
    "    print(f\"   üìà Total documentos: {analisis_calidad['metricas_globales']['total_documentos']}\")\n",
    "    print(f\"   üìà Calidad promedio: {analisis_calidad['metricas_globales']['calidad_promedio_global']:.3f}\")\n",
    "    print(f\"   üìà Documentos alta calidad: {analisis_calidad['metricas_globales']['documentos_alta_calidad']}\")\n",
    "    \n",
    "    print(\"\\\\nüìã DISTRIBUCI√ìN POR TIPOS:\")\n",
    "    for tipo, cantidad in analisis_calidad['distribucion_tipos'].items():\n",
    "        calidad = analisis_calidad['calidad_promedio_por_tipo'][tipo]\n",
    "        print(f\"   üìÑ {tipo}: {cantidad} docs (calidad: {calidad:.3f})\")\n",
    "    \n",
    "    print(\"\\\\nüîå COBERTURA POR REDES CAN:\")\n",
    "    for red, cantidad in analisis_calidad['cobertura_redes_can'].items():\n",
    "        print(f\"   {red}: {cantidad} documentos\")\n",
    "    \n",
    "    if analisis_calidad['recomendaciones_ibm']:\n",
    "        print(\"\\\\nüí° RECOMENDACIONES:\")\n",
    "        for i, rec in enumerate(analisis_calidad['recomendaciones_ibm'], 1):\n",
    "            print(f\"   {i}. {rec}\")\n",
    "    \n",
    "    # Generar reporte ejecutivo\n",
    "    reporte_ejecutivo = generar_reporte_ejecutivo_ibm(analisis_calidad)\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\"*70)\n",
    "    print(\"‚úÖ AN√ÅLISIS DE CALIDAD COMPLETADO\")\n",
    "    print(\"‚úÖ Dataset RAG validado para IBM Watson Studio\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No se encontraron documentos para analizar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc6fa64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# SECCI√ìN 10: EXPORTACI√ìN Y ENTREGABLES FINALES\n",
    "# ===================================================================\n",
    "\n",
    "def crear_visualizaciones_ibm():\n",
    "    \"\"\"Crea visualizaciones optimizadas para IBM Watson Studio\"\"\"\n",
    "    \n",
    "    print(\"üìä Generando visualizaciones para IBM Watson Studio...\")\n",
    "    \n",
    "    try:\n",
    "        # 1. Distribuci√≥n de tipos de documentos\n",
    "        tipos_docs = [doc.tipo_documento for doc in constructor_rag.documentos_rag]\n",
    "        df_tipos = pd.DataFrame({'tipo': tipos_docs}).value_counts().reset_index()\n",
    "        df_tipos.columns = ['tipo_documento', 'cantidad']\n",
    "        \n",
    "        fig1 = px.pie(df_tipos, values='cantidad', names='tipo_documento',\n",
    "                      title='Distribuci√≥n de Tipos de Documentos RAG DECODE-EV',\n",
    "                      color_discrete_sequence=colores_ibm)\n",
    "        fig1.show()\n",
    "        \n",
    "        # 2. Calidad de documentos por tipo\n",
    "        calidades = [(doc.tipo_documento, doc.calidad_descripcion) for doc in constructor_rag.documentos_rag]\n",
    "        df_calidad = pd.DataFrame(calidades, columns=['tipo', 'calidad'])\n",
    "        \n",
    "        fig2 = px.box(df_calidad, x='tipo', y='calidad',\n",
    "                      title='Distribuci√≥n de Calidad por Tipo de Documento',\n",
    "                      color='tipo', color_discrete_sequence=colores_ibm)\n",
    "        fig2.show()\n",
    "        \n",
    "        # 3. M√©tricas del dataset\n",
    "        metricas = list(constructor_rag.stats.keys())\n",
    "        valores = list(constructor_rag.stats.values())\n",
    "        \n",
    "        fig3 = go.Figure(data=[\n",
    "            go.Bar(x=metricas, y=valores, marker_color=colores_ibm[0])\n",
    "        ])\n",
    "        fig3.update_layout(title='M√©tricas del Dataset RAG DECODE-EV IBM',\n",
    "                           xaxis_title='M√©tricas',\n",
    "                           yaxis_title='Valores')\n",
    "        fig3.show()\n",
    "        \n",
    "        print(\"‚úÖ Visualizaciones generadas exitosamente\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error generando visualizaciones: {e}\")\n",
    "        print(\"üìä Datos disponibles para visualizaci√≥n manual:\")\n",
    "        print(f\"   - {len(constructor_rag.documentos_rag)} documentos totales\")\n",
    "        print(f\"   - Calidad promedio: {constructor_rag.stats['calidad_promedio']:.3f}\")\n",
    "\n",
    "def exportar_metadatos_complementarios():\n",
    "    \"\"\"Exporta metadatos complementarios para integraci√≥n IBM\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Crear archivo de configuraci√≥n para IBM Watson\n",
    "        config_ibm = {\n",
    "            'proyecto': 'DECODE-EV RAG System',\n",
    "            'version': '1.0.0',\n",
    "            'fecha_generacion': datetime.now().isoformat(),\n",
    "            'estadisticas': constructor_rag.stats,\n",
    "            'configuracion_rag': {\n",
    "                'chunk_size_optimo': 800,\n",
    "                'overlap_recomendado': 150,\n",
    "                'calidad_minima': 0.6,\n",
    "                'tipos_documentos': list(set([doc.tipo_documento for doc in constructor_rag.documentos_rag]))\n",
    "            },\n",
    "            'integracion_watson': {\n",
    "                'discovery_ready': True,\n",
    "                'embedding_compatible': True,\n",
    "                'conversational_optimized': True\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Guardar configuraci√≥n\n",
    "        config_file = Path('./config_decode_ev_ibm.json')\n",
    "        with open(config_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(config_ibm, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"‚úÖ Configuraci√≥n IBM exportada: {config_file}\")\n",
    "        \n",
    "        # Exportar esquema de metadatos\n",
    "        esquema_metadatos = {\n",
    "            'CANEventMetadata': {\n",
    "                'campos_requeridos': ['timestamp_inicio', 'timestamp_fin', 'red_can', 'evento_vehiculo'],\n",
    "                'campos_opcionales': ['intensidad', 'contexto_operativo'],\n",
    "                'tipos_eventos': list(set([doc.metadatos.evento_vehiculo for doc in constructor_rag.documentos_rag])),\n",
    "                'redes_can': list(set([doc.metadatos.red_can for doc in constructor_rag.documentos_rag]))\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        esquema_file = Path('./esquema_metadatos_decode_ev.json')\n",
    "        with open(esquema_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(esquema_metadatos, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"‚úÖ Esquema de metadatos exportado: {esquema_file}\")\n",
    "        \n",
    "        return config_file, esquema_file\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error exportando metadatos: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def generar_documentacion_final():\n",
    "    \"\"\"Genera documentaci√≥n final del proyecto\"\"\"\n",
    "    \n",
    "    documentacion = f\"\"\"\n",
    "# DECODE-EV: Sistema RAG para Datos Vehiculares CAN\n",
    "## Documentaci√≥n Final - IBM Watson Studio\n",
    "\n",
    "### Resumen del Proyecto\n",
    "**Fecha:** {datetime.now().strftime('%d/%m/%Y')}\n",
    "**Versi√≥n:** 1.0.0\n",
    "**Entorno:** IBM Watson Studio / Cloud Pak for Data\n",
    "\n",
    "### Resultados Obtenidos\n",
    "- ‚úÖ Dataset RAG generado: {constructor_rag.stats['total_documentos']} documentos\n",
    "- ‚úÖ Eventos CAN procesados: {constructor_rag.stats['eventos_can']}\n",
    "- ‚úÖ Documentaci√≥n t√©cnica: {constructor_rag.stats['documentacion_tecnica']} chunks\n",
    "- ‚úÖ Hip√≥tesis CATL: {constructor_rag.stats['hipotesis_catl']} generadas\n",
    "- ‚úÖ Calidad promedio: {constructor_rag.stats['calidad_promedio']:.3f}/1.0\n",
    "\n",
    "### Archivos Generados\n",
    "1. **dataset_rag_decode_ev_ibm.jsonl** - Dataset principal para RAG\n",
    "2. **config_decode_ev_ibm.json** - Configuraci√≥n para Watson\n",
    "3. **esquema_metadatos_decode_ev.json** - Esquema de metadatos\n",
    "4. **IBM_DECODE_EV_RAG_Notebook.ipynb** - Notebook ejecutable\n",
    "\n",
    "### Integraci√≥n con IBM Watson\n",
    "El dataset est√° optimizado para:\n",
    "- **Watson Discovery**: Indexaci√≥n y b√∫squeda sem√°ntica\n",
    "- **watsonx.ai**: Generaci√≥n aumentada por recuperaci√≥n\n",
    "- **Watson NLP**: Procesamiento de lenguaje natural\n",
    "- **Cloud Pak for Data**: Gesti√≥n de datos empresarial\n",
    "\n",
    "### Siguiente Fase Recomendada\n",
    "1. Vectorizaci√≥n con embeddings de Watson\n",
    "2. Indexaci√≥n en Watson Discovery\n",
    "3. Implementaci√≥n de pipeline RAG\n",
    "4. Evaluaci√≥n de m√©tricas de precisi√≥n\n",
    "\n",
    "### Contacto T√©cnico\n",
    "Proyecto Integrador Grupo 7 - IBM AI Engineering\n",
    "Instituto Tecnol√≥gico de Monterrey\n",
    "\"\"\"\n",
    "    \n",
    "    doc_file = Path('./README_DECODE_EV_IBM.md')\n",
    "    with open(doc_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(documentacion)\n",
    "    \n",
    "    print(f\"‚úÖ Documentaci√≥n final generada: {doc_file}\")\n",
    "    return doc_file\n",
    "\n",
    "# Ejecutar exportaci√≥n final\n",
    "print(\"üöÄ Ejecutando exportaci√≥n y entregables finales...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Crear visualizaciones\n",
    "crear_visualizaciones_ibm()\n",
    "\n",
    "# Exportar metadatos complementarios\n",
    "config_file, esquema_file = exportar_metadatos_complementarios()\n",
    "\n",
    "# Generar documentaci√≥n\n",
    "doc_file = generar_documentacion_final()\n",
    "\n",
    "# Resumen final\n",
    "print(\"\\\\n\" + \"üéâ\" * 20)\n",
    "print(\"üéâ PROYECTO DECODE-EV COMPLETADO EXITOSAMENTE\")\n",
    "print(\"üéâ\" * 20)\n",
    "\n",
    "print(\"\\\\nüì¶ ENTREGABLES GENERADOS:\")\n",
    "print(f\"   üìÑ Dataset RAG: {archivo_dataset}\")\n",
    "if config_file:\n",
    "    print(f\"   ‚öôÔ∏è  Configuraci√≥n IBM: {config_file}\")\n",
    "if esquema_file:\n",
    "    print(f\"   üìã Esquema metadatos: {esquema_file}\")\n",
    "if doc_file:\n",
    "    print(f\"   üìö Documentaci√≥n: {doc_file}\")\n",
    "\n",
    "print(f\"\\\\nüìä ESTAD√çSTICAS FINALES:\")\n",
    "print(f\"   üî¢ Total documentos: {constructor_rag.stats['total_documentos']}\")\n",
    "print(f\"   üöó Eventos CAN: {constructor_rag.stats['eventos_can']}\")\n",
    "print(f\"   üìö Doc. t√©cnica: {constructor_rag.stats['documentacion_tecnica']}\")\n",
    "print(f\"   üîç Hip√≥tesis CATL: {constructor_rag.stats['hipotesis_catl']}\")\n",
    "print(f\"   ‚≠ê Calidad: {constructor_rag.stats['calidad_promedio']:.3f}\")\n",
    "\n",
    "print(\"\\\\nüöÄ SISTEMA RAG DECODE-EV LISTO PARA IBM WATSON STUDIO\")\n",
    "print(\"üîó Preparado para integraci√≥n con watsonx.ai\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
