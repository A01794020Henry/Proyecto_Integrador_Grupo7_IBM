{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68c86249",
   "metadata": {},
   "source": [
    "## Uso del PDF de capa de aplicación CAN\n",
    "\n",
    "Si cuentas con un PDF que define la capa de aplicación de la red CAN del vehículo, úsalo para construir `definiciones_dbc` cuando no dispongas de un `.dbc`:\n",
    "\n",
    "- Extrae por señal: factor (scale), offset, unidad y una breve descripción/comentario.\n",
    "- Vuelca esa información a un JSON con formato:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"Velocidad_Vehiculo\": {\"factor\": 1.0, \"offset\": 0.0, \"unit\": \"km/h\", \"comment\": \"Velocidad del vehículo\"},\n",
    "  \"Pedal_Freno\": {\"factor\": 1.0, \"offset\": 0.0, \"unit\": \"%\", \"comment\": \"Posición pedal freno\"}\n",
    "}\n",
    "```\n",
    "\n",
    "Luego, carga con `cargar_definiciones_dbc(path_json_def=...)` y el flujo aplicará los factores/offset y usará las unidades y comentarios en las descripciones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4970f2",
   "metadata": {},
   "source": [
    "# Ingeniería de Requerimientos LLM RAG — Detección de Eventos CAN y Descripciones Multi-señal\n",
    "\n",
    "Este notebook extiende el flujo previo para:\n",
    "- Integración profunda con definiciones DBC (factor, offset, unidad, comentario)\n",
    "- Reemplazo de segmentación fija por detección de eventos basados en reglas\n",
    "- Generación de descripciones multi-señal con correlaciones\n",
    "- Metadatos mejorados para indexación/búsqueda en RAG\n",
    "- Optimización por streaming al leer BLF cuando sea posible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4c011f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalación opcional (solo si hace falta) y librerías\n",
    "# Nota: Ejecuta esta celda si tu entorno no tiene estas dependencias.\n",
    "try:\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "except Exception:\n",
    "    import sys, subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pandas\", \"numpy\"])  # noqa\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "# Librerías opcionales para CAN/DBC\n",
    "try:\n",
    "    import can  # python-can\n",
    "except Exception:\n",
    "    can = None\n",
    "\n",
    "try:\n",
    "    import cantools  # para parsear DBC (opcional pero recomendado)\n",
    "except Exception:\n",
    "    cantools = None\n",
    "\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Tuple, Dict, Optional, Any\n",
    "import json\n",
    "import math\n",
    "import io\n",
    "\n",
    "# Utilidades genéricas\n",
    "def _safe_pct_change(arr: np.ndarray) -> float:\n",
    "    if arr.size < 2:\n",
    "        return 0.0\n",
    "    a, b = float(arr[0]), float(arr[-1])\n",
    "    if a == 0:\n",
    "        return float(\"inf\") if b != 0 else 0.0\n",
    "    return (b - a) / abs(a)\n",
    "\n",
    "\n",
    "def _coef_variacion(arr: np.ndarray) -> float:\n",
    "    if arr.size == 0:\n",
    "        return 0.0\n",
    "    m = float(np.mean(arr))\n",
    "    s = float(np.std(arr))\n",
    "    return 0.0 if m == 0 else s / abs(m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "93d3e1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga de definiciones DBC (factor, offset, unidad, comentario)\n",
    "# Devuelve un diccionario: { signal_name: {\"factor\": float, \"offset\": float, \"unit\": str, \"comment\": str} }\n",
    "\n",
    "# Hacer esta celda auto-contenida para evitar NameError si se ejecuta primero\n",
    "from typing import Optional, Dict, Any, List\n",
    "try:\n",
    "    import json\n",
    "except Exception:\n",
    "    import json  # aseguramos disponibilidad\n",
    "try:\n",
    "    import cantools  # para parsear DBC (opcional)\n",
    "except Exception:\n",
    "    cantools = None\n",
    "\n",
    "\n",
    "def cargar_definiciones_dbc(path_dbc: Optional[str] = None,\n",
    "                            path_json_def: Optional[str] = None) -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Carga definiciones de señales desde un archivo .dbc (vía cantools) o\n",
    "    desde un JSON ya estructurado. Si ambos son None, devuelve {}.\n",
    "    \"\"\"\n",
    "    definiciones: Dict[str, Dict[str, Any]] = {}\n",
    "\n",
    "    # Prioridad 1: JSON ya preparado\n",
    "    if path_json_def:\n",
    "        with open(path_json_def, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        # Validar formato mínimo\n",
    "        if isinstance(data, dict):\n",
    "            for sig, meta in data.items():\n",
    "                definiciones[str(sig)] = {\n",
    "                    \"factor\": float(meta.get(\"factor\", 1.0)),\n",
    "                    \"offset\": float(meta.get(\"offset\", 0.0)),\n",
    "                    \"unit\": str(meta.get(\"unit\", \"\")),\n",
    "                    \"comment\": str(meta.get(\"comment\", \"\")),\n",
    "                }\n",
    "        return definiciones\n",
    "\n",
    "    # Prioridad 2: DBC si cantools está disponible\n",
    "    if path_dbc and cantools is not None:\n",
    "        db = cantools.database.load_file(path_dbc)\n",
    "        # Recorremos señales en todos los mensajes\n",
    "        for msg in db.messages:\n",
    "            for sig in msg.signals:\n",
    "                definiciones[sig.name] = {\n",
    "                    \"factor\": float(getattr(sig, \"scale\", 1.0) or 1.0),\n",
    "                    \"offset\": float(getattr(sig, \"offset\", 0.0) or 0.0),\n",
    "                    \"unit\": str(getattr(sig, \"unit\", \"\") or \"\"),\n",
    "                    \"comment\": str(getattr(sig, \"comment\", \"\") or \"\"),\n",
    "                }\n",
    "        return definiciones\n",
    "\n",
    "    return definiciones\n",
    "\n",
    "\n",
    "def cargar_definiciones_dbc_multiples(paths_dbc: List[str]) -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"Carga y fusiona definiciones desde múltiples DBC. La primera aparición gana,\n",
    "    salvo que una posterior aporte unit/comment no vacíos.\n",
    "    \"\"\"\n",
    "    defs: Dict[str, Dict[str, Any]] = {}\n",
    "    if not paths_dbc or cantools is None:\n",
    "        return defs\n",
    "    for path in paths_dbc:\n",
    "        try:\n",
    "            db = cantools.database.load_file(path)\n",
    "            for msg in db.messages:\n",
    "                for sig in msg.signals:\n",
    "                    nuevo = {\n",
    "                        \"factor\": float(getattr(sig, \"scale\", 1.0) or 1.0),\n",
    "                        \"offset\": float(getattr(sig, \"offset\", 0.0) or 0.0),\n",
    "                        \"unit\": str(getattr(sig, \"unit\", \"\") or \"\"),\n",
    "                        \"comment\": str(getattr(sig, \"comment\", \"\") or \"\"),\n",
    "                    }\n",
    "                    if sig.name not in defs:\n",
    "                        defs[sig.name] = nuevo\n",
    "                    else:\n",
    "                        # Completar unit/comment si estaban vacíos\n",
    "                        if not defs[sig.name].get(\"unit\") and nuevo.get(\"unit\"):\n",
    "                            defs[sig.name][\"unit\"] = nuevo[\"unit\"]\n",
    "                        if not defs[sig.name].get(\"comment\") and nuevo.get(\"comment\"):\n",
    "                            defs[sig.name][\"comment\"] = nuevo[\"comment\"]\n",
    "        except Exception:\n",
    "            continue\n",
    "    return defs\n",
    "\n",
    "\n",
    "def cargar_definiciones_combinadas(paths_dbc: List[str],\n",
    "                                   path_json_def: Optional[str] = None) -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"Fusiona definiciones de múltiples DBC con un JSON opcional.\n",
    "    Estrategia: DBCs primero, luego JSON sobreescribe/completa.\n",
    "    \"\"\"\n",
    "    defs = cargar_definiciones_dbc_multiples(paths_dbc)\n",
    "    if path_json_def:\n",
    "        try:\n",
    "            with open(path_json_def, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "            if isinstance(data, dict):\n",
    "                for sig, meta in data.items():\n",
    "                    defs[sig] = {\n",
    "                        \"factor\": float(meta.get(\"factor\", defs.get(sig, {}).get(\"factor\", 1.0))),\n",
    "                        \"offset\": float(meta.get(\"offset\", defs.get(sig, {}).get(\"offset\", 0.0))),\n",
    "                        \"unit\": str(meta.get(\"unit\", defs.get(sig, {}).get(\"unit\", \"\"))),\n",
    "                        \"comment\": str(meta.get(\"comment\", defs.get(sig, {}).get(\"comment\", \"\"))),\n",
    "                    }\n",
    "        except Exception:\n",
    "            pass\n",
    "    return defs\n",
    "\n",
    "\n",
    "def normalizar_nombre_senal(nombre: str) -> str:\n",
    "    return nombre.strip().replace(\" \", \"_\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe6f6eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataclasses ampliadas para metadatos y entradas JSONL\n",
    "\n",
    "@dataclass\n",
    "class CANEventMetadata:\n",
    "    indice_segmento: int\n",
    "    red_can: Optional[str]\n",
    "    senales_involucradas: List[str]\n",
    "    analisis_estadistico: Dict[str, Dict[str, float]]  # por señal\n",
    "    correlaciones_detectadas: Optional[List[Dict[str, Any]]] = None\n",
    "    tipo_evento: Optional[str] = None\n",
    "    tipo_evento_complejo: Optional[str] = None\n",
    "    comentarios_dbc: Optional[Dict[str, str]] = None  # por señal\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RAGDatasetEntry:\n",
    "    id_evento: str\n",
    "    descripcion_textual: str\n",
    "    descripcion_tecnica: str\n",
    "    metadatos: CANEventMetadata\n",
    "    fuente: Optional[str] = None  # ruta BLF/CSV u origen\n",
    "\n",
    "    def to_jsonl_entry(self) -> str:\n",
    "        payload = {\n",
    "            \"id\": self.id_evento,\n",
    "            \"text\": self.descripcion_textual,\n",
    "            \"technical_description\": self.descripcion_tecnica,\n",
    "            \"metadata\": asdict(self.metadatos),\n",
    "            \"source\": self.fuente,\n",
    "        }\n",
    "        return json.dumps(payload, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9963d3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detección de eventos basada en reglas sobre columnas disponibles del DataFrame\n",
    "# Devuelve lista de (indice_inicio, indice_fin) en términos de índices del DataFrame\n",
    "from typing import Iterable\n",
    "\n",
    "\n",
    "def detectar_segmentos_eventos(df_red: pd.DataFrame,\n",
    "                               min_duracion: int = 15,\n",
    "                               columnas_umbral: Optional[Dict[str, Dict[str, float]]] = None) -> List[Tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    Detecta segmentos operativos significativos usando reglas: si alguna condición se activa\n",
    "    (p. ej. Velocidad_Vehiculo > 5, Pedal_Freno > 10, Corriente_Carga != 0) se considera dentro de evento.\n",
    "\n",
    "    columnas_umbral: {col_name: {\"op\": \">|>=|<|<=|!=|==\", \"valor\": float}}\n",
    "    min_duracion: longitud mínima (en número de filas) para considerar un segmento.\n",
    "    \"\"\"\n",
    "    if df_red.empty:\n",
    "        return []\n",
    "\n",
    "    # Reglas por defecto (adáptalas a tus nombres reales de columnas)\n",
    "    default_rules = {\n",
    "        \"Velocidad_Vehiculo\": {\"op\": \">\", \"valor\": 5.0},\n",
    "        \"Pedal_Freno\": {\"op\": \">\", \"valor\": 10.0},\n",
    "        \"Corriente_Carga\": {\"op\": \"!=\", \"valor\": 0.0},\n",
    "        \"Torque_Motor_Nm\": {\"op\": \">\", \"valor\": 5.0},\n",
    "    }\n",
    "    rules = columnas_umbral or default_rules\n",
    "\n",
    "    # Construye máscara booleana por fila: True si alguna regla se cumple\n",
    "    mask = np.zeros(len(df_red), dtype=bool)\n",
    "    for col, spec in rules.items():\n",
    "        if col in df_red.columns:\n",
    "            op = spec.get(\"op\", \">\")\n",
    "            val = spec.get(\"valor\", 0.0)\n",
    "            serie = pd.to_numeric(df_red[col], errors=\"coerce\")\n",
    "            if op == \">\":\n",
    "                m = serie > val\n",
    "            elif op == \">=\":\n",
    "                m = serie >= val\n",
    "            elif op == \"<\":\n",
    "                m = serie < val\n",
    "            elif op == \"<=\":\n",
    "                m = serie <= val\n",
    "            elif op == \"!=\":\n",
    "                m = serie != val\n",
    "            elif op == \"==\":\n",
    "                m = serie == val\n",
    "            else:\n",
    "                m = serie > val\n",
    "            mask = mask | m.fillna(False).values\n",
    "\n",
    "    # Extraer segmentos contiguos True de longitud >= min_duracion\n",
    "    segmentos: List[Tuple[int, int]] = []\n",
    "    en_evento = False\n",
    "    ini = 0\n",
    "    for i, v in enumerate(mask):\n",
    "        if v and not en_evento:\n",
    "            en_evento = True\n",
    "            ini = i\n",
    "        elif not v and en_evento:\n",
    "            fin = i\n",
    "            if fin - ini >= min_duracion:\n",
    "                segmentos.append((ini, fin))\n",
    "            en_evento = False\n",
    "    # Cierre si termina en True\n",
    "    if en_evento:\n",
    "        fin = len(mask)\n",
    "        if fin - ini >= min_duracion:\n",
    "            segmentos.append((ini, fin))\n",
    "\n",
    "    return segmentos\n",
    "\n",
    "\n",
    "# Construcción automática de reglas desde DBC + DF\n",
    "import numpy as np\n",
    "\n",
    "def construir_reglas_automaticas(df: pd.DataFrame,\n",
    "                                 definiciones_dbc: Dict[str, Dict[str, Any]],\n",
    "                                 percentil_activacion: float = 60.0) -> Dict[str, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Genera reglas de detección usando unidades/comentarios del DBC y estadísticas del DF.\n",
    "    Heurísticas:\n",
    "    - Velocidad: unit ~ km/h|m/s|mph o nombre con speed/velocidad -> op '>' p60\n",
    "    - Freno/Pedal: nombre con brake/freno/pedal -> umbral 0.2 si max<=1.5, si no 10\n",
    "    - Torque: unit ~ Nm o nombre con torque -> op '>' p60\n",
    "    - Corriente: unit ~ A|Amp -> op '!=' 0 (si hay variación), si no '>' p60\n",
    "    \"\"\"\n",
    "    if df is None or df.empty:\n",
    "        return {}\n",
    "    rules: Dict[str, Dict[str, float]] = {}\n",
    "\n",
    "    def pctl(col):\n",
    "        s = pd.to_numeric(df[col], errors='coerce').dropna()\n",
    "        return float(np.percentile(s, percentil_activacion)) if len(s) else 0.0\n",
    "\n",
    "    def find_candidates(pred):\n",
    "        # Buscar por DBC (unit/comment) y por nombre de columna\n",
    "        cands = []\n",
    "        for sig, meta in (definiciones_dbc or {}).items():\n",
    "            unit = str(meta.get('unit', '')).lower()\n",
    "            comment = str(meta.get('comment', '')).lower()\n",
    "            if pred(sig.lower(), unit, comment):\n",
    "                # Si la columna existe en df (normalizada o no), úsala\n",
    "                if sig in df.columns:\n",
    "                    cands.append(sig)\n",
    "                else:\n",
    "                    alt = normalizar_nombre_senal(sig)\n",
    "                    if alt in df.columns:\n",
    "                        cands.append(alt)\n",
    "        # Añadir por nombres de columnas\n",
    "        for col in df.columns:\n",
    "            cl = col.lower()\n",
    "            if pred(cl, cl, cl) and col not in cands:\n",
    "                cands.append(col)\n",
    "        return cands\n",
    "\n",
    "    # Predicados\n",
    "    is_speed   = lambda name, unit, comm: (('km/h' in unit) or ('m/s' in unit) or ('mph' in unit)\n",
    "                                           or ('speed' in name) or ('velocidad' in name))\n",
    "    is_brake   = lambda name, unit, comm: (('brake' in name) or ('freno' in name) or ('pedal' in name))\n",
    "    is_torque  = lambda name, unit, comm: (('nm' in unit) or ('torque' in name))\n",
    "    is_current = lambda name, unit, comm: (('a' == unit.strip()) or ('amp' in unit) or ('corriente' in name) or ('current' in name) or ('ibatt' in name))\n",
    "\n",
    "    # Velocidad\n",
    "    speed_cols = find_candidates(is_speed)\n",
    "    if speed_cols:\n",
    "        col = speed_cols[0]\n",
    "        rules[col] = {\"op\": \">\", \"valor\": round(pctl(col), 3)}\n",
    "\n",
    "    # Freno/Pedal\n",
    "    brake_cols = find_candidates(is_brake)\n",
    "    if brake_cols:\n",
    "        col = brake_cols[0]\n",
    "        s = pd.to_numeric(df[col], errors='coerce').dropna()\n",
    "        mx = float(s.max()) if len(s) else 0.0\n",
    "        rules[col] = {\"op\": \">\", \"valor\": 0.2 if mx <= 1.5 else 10.0}\n",
    "\n",
    "    # Torque\n",
    "    torq_cols = find_candidates(is_torque)\n",
    "    if torq_cols:\n",
    "        col = torq_cols[0]\n",
    "        rules[col] = {\"op\": \">\", \"valor\": round(pctl(col), 3)}\n",
    "\n",
    "    # Corriente\n",
    "    curr_cols = find_candidates(is_current)\n",
    "    if curr_cols:\n",
    "        col = curr_cols[0]\n",
    "        s = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
    "        if (s != 0).any():\n",
    "            rules[col] = {\"op\": \"!=\", \"valor\": 0.0}\n",
    "        else:\n",
    "            rules[col] = {\"op\": \">\", \"valor\": round(pctl(col), 3)}\n",
    "\n",
    "    return rules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "406c3167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generador de descripciones (señal y multi-señal) con integración DBC\n",
    "\n",
    "class GeneradorDescripcionesTextual:\n",
    "    def __init__(self, definiciones_dbc: Dict[str, Dict[str, Any]]):\n",
    "        self.definiciones_dbc = definiciones_dbc or {}\n",
    "\n",
    "    def _aplicar_factor_offset(self, senal: str, serie: pd.Series) -> Tuple[pd.Series, Dict[str, Any]]:\n",
    "        info = self.definiciones_dbc.get(senal, {})\n",
    "        factor = float(info.get(\"factor\", 1.0))\n",
    "        offset = float(info.get(\"offset\", 0.0))\n",
    "        unit = str(info.get(\"unit\", \"\"))\n",
    "        comment = str(info.get(\"comment\", \"\"))\n",
    "        serie_fisica = serie.astype(float) * factor + offset\n",
    "        return serie_fisica, {\"factor\": factor, \"offset\": offset, \"unit\": unit, \"comment\": comment}\n",
    "\n",
    "    def analizar_serie_temporal_blf(self, serie: pd.Series, unidad: str = \"\") -> Dict[str, float]:\n",
    "        arr = serie.astype(float).values\n",
    "        return {\n",
    "            \"mean\": float(np.mean(arr)) if arr.size else 0.0,\n",
    "            \"min\": float(np.min(arr)) if arr.size else 0.0,\n",
    "            \"max\": float(np.max(arr)) if arr.size else 0.0,\n",
    "            \"std\": float(np.std(arr)) if arr.size else 0.0,\n",
    "            \"coef_variacion\": _coef_variacion(arr),\n",
    "            \"cambio_porcentual\": _safe_pct_change(arr),\n",
    "            \"unidad\": unidad,\n",
    "        }\n",
    "\n",
    "    def generar_descripcion_senal(self,\n",
    "                                  signal_name: str,\n",
    "                                  serie: pd.Series,\n",
    "                                  red_can: Optional[str] = None) -> Tuple[str, str, Dict[str, float], Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Integra DBC: aplica factor/offset y usa unidad y comment en las plantillas.\n",
    "        Retorna: (descripcion_textual, descripcion_tecnica, analisis_estadistico, info_dbc)\n",
    "        \"\"\"\n",
    "        serie_fisica, info = self._aplicar_factor_offset(signal_name, serie)\n",
    "        stats = self.analizar_serie_temporal_blf(serie_fisica, unidad=info.get(\"unit\", \"\"))\n",
    "\n",
    "        # Plantilla textual básica por señal\n",
    "        desc = (\n",
    "            f\"Señal {signal_name} en red {red_can or 'CAN'} muestra valor medio {stats['mean']:.2f} {stats['unidad']} \"\n",
    "            f\"(min {stats['min']:.2f}, max {stats['max']:.2f}). \"\n",
    "            f\"Cambio porcentual={stats['cambio_porcentual']:.2f}, CV={stats['coef_variacion']:.2f}.\"\n",
    "        )\n",
    "\n",
    "        # Descripción técnica ampliada con comentario DBC\n",
    "        tech = (\n",
    "            f\"Unidad={stats['unidad']}; factor={info.get('factor')}, offset={info.get('offset')}. \"\n",
    "            f\"Comentario DBC: {info.get('comment', '')}\"\n",
    "        )\n",
    "\n",
    "        return desc, tech, stats, info\n",
    "\n",
    "    def generar_descripcion_evento_multisenal(self,\n",
    "                                              segmento_df: pd.DataFrame,\n",
    "                                              senales_relevantes: List[str],\n",
    "                                              matriz_correlacion: pd.DataFrame,\n",
    "                                              red_can: Optional[str],\n",
    "                                              indice_segmento: int) -> Tuple[str, str, Dict[str, Dict[str, float]], Dict[str, str], List[Dict[str, Any]]]:\n",
    "        \"\"\"\n",
    "        Crea descripciones ricas considerando varias señales y sus correlaciones.\n",
    "        Retorna: (desc_textual, desc_tecnica, analisis_por_senal, comentarios_dbc, correlaciones_detectadas)\n",
    "        \"\"\"\n",
    "        analisis_por_senal: Dict[str, Dict[str, float]] = {}\n",
    "        comentarios_dbc: Dict[str, str] = {}\n",
    "        piezas_texto: List[str] = []\n",
    "\n",
    "        # Calcular análisis por señal con unidades físicas\n",
    "        for s in senales_relevantes:\n",
    "            if s in segmento_df.columns:\n",
    "                d, t, stats, info = self.generar_descripcion_senal(s, segmento_df[s], red_can)\n",
    "                analisis_por_senal[s] = stats\n",
    "                if info.get(\"comment\"):\n",
    "                    comentarios_dbc[s] = info[\"comment\"]\n",
    "                piezas_texto.append(f\"{s}: mean={stats['mean']:.2f}{stats['unidad']} (Δ%={stats['cambio_porcentual']:.2f})\")\n",
    "\n",
    "        # Extraer correlaciones significativas\n",
    "        correlaciones_detectadas: List[Dict[str, Any]] = []\n",
    "        if matriz_correlacion is not None and not matriz_correlacion.empty:\n",
    "            umbral = 0.6\n",
    "            for a in senales_relevantes:\n",
    "                for b in senales_relevantes:\n",
    "                    if a < b and a in matriz_correlacion.index and b in matriz_correlacion.columns:\n",
    "                        r = float(matriz_correlacion.loc[a, b])\n",
    "                        if abs(r) >= umbral and not math.isnan(r):\n",
    "                            correlaciones_detectadas.append({\"senal_a\": a, \"senal_b\": b, \"r\": r})\n",
    "\n",
    "        # Texto principal del evento\n",
    "        titulo = f\"Evento {indice_segmento} — Red {red_can or 'CAN'}\"\n",
    "        resumen_senales = \", \".join(piezas_texto)\n",
    "        resumen_corr = \"; \".join([f\"{c['senal_a']}~{c['senal_b']} (r={c['r']:.2f})\" for c in correlaciones_detectadas])\n",
    "\n",
    "        desc_textual = (\n",
    "            f\"{titulo}: Se observaron variaciones en {len(senales_relevantes)} señales clave. \"\n",
    "            f\"Principales: {resumen_senales}. \"\n",
    "            + (f\"Correlaciones destacadas: {resumen_corr}.\" if resumen_corr else \"\")\n",
    "        )\n",
    "\n",
    "        desc_tecnica = (\n",
    "            \"Descripción multi-señal con correlaciones y unidades físicas. \"\n",
    "            f\"Señales consideradas: {', '.join(senales_relevantes)}.\"\n",
    "        )\n",
    "\n",
    "        return desc_textual, desc_tecnica, analisis_por_senal, comentarios_dbc, correlaciones_detectadas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ac81e9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clasificación de metadatos basada en estadísticas y señales involucradas\n",
    "\n",
    "class GeneradorMetadatosCompleto:\n",
    "    def clasificar_evento_inteligente(self,\n",
    "                                      analisis_estadistico: Dict[str, Dict[str, float]],\n",
    "                                      senales_involucradas: List[str]) -> Tuple[Optional[str], Optional[str]]:\n",
    "        \"\"\"\n",
    "        Retorna (tipo_evento simple, tipo_evento_complejo) basado en heurísticas de\n",
    "        cambio_porcentual y coef_variacion.\n",
    "        \"\"\"\n",
    "        # Heurísticas simples de ejemplo — adapta a tu dominio\n",
    "        tipo_evento = None\n",
    "        tipo_evento_complejo = None\n",
    "\n",
    "        # Señales clave si existen\n",
    "        v = analisis_estadistico.get(\"Velocidad_Vehiculo\", {})\n",
    "        rpm = analisis_estadistico.get(\"Velocidad_Motor_RPM\", {})\n",
    "        i_batt = analisis_estadistico.get(\"Corriente_Bateria\", {})\n",
    "        torque = analisis_estadistico.get(\"Torque_Motor_Nm\", {})\n",
    "        freno = analisis_estadistico.get(\"Pedal_Freno\", {})\n",
    "\n",
    "        # Aceleración fuerte: cambio % alto en velocidad o rpm + torque alto\n",
    "        if (v.get(\"cambio_porcentual\", 0) > 0.3 or rpm.get(\"cambio_porcentual\", 0) > 0.3) and torque.get(\"mean\", 0) > 20:\n",
    "            tipo_evento = \"Aceleracion_Fuerte\"\n",
    "            # Regeneración si corriente batería negativa (recuperando) y CV alto\n",
    "            if i_batt.get(\"mean\", 0) < -5 and i_batt.get(\"coef_variacion\", 0) > 0.2:\n",
    "                tipo_evento_complejo = \"Aceleracion_Fuerte_Regeneracion\"\n",
    "\n",
    "        # Frenado: pedal freno alto y caída de velocidad\n",
    "        if freno.get(\"mean\", 0) > 10 and v.get(\"cambio_porcentual\", 0) < -0.2:\n",
    "            tipo_evento = tipo_evento or \"Frenado\"\n",
    "\n",
    "        # Marcha lenta: velocidad baja y CV bajo en rpm\n",
    "        if v.get(\"mean\", 0) < 2 and rpm.get(\"coef_variacion\", 1) < 0.05:\n",
    "            tipo_evento = tipo_evento or \"Marcha_Lenta\"\n",
    "\n",
    "        return tipo_evento, tipo_evento_complejo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "12444875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructor del dataset RAG con detección de eventos y descripciones multi-señal\n",
    "\n",
    "class ConstructorDatasetRAG:\n",
    "    def __init__(self,\n",
    "                 definiciones_dbc: Dict[str, Dict[str, Any]],\n",
    "                 red_can: Optional[str] = None,\n",
    "                 top_n_senales: int = 5,\n",
    "                 detection_rules: Optional[Dict[str, Dict[str, float]]] = None,\n",
    "                 min_duracion_evento: int = 15):\n",
    "        self.defs = definiciones_dbc or {}\n",
    "        self.red_can = red_can\n",
    "        self.top_n_senales = max(3, int(top_n_senales))\n",
    "        self.desc_gen = GeneradorDescripcionesTextual(self.defs)\n",
    "        self.meta_gen = GeneradorMetadatosCompleto()\n",
    "        self.detection_rules = detection_rules or None\n",
    "        self.min_duracion_evento = int(min_duracion_evento)\n",
    "\n",
    "    def _seleccionar_senales_relevantes(self, df_segmento: pd.DataFrame) -> List[str]:\n",
    "        # Criterio: mayor varianza relativa o mayor cambio % (si la columna es numérica)\n",
    "        variancias: Dict[str, float] = {}\n",
    "        cambios: Dict[str, float] = {}\n",
    "        for col in df_segmento.columns:\n",
    "            if pd.api.types.is_numeric_dtype(df_segmento[col]):\n",
    "                arr = df_segmento[col].astype(float).values\n",
    "                variancias[col] = float(np.var(arr)) if arr.size else 0.0\n",
    "                cambios[col] = abs(_safe_pct_change(arr))\n",
    "        # Combinar puntuación simple\n",
    "        puntaje = {c: (variancias.get(c, 0.0) + cambios.get(c, 0.0)) for c in df_segmento.columns}\n",
    "        orden = sorted(puntaje.keys(), key=lambda k: puntaje[k], reverse=True)\n",
    "        # Filtrar señales obvias de tiempo/índice si existen\n",
    "        orden = [c for c in orden if c.lower() not in (\"timestamp\", \"time\", \"index\")]\n",
    "        return orden[: self.top_n_senales]\n",
    "\n",
    "    def generar_evento_can_completo(self,\n",
    "                                    df_segmento: pd.DataFrame,\n",
    "                                    indice_segmento: int,\n",
    "                                    fuente: Optional[str] = None) -> Optional[RAGDatasetEntry]:\n",
    "        if df_segmento.empty:\n",
    "            return None\n",
    "        senales_relevantes = self._seleccionar_senales_relevantes(df_segmento)\n",
    "        if not senales_relevantes:\n",
    "            return None\n",
    "\n",
    "        # Matriz de correlación sobre señales relevantes (usando Pearson por defecto)\n",
    "        try:\n",
    "            corr = df_segmento[senales_relevantes].corr().fillna(0.0)\n",
    "        except Exception:\n",
    "            corr = pd.DataFrame(np.eye(len(senales_relevantes)), index=senales_relevantes, columns=senales_relevantes)\n",
    "\n",
    "        desc, tech, analisis_por_senal, comentarios_dbc, correlaciones = (\n",
    "            self.desc_gen.generar_descripcion_evento_multisenal(\n",
    "                segmento_df=df_segmento,\n",
    "                senales_relevantes=senales_relevantes,\n",
    "                matriz_correlacion=corr,\n",
    "                red_can=self.red_can,\n",
    "                indice_segmento=indice_segmento,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Clasificación\n",
    "        tipo_evento, tipo_evento_complejo = self.meta_gen.clasificar_evento_inteligente(\n",
    "            analisis_estadistico=analisis_por_senal,\n",
    "            senales_involucradas=senales_relevantes,\n",
    "        )\n",
    "\n",
    "        metadatos = CANEventMetadata(\n",
    "            indice_segmento=indice_segmento,\n",
    "            red_can=self.red_can,\n",
    "            senales_involucradas=senales_relevantes,\n",
    "            analisis_estadistico=analisis_por_senal,\n",
    "            correlaciones_detectadas=correlaciones,\n",
    "            tipo_evento=tipo_evento,\n",
    "            tipo_evento_complejo=tipo_evento_complejo,\n",
    "            comentarios_dbc=comentarios_dbc,\n",
    "        )\n",
    "\n",
    "        entry = RAGDatasetEntry(\n",
    "            id_evento=f\"evento_{indice_segmento}\",\n",
    "            descripcion_textual=desc,\n",
    "            descripcion_tecnica=tech,\n",
    "            metadatos=metadatos,\n",
    "            fuente=fuente,\n",
    "        )\n",
    "        return entry\n",
    "\n",
    "    def construir_dataset_completo(self,\n",
    "                                   df: pd.DataFrame,\n",
    "                                   fuente: Optional[str] = None) -> List[RAGDatasetEntry]:\n",
    "        \"\"\"\n",
    "        Reemplaza segmentación fija por detección de eventos.\n",
    "        df: DataFrame ya decodificado (ideálmente con columnas de señales físicas\n",
    "            o al menos brutas con factores/offset en self.defs).\n",
    "        \"\"\"\n",
    "        dataset: List[RAGDatasetEntry] = []\n",
    "        segmentos = detectar_segmentos_eventos(df,\n",
    "                                               min_duracion=self.min_duracion_evento,\n",
    "                                               columnas_umbral=self.detection_rules)\n",
    "        for idx, (ini, fin) in enumerate(segmentos, start=1):\n",
    "            df_seg = df.iloc[ini:fin].reset_index(drop=True)\n",
    "            entry = self.generar_evento_can_completo(df_seg, indice_segmento=idx, fuente=fuente)\n",
    "            if entry is not None:\n",
    "                dataset.append(entry)\n",
    "        return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c2bb7ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook preparado: carga tus definiciones DBC y DataFrames/BLF para ejecutar el flujo.\n"
     ]
    }
   ],
   "source": [
    "# Escritura JSONL y ejemplo de uso\n",
    "\n",
    "def escribir_jsonl(entries: List[RAGDatasetEntry], path_salida: str) -> None:\n",
    "    with open(path_salida, \"w\", encoding=\"utf-8\") as f:\n",
    "        for e in entries:\n",
    "            f.write(e.to_jsonl_entry() + \"\\n\")\n",
    "\n",
    "\n",
    "# --- Ejemplo de uso (ajusta rutas y columnas a tu caso) ---\n",
    "# 1) Cargar definiciones DBC (usa path a .dbc o a un JSON con definiciones)\n",
    "# defs = cargar_definiciones_dbc(path_dbc=r\"C:\\ruta\\a\\tu\\archivo.dbc\")\n",
    "# o bien\n",
    "# defs = cargar_definiciones_dbc(path_json_def=r\"C:\\ruta\\a\\definiciones_dbc.json\")\n",
    "\n",
    "# 2) Preparar DataFrame decodificado con columnas de señales (ejemplo sintético)\n",
    "# data = {\n",
    "#     \"timestamp\": np.arange(0, 300),\n",
    "#     \"Velocidad_Vehiculo\": np.clip(np.linspace(0, 80, 300) + np.random.randn(300)*2, 0, None),\n",
    "#     \"Velocidad_Motor_RPM\": np.clip(np.linspace(900, 3000, 300) + np.random.randn(300)*50, 0, None),\n",
    "#     \"Torque_Motor_Nm\": np.abs(np.random.randn(300)*10 + 20),\n",
    "#     \"Pedal_Freno\": np.concatenate([np.zeros(100), np.ones(50)*20, np.zeros(150)]),\n",
    "#     \"Corriente_Bateria\": np.concatenate([np.ones(150)*(-5), np.ones(150)*(10)])\n",
    "# }\n",
    "# df = pd.DataFrame(data)\n",
    "\n",
    "# 3) Construir dataset\n",
    "# constructor = ConstructorDatasetRAG(definiciones_dbc=defs, red_can=\"CAN_1\", top_n_senales=5)\n",
    "# entries = constructor.construir_dataset_completo(df, fuente=\"sintetico\")\n",
    "\n",
    "# 4) Guardar JSONL para RAG/Watson Discovery\n",
    "# escribir_jsonl(entries, r\"C:\\ruta\\salida\\dataset_rag_decode_ev.jsonl\")\n",
    "\n",
    "print(\"Notebook preparado: carga tus definiciones DBC y DataFrames/BLF para ejecutar el flujo.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2794808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Opcional Avanzado) Decodificación BLF -> DataFrame por streaming usando cantools + python-can\n",
    "\n",
    "# Hacer esta celda más robusta si se ejecuta antes de la celda de imports\n",
    "import pandas as pd\n",
    "from typing import List, Optional, Set\n",
    "try:\n",
    "    import can  # python-can\n",
    "except Exception:\n",
    "    can = None\n",
    "try:\n",
    "    import cantools\n",
    "except Exception:\n",
    "    cantools = None\n",
    "\n",
    "\n",
    "def decodificar_blf_a_dataframe_stream(path_blf: str,\n",
    "                                       path_dbc: Optional[str] = None,\n",
    "                                       max_mensajes: Optional[int] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Decodifica un .blf a DataFrame usando streaming. Requiere cantools y python-can.\n",
    "    Advertencia: La estructura resultante depende del DBC; señales ausentes no aparecerán.\n",
    "    \"\"\"\n",
    "    if can is None or cantools is None:\n",
    "        raise RuntimeError(\"Se requiere python-can y cantools para decodificación BLF.\")\n",
    "    if not path_dbc:\n",
    "        raise ValueError(\"Proporciona path_dbc para decodificar por DBC.\")\n",
    "\n",
    "    db = cantools.database.load_file(path_dbc)\n",
    "    id_to_msg = {m.frame_id: m for m in db.messages}\n",
    "\n",
    "    filas = []\n",
    "    cont = 0\n",
    "    with can.BLFReader(path_blf) as reader:\n",
    "        for msg in reader:\n",
    "            if max_mensajes and cont >= max_mensajes:\n",
    "                break\n",
    "            cont += 1\n",
    "            try:\n",
    "                m = id_to_msg.get(msg.arbitration_id)\n",
    "                if m is None:\n",
    "                    continue\n",
    "                decoded = m.decode(bytes(msg.data))  # dict {signal_name: value}\n",
    "                decoded_row = {normalizar_nombre_senal(k): v for k, v in decoded.items()}\n",
    "                decoded_row[\"timestamp\"] = float(msg.timestamp)\n",
    "                filas.append(decoded_row)\n",
    "            except Exception:\n",
    "                # Ignora mensajes no decodificables\n",
    "                continue\n",
    "    if not filas:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.DataFrame(filas)\n",
    "    # Orden aproximado por timestamp\n",
    "    if \"timestamp\" in df.columns:\n",
    "        df = df.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def decodificar_blf_a_dataframe_stream_multi(path_blf: str,\n",
    "                                             paths_dbc: List[str],\n",
    "                                             max_mensajes: Optional[int] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Variante multi-DBC: carga varias bases y trata de decodificar cada mensaje con la que corresponda.\n",
    "    Si hay colisiones de frame_id, intenta en orden hasta que alguna decodifique sin error.\n",
    "    \"\"\"\n",
    "    if can is None or cantools is None:\n",
    "        raise RuntimeError(\"Se requiere python-can y cantools para decodificación BLF.\")\n",
    "    if not paths_dbc:\n",
    "        raise ValueError(\"Proporciona al menos un DBC en paths_dbc.\")\n",
    "\n",
    "    dbs = []\n",
    "    for p in paths_dbc:\n",
    "        try:\n",
    "            dbs.append(cantools.database.load_file(p))\n",
    "        except Exception:\n",
    "            continue\n",
    "    if not dbs:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Mapa: frame_id -> lista de definiciones de mensaje\n",
    "    map_msgs = {}\n",
    "    for db in dbs:\n",
    "        for m in db.messages:\n",
    "            map_msgs.setdefault(m.frame_id, []).append(m)\n",
    "\n",
    "    filas = []\n",
    "    cont = 0\n",
    "    with can.BLFReader(path_blf) as reader:\n",
    "        for msg in reader:\n",
    "            if max_mensajes and cont >= max_mensajes:\n",
    "                break\n",
    "            cont += 1\n",
    "            lst = map_msgs.get(msg.arbitration_id)\n",
    "            if not lst:\n",
    "                continue\n",
    "            decoded_ok = False\n",
    "            for m in lst:\n",
    "                try:\n",
    "                    decoded = m.decode(bytes(msg.data))\n",
    "                    decoded_row = {normalizar_nombre_senal(k): v for k, v in decoded.items()}\n",
    "                    decoded_row[\"timestamp\"] = float(msg.timestamp)\n",
    "                    filas.append(decoded_row)\n",
    "                    decoded_ok = True\n",
    "                    break\n",
    "                except Exception:\n",
    "                    continue\n",
    "            # si ninguna pudo decodificar, saltar\n",
    "    if not filas:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.DataFrame(filas)\n",
    "    if \"timestamp\" in df.columns:\n",
    "        df = df.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def decodificar_blf_a_dataframe_stream_multi_filtrado(path_blf: str,\n",
    "                                                      paths_dbc: List[str],\n",
    "                                                      signals_allow: Optional[Set[str]] = None,\n",
    "                                                      max_mensajes: Optional[int] = None,\n",
    "                                                      resample_ms: Optional[int] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Igual que la versión multi-DBC, pero sólo conserva señales en signals_allow y permite remuestrear por tiempo.\n",
    "    Esto evita DataFrames \"anchísimos\" y reduce memoria.\n",
    "    - signals_allow: nombres normalizados (normalizar_nombre_senal) a conservar. Si None, no filtra.\n",
    "    - resample_ms: p.ej. 100 para remuestrear a 10 Hz (media por ventana).\n",
    "    \"\"\"\n",
    "    if can is None or cantools is None:\n",
    "        raise RuntimeError(\"Se requiere python-can y cantools para decodificación BLF.\")\n",
    "    if not paths_dbc:\n",
    "        raise ValueError(\"Proporciona al menos un DBC en paths_dbc.\")\n",
    "\n",
    "    dbs = []\n",
    "    for p in paths_dbc:\n",
    "        try:\n",
    "            dbs.append(cantools.database.load_file(p))\n",
    "        except Exception:\n",
    "            continue\n",
    "    if not dbs:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    map_msgs = {}\n",
    "    for db in dbs:\n",
    "        for m in db.messages:\n",
    "            map_msgs.setdefault(m.frame_id, []).append(m)\n",
    "\n",
    "    filas = []\n",
    "    cont = 0\n",
    "    with can.BLFReader(path_blf) as reader:\n",
    "        for msg in reader:\n",
    "            if max_mensajes and cont >= max_mensajes:\n",
    "                break\n",
    "            cont += 1\n",
    "            lst = map_msgs.get(msg.arbitration_id)\n",
    "            if not lst:\n",
    "                continue\n",
    "            for m in lst:\n",
    "                try:\n",
    "                    decoded = m.decode(bytes(msg.data))\n",
    "                    if signals_allow is not None:\n",
    "                        decoded_row = {normalizar_nombre_senal(k): v for k, v in decoded.items() if normalizar_nombre_senal(k) in signals_allow}\n",
    "                        if not decoded_row:\n",
    "                            # Nada que conservar de este mensaje\n",
    "                            break\n",
    "                    else:\n",
    "                        decoded_row = {normalizar_nombre_senal(k): v for k, v in decoded.items()}\n",
    "                    decoded_row[\"timestamp\"] = float(msg.timestamp)\n",
    "                    filas.append(decoded_row)\n",
    "                    break\n",
    "                except Exception:\n",
    "                    continue\n",
    "    if not filas:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.DataFrame(filas)\n",
    "    if \"timestamp\" in df.columns:\n",
    "        df = df.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "\n",
    "    # Remuestreo opcional por tiempo\n",
    "    if resample_ms and not df.empty and \"timestamp\" in df.columns:\n",
    "        try:\n",
    "            ts = pd.to_datetime(df[\"timestamp\"], unit=\"s\")\n",
    "            df = df.drop(columns=[c for c in df.columns if c == \"timestamp\"]).set_index(ts)\n",
    "            # Sólo numéricas para agregación\n",
    "            num_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]\n",
    "            df = df[num_cols]\n",
    "            df = df.resample(f\"{int(resample_ms)}ms\").mean().ffill().reset_index()\n",
    "            df.rename(columns={\"index\": \"timestamp\"}, inplace=True)\n",
    "            df[\"timestamp\"] = df[\"timestamp\"].astype(\"int64\") / 1e9\n",
    "        except Exception:\n",
    "            # Si falla el remuestreo, devuelve sin remuestrear\n",
    "            df = pd.DataFrame(filas)\n",
    "            if \"timestamp\" in df.columns:\n",
    "                df = df.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "996099a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnóstico rápido de reglas de eventos y sugerencias de umbrales\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, Optional, Tuple, List\n",
    "\n",
    "OPERADORES = {\n",
    "    '>':  lambda s, v: s > v,\n",
    "    '>=': lambda s, v: s >= v,\n",
    "    '<':  lambda s, v: s < v,\n",
    "    '<=': lambda s, v: s <= v,\n",
    "    '!=': lambda s, v: s != v,\n",
    "    '==': lambda s, v: s == v,\n",
    "}\n",
    "\n",
    "def diagnosticar_reglas_eventos(df: pd.DataFrame,\n",
    "                                reglas: Dict[str, Dict[str, float]],\n",
    "                                min_duracion: int = 15) -> None:\n",
    "    if df is None or df.empty:\n",
    "        print(\"DF vacío: no hay datos para diagnosticar.\")\n",
    "        return\n",
    "    print(f\"Filas: {len(df)} | Columnas: {len(df.columns)}\")\n",
    "    presentes = [c for c in reglas.keys() if c in df.columns]\n",
    "    ausentes = [c for c in reglas.keys() if c not in df.columns]\n",
    "    print(\"Reglas con columnas presentes:\", presentes or '<ninguna>')\n",
    "    if ausentes:\n",
    "        print(\"Reglas con columnas AUSENTES (no se aplican):\", ausentes)\n",
    "    mask_total = np.zeros(len(df), dtype=bool)\n",
    "    for col, spec in reglas.items():\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "        op = spec.get('op', '>')\n",
    "        val = spec.get('valor', 0.0)\n",
    "        serie = pd.to_numeric(df[col], errors='coerce').fillna(np.nan)\n",
    "        try:\n",
    "            m = OPERADORES.get(op, OPERADORES['>'])(serie, val)\n",
    "        except Exception:\n",
    "            m = OPERADORES['>'](serie, val)\n",
    "        true_count = int(np.nansum(m.values))\n",
    "        ratio = true_count / max(1, len(serie))\n",
    "        print(f\" - {col} {op} {val}: {true_count} filas verdaderas ({ratio:.1%})\")\n",
    "        mask_total |= m.fillna(False).values\n",
    "    total_true = int(mask_total.sum())\n",
    "    print(f\"Filas que cumplen AL MENOS una regla: {total_true} ({total_true/max(1,len(df)):.1%})\")\n",
    "    # Extraer segmentos\n",
    "    segs: List[Tuple[int,int]] = []\n",
    "    en_evento = False\n",
    "    ini = 0\n",
    "    for i, v in enumerate(mask_total):\n",
    "        if v and not en_evento:\n",
    "            en_evento = True\n",
    "            ini = i\n",
    "        elif not v and en_evento:\n",
    "            fin = i\n",
    "            if fin - ini >= min_duracion:\n",
    "                segs.append((ini, fin))\n",
    "            en_evento = False\n",
    "    if en_evento:\n",
    "        fin = len(mask_total)\n",
    "        if fin - ini >= min_duracion:\n",
    "            segs.append((ini, fin))\n",
    "    print(f\"Segmentos detectados (min_duracion={min_duracion} filas): {len(segs)}\")\n",
    "    if segs:\n",
    "        for k,(a,b) in enumerate(segs[:10],1):\n",
    "            print(f\"   [{k}] {a}-{b} (duración {b-a} filas)\")\n",
    "        if len(segs) > 10:\n",
    "            print(f\"   ... y {len(segs)-10} más\")\n",
    "\n",
    "\n",
    "def sugerir_reglas_basicas(df: pd.DataFrame) -> Dict[str, Dict[str, float]]:\n",
    "    \"\"\"Intenta adivinar columnas típicas y proponer umbrales iniciales.\n",
    "    Usa heurísticas por nombre y percentiles/escala.\n",
    "    \"\"\"\n",
    "    cols = list(df.columns)\n",
    "    def pick(cands: List[str]) -> Optional[str]:\n",
    "        for c in cols:\n",
    "            cl = c.lower()\n",
    "            if any(k in cl for k in cands):\n",
    "                return c\n",
    "        return None\n",
    "    col_speed   = pick(['velocidad', 'vehicle_speed', 'veh_speed', 'speed'])\n",
    "    col_brake   = pick(['freno', 'brake'])\n",
    "    col_torque  = pick(['torque'])\n",
    "    col_current = pick(['corriente', 'current', 'ibatt', 'battery_current'])\n",
    "\n",
    "    reglas: Dict[str, Dict[str, float]] = {}\n",
    "    def pctl(col, p):\n",
    "        s = pd.to_numeric(df[col], errors='coerce').dropna()\n",
    "        return float(np.percentile(s, p)) if len(s) else 0.0\n",
    "\n",
    "    if col_speed:\n",
    "        # Umbral en el percentil 60 para activar eventos de movimiento\n",
    "        reglas[col_speed] = {'op': '>', 'valor': round(pctl(col_speed, 60), 3)}\n",
    "    if col_brake:\n",
    "        s = pd.to_numeric(df[col_brake], errors='coerce').dropna()\n",
    "        if len(s):\n",
    "            mx = float(s.max())\n",
    "            # Si parece 0..1, umbral 0.2; si 0..100, umbral 10\n",
    "            umbral = 0.2 if mx <= 1.5 else 10.0\n",
    "        else:\n",
    "            umbral = 10.0\n",
    "        reglas[col_brake] = {'op': '>', 'valor': umbral}\n",
    "    if col_torque:\n",
    "        reglas[col_torque] = {'op': '>', 'valor': round(pctl(col_torque, 60), 3)}\n",
    "    if col_current:\n",
    "        # Considerar actividad cuando corriente != 0\n",
    "        reglas[col_current] = {'op': '!=', 'valor': 0.0}\n",
    "\n",
    "    return reglas\n",
    "\n",
    "# Ejemplo de uso (descomentando):\n",
    "# df_test = decodificar_blf_a_dataframe_stream_multi(PATHS_BLF[0], PATHS_DBC, max_mensajes=50000)\n",
    "# reglas_auto = sugerir_reglas_basicas(df_test)\n",
    "# print('Reglas sugeridas:', reglas_auto)\n",
    "# diagnosticar_reglas_eventos(df_test, reglas_auto, min_duracion=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505c212d",
   "metadata": {},
   "source": [
    "## Selección interactiva de archivos DBC/JSON y BLF\n",
    "\n",
    "En las siguientes celdas se seleccionan:\n",
    "- Uno o varios archivos DBC y/o un JSON de definiciones (usa uno o ambos según tu caso).\n",
    "- Uno o varios archivos BLF (o una carpeta con muchos BLF).\n",
    "- La ruta de salida del JSONL para cargar en el RAG de IBM.\n",
    "\n",
    "Si la ventana de selección no se abre en tu entorno, el flujo te pedirá las rutas por consola. Para múltiples rutas, ase deben separar con punto y coma (;)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c848125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.13.7 | SO: Windows | Tk version: 8.6.15\n",
      "Abriendo un diálogo de prueba... (puede aparecer detrás de otras ventanas)\n",
      "Selección: <cancelado>\n",
      "Selección: <cancelado>\n"
     ]
    }
   ],
   "source": [
    "# Prueba rápida de Tkinter (opcional) para verificar que los diálogos se pueden abrir en tu entorno\n",
    "# Ejecuta esta celda: debería abrir un cuadro de diálogo de archivo. Si no aparece, verás un diagnóstico.\n",
    "import sys\n",
    "import platform\n",
    "\n",
    "try:\n",
    "    import tkinter as tk\n",
    "    from tkinter import filedialog\n",
    "    _tk_loaded = True\n",
    "except Exception as e:\n",
    "    _tk_loaded = False\n",
    "    print(\"tkinter no está disponible en este intérprete.\")\n",
    "    print(\"Detalle:\", repr(e))\n",
    "\n",
    "if _tk_loaded:\n",
    "    try:\n",
    "        root = tk.Tk()\n",
    "        try:\n",
    "            root.withdraw()\n",
    "            # Forzar al frente\n",
    "            root.attributes('-topmost', True)\n",
    "            root.lift()\n",
    "            root.update()\n",
    "        except Exception:\n",
    "            pass\n",
    "        print(f\"Python: {sys.version.split()[0]} | SO: {platform.system()} | Tk version: {root.tk.call('info','patchlevel')}\")\n",
    "        print(\"Abriendo un diálogo de prueba... (puede aparecer detrás de otras ventanas)\")\n",
    "        try:\n",
    "            path = filedialog.askopenfilename(title=\"Prueba de diálogo de archivo\")\n",
    "            print(\"Selección:\", path or \"<cancelado>\")\n",
    "        finally:\n",
    "            try:\n",
    "                root.destroy()\n",
    "            except Exception:\n",
    "                pass\n",
    "    except Exception as e:\n",
    "        print(\"Fallo al abrir el diálogo de Tkinter.\")\n",
    "        print(\"Sugerencias: 1) Cambia de intérprete Python a uno con Tk instalado, 2) Ejecuta VS Code fuera de servidor remoto, 3) Usa el fallback por consola.\")\n",
    "        print(\"Detalle:\", repr(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b4fe47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecciona uno o más DBC y/o un JSON de definiciones (puedes dejar alguno vacío).\n",
      "Entorno: Python 3.13.7 | SO Windows | Tkinter: OK\n",
      "Selecciona uno o más archivos BLF\n",
      "Selecciona uno o más archivos BLF\n",
      "Selecciona ruta de salida JSONL\n",
      "Selecciona ruta de salida JSONL\n",
      "DBC seleccionados: 4\n",
      "JSON definiciones: <no especificado>\n",
      "BLF seleccionados: 4\n",
      "Salida JSONL: C:/Users/henry/OneDrive/Documentos/2. Formación/Maestria/Proyecto Integrador/Datos/pREUBA.jsonl\n",
      "DBC seleccionados: 4\n",
      "JSON definiciones: <no especificado>\n",
      "BLF seleccionados: 4\n",
      "Salida JSONL: C:/Users/henry/OneDrive/Documentos/2. Formación/Maestria/Proyecto Integrador/Datos/pREUBA.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Utilidades para seleccionar archivos con diálogo (tkinter) o por consola\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import platform\n",
    "\n",
    "# Intentar usar diálogo gráfico\n",
    "try:\n",
    "    import tkinter as tk\n",
    "    from tkinter import filedialog\n",
    "    _TK_OK = True\n",
    "except Exception:\n",
    "    _TK_OK = False\n",
    "\n",
    "\n",
    "def _with_root():\n",
    "    \"\"\"Crea una raíz Tk configurada para aparecer en primer plano.\"\"\"\n",
    "    root = tk.Tk()\n",
    "    try:\n",
    "        root.withdraw()\n",
    "        root.attributes('-topmost', True)\n",
    "        root.lift()\n",
    "        root.update()\n",
    "    except Exception:\n",
    "        pass\n",
    "    return root\n",
    "\n",
    "\n",
    "def _select_file_dialog(title: str, filetypes=(('All files', '*.*'),)) -> str:\n",
    "    if not _TK_OK:\n",
    "        print(\"tkinter no disponible; usando entrada por consola.\")\n",
    "        return \"\"\n",
    "    try:\n",
    "        root = _with_root()\n",
    "        path = filedialog.askopenfilename(title=title, filetypes=filetypes)\n",
    "        try:\n",
    "            root.destroy()\n",
    "        except Exception:\n",
    "            pass\n",
    "        return path or \"\"\n",
    "    except Exception as e:\n",
    "        print(\"No se pudo abrir el diálogo (Tkinter). Detalle:\", repr(e))\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def _select_files_dialog(title: str, filetypes=(('All files', '*.*'),)) -> list:\n",
    "    if not _TK_OK:\n",
    "        print(\"tkinter no disponible; usando entrada por consola.\")\n",
    "        return []\n",
    "    try:\n",
    "        root = _with_root()\n",
    "        paths = filedialog.askopenfilenames(title=title, filetypes=filetypes)\n",
    "        try:\n",
    "            root.destroy()\n",
    "        except Exception:\n",
    "            pass\n",
    "        return list(paths) if paths else []\n",
    "    except Exception as e:\n",
    "        print(\"No se pudo abrir el diálogo múltiple (Tkinter). Detalle:\", repr(e))\n",
    "        return []\n",
    "\n",
    "\n",
    "def _select_folder_dialog(title: str) -> str:\n",
    "    if not _TK_OK:\n",
    "        print(\"tkinter no disponible; usando entrada por consola.\")\n",
    "        return \"\"\n",
    "    try:\n",
    "        root = _with_root()\n",
    "        path = filedialog.askdirectory(title=title)\n",
    "        try:\n",
    "            root.destroy()\n",
    "        except Exception:\n",
    "            pass\n",
    "        return path or \"\"\n",
    "    except Exception as e:\n",
    "        print(\"No se pudo abrir el diálogo de carpeta (Tkinter). Detalle:\", repr(e))\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def _save_file_dialog(title: str, defaultextension='.jsonl', filetypes=(('JSONL', '*.jsonl'), ('All files', '*.*'))) -> str:\n",
    "    if not _TK_OK:\n",
    "        print(\"tkinter no disponible; usando entrada por consola.\")\n",
    "        return \"\"\n",
    "    try:\n",
    "        root = _with_root()\n",
    "        path = filedialog.asksaveasfilename(title=title, defaultextension=defaultextension, filetypes=filetypes)\n",
    "        try:\n",
    "            root.destroy()\n",
    "        except Exception:\n",
    "            pass\n",
    "        return path or \"\"\n",
    "    except Exception as e:\n",
    "        print(\"No se pudo abrir el diálogo de guardado (Tkinter). Detalle:\", repr(e))\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def _buscar_por_extension(base: Path, patron: str) -> list:\n",
    "    try:\n",
    "        return [str(p.resolve()) for p in base.rglob(patron)]\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "\n",
    "def solicitar_rutas_interactivas():\n",
    "    print(\"Selecciona uno o más DBC y/o un JSON de definiciones (puedes dejar alguno vacío).\")\n",
    "    print(f\"Entorno: Python {sys.version.split()[0]} | SO {platform.system()} | Tkinter: {'OK' if _TK_OK else 'NO'}\")\n",
    "\n",
    "    # 1) Selección múltiple de DBC por diálogo\n",
    "    paths_dbc = _select_files_dialog(\"Selecciona archivos DBC\", filetypes=((\"DBC\", \"*.dbc\"), (\"All files\", \"*.*\")))\n",
    "\n",
    "    # 2) Si no hay selección, intentar carpeta con DBC\n",
    "    if not paths_dbc:\n",
    "        folder_dbc = _select_folder_dialog(\"Selecciona carpeta con DBC\")\n",
    "        if folder_dbc:\n",
    "            paths_dbc = [str(p.resolve()) for p in Path(folder_dbc).rglob(\"*.dbc\")]\n",
    "\n",
    "    # 3) Fallback por texto (pero sin input() si el frontend no lo soporta)\n",
    "    paths_dbc = paths_dbc or []\n",
    "\n",
    "    # Normalizar y quitar duplicados\n",
    "    norm_dbc = []\n",
    "    seen_dbc = set()\n",
    "    for p in paths_dbc:\n",
    "        q = str(Path(p).resolve())\n",
    "        if q not in seen_dbc:\n",
    "            seen_dbc.add(q)\n",
    "            norm_dbc.append(q)\n",
    "\n",
    "    # JSON opcional\n",
    "    path_json_def = _select_file_dialog(\"Selecciona JSON de definiciones (opcional)\", filetypes=((\"JSON\", \"*.json\"), (\"All files\", \"*.*\")))\n",
    "    path_json_def = path_json_def or \"\"\n",
    "\n",
    "    print(\"Selecciona uno o más archivos BLF\")\n",
    "    # 1) Intentar selección múltiple por diálogo\n",
    "    paths_blf = _select_files_dialog(\"Selecciona archivos BLF\", filetypes=((\"BLF\", \"*.blf\"), (\"All files\", \"*.*\")))\n",
    "\n",
    "    # 2) Si no hay selección, intentar seleccionar carpeta y buscar *.blf recursivamente\n",
    "    if not paths_blf:\n",
    "        folder = _select_folder_dialog(\"Selecciona carpeta con BLF\")\n",
    "        if folder:\n",
    "            paths_blf = [str(p.resolve()) for p in Path(folder).rglob(\"*.blf\")]\n",
    "\n",
    "    # 3) Sin input(): si sigue vacío, el usuario puede usar la celda de widgets de abajo\n",
    "    paths_blf = paths_blf or []\n",
    "\n",
    "    # Normalizar y quitar duplicados conservando orden\n",
    "    norm_blf = []\n",
    "    seen = set()\n",
    "    for p in paths_blf:\n",
    "        q = str(Path(p).resolve())\n",
    "        if q not in seen:\n",
    "            seen.add(q)\n",
    "            norm_blf.append(q)\n",
    "\n",
    "    print(\"Selecciona ruta de salida JSONL\")\n",
    "    output_jsonl = _save_file_dialog(\"Guardar dataset JSONL\", defaultextension=\".jsonl\") or \"\"\n",
    "\n",
    "    return norm_dbc, path_json_def, norm_blf, output_jsonl\n",
    "\n",
    "\n",
    "# Ejecuta para capturar rutas (si el frontend no soporta input(), no bloqueará)\n",
    "try:\n",
    "    PATHS_DBC, PATH_JSON_DEF, PATHS_BLF, OUTPUT_JSONL = solicitar_rutas_interactivas()\n",
    "except Exception as e:\n",
    "    print(\"Selección por diálogos no disponible en este entorno. Usa la celda de widgets de abajo.\")\n",
    "    print(\"Detalle:\", repr(e))\n",
    "    PATHS_DBC, PATH_JSON_DEF, PATHS_BLF, OUTPUT_JSONL = [], \"\", [], \"\"\n",
    "\n",
    "print(\"DBC seleccionados:\", len(PATHS_DBC))\n",
    "print(\"JSON definiciones:\", PATH_JSON_DEF or \"<no especificado>\")\n",
    "print(\"BLF seleccionados:\", len(PATHS_BLF))\n",
    "print(\"Salida JSONL:\", OUTPUT_JSONL or \"<no especificado>\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30263728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selector por widgets (no bloquea y evita input()): usa este formulario si no aparecen ventanas\n",
    "import os\n",
    "from pathlib import Path\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Widgets\n",
    "w_dbc_text = widgets.Text(\n",
    "    value=';',\n",
    "    placeholder='Rutas DBC separadas por ; (opcional)',\n",
    "    description='DBCs:',\n",
    "    layout=widgets.Layout(width='95%')\n",
    ")\n",
    "w_json_text = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Ruta JSON de definiciones (opcional)',\n",
    "    description='JSON:',\n",
    "    layout=widgets.Layout(width='95%')\n",
    ")\n",
    "w_blf_text = widgets.Text(\n",
    "    value=';',\n",
    "    placeholder='Rutas BLF separadas por ;',\n",
    "    description='BLFs:',\n",
    "    layout=widgets.Layout(width='95%')\n",
    ")\n",
    "w_out_text = widgets.Text(\n",
    "    value=str(Path.cwd() / 'dataset_rag_decode_ev.jsonl'),\n",
    "    placeholder='Ruta de salida JSONL',\n",
    "    description='Salida:',\n",
    "    layout=widgets.Layout(width='95%')\n",
    ")\n",
    "w_btn_scan_dbc = widgets.Button(description='Buscar DBC en workspace', button_style='')\n",
    "w_btn_scan_blf = widgets.Button(description='Buscar BLF en workspace', button_style='')\n",
    "w_btn_set = widgets.Button(description='Usar valores', button_style='success')\n",
    "w_status = widgets.HTML(value='')\n",
    "\n",
    "box = widgets.VBox([\n",
    "    widgets.HTML('<b>Formulario de selección (widgets)</b> — Pega rutas o usa los botones de búsqueda.'),\n",
    "    widgets.HBox([w_dbc_text, w_btn_scan_dbc]),\n",
    "    w_json_text,\n",
    "    widgets.HBox([w_blf_text, w_btn_scan_blf]),\n",
    "    w_out_text,\n",
    "    w_btn_set,\n",
    "    w_status\n",
    "])\n",
    "\n",
    "def _scan(pattern: str, max_items=20):\n",
    "    try:\n",
    "        found = [str(p.resolve()) for p in Path.cwd().rglob(pattern)]\n",
    "        return found[:max_items]\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "\n",
    "def _uniq_split(val: str):\n",
    "    parts = [s.strip() for s in val.split(';') if s.strip()]\n",
    "    out, seen = [], set()\n",
    "    for p in parts:\n",
    "        q = str(Path(p).resolve())\n",
    "        if q not in seen:\n",
    "            seen.add(q)\n",
    "            out.append(q)\n",
    "    return out\n",
    "\n",
    "\n",
    "def _on_scan_dbc(_):\n",
    "    found = _scan('*.dbc')\n",
    "    if found:\n",
    "        w_dbc_text.value = ';'.join(found)\n",
    "        w_status.value = f'<span style=\"color:green\">Sugeridos {len(found)} DBC</span>'\n",
    "    else:\n",
    "        w_status.value = '<span style=\"color:red\">No se encontraron DBC en el workspace</span>'\n",
    "\n",
    "\n",
    "def _on_scan_blf(_):\n",
    "    found = _scan('*.blf', max_items=30)\n",
    "    if found:\n",
    "        w_blf_text.value = ';'.join(found)\n",
    "        w_status.value = f'<span style=\"color:green\">Sugeridos {len(found)} BLF</span>'\n",
    "    else:\n",
    "        w_status.value = '<span style=\"color:red\">No se encontraron BLF en el workspace</span>'\n",
    "\n",
    "\n",
    "def _on_set(_):\n",
    "    global PATHS_DBC, PATH_JSON_DEF, PATHS_BLF, OUTPUT_JSONL\n",
    "    PATHS_DBC = _uniq_split(w_dbc_text.value)\n",
    "    PATH_JSON_DEF = w_json_text.value.strip()\n",
    "    PATHS_BLF = _uniq_split(w_blf_text.value)\n",
    "    OUTPUT_JSONL = w_out_text.value.strip()\n",
    "    w_status.value = (\n",
    "        f'<b>Asignado</b>: DBC={len(PATHS_DBC)} | JSON={PATH_JSON_DEF or \"<no>\"} | BLF={len(PATHS_BLF)} | Salida={OUTPUT_JSONL or \"<no>\"}'\n",
    "    )\n",
    "\n",
    "w_btn_scan_dbc.on_click(_on_scan_dbc)\n",
    "w_btn_scan_blf.on_click(_on_scan_blf)\n",
    "w_btn_set.on_click(_on_set)\n",
    "\n",
    "display(box)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5653e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores actuales:\n",
      "  DBCs: ['C:\\\\Users\\\\henry\\\\OneDrive\\\\Documentos\\\\2. Formación\\\\Maestria\\\\Proyecto Integrador\\\\Datos\\\\IP_JZ - AUX_CHG.dbc', 'C:\\\\Users\\\\henry\\\\OneDrive\\\\Documentos\\\\2. Formación\\\\Maestria\\\\Proyecto Integrador\\\\Datos\\\\IP_JZ - CAN CARROC.DBC', 'C:\\\\Users\\\\henry\\\\OneDrive\\\\Documentos\\\\2. Formación\\\\Maestria\\\\Proyecto Integrador\\\\Datos\\\\IP_JZ - CAN CATL.dbc', 'C:\\\\Users\\\\henry\\\\OneDrive\\\\Documentos\\\\2. Formación\\\\Maestria\\\\Proyecto Integrador\\\\Datos\\\\IP_JZ - CAN EV.DBC']\n",
      "  JSON definiciones: <no>\n",
      "  BLFs: ['C:\\\\Users\\\\henry\\\\OneDrive\\\\Documentos\\\\2. Formación\\\\Maestria\\\\Proyecto Integrador\\\\Datos\\\\Logs_Recorrido2_19092025\\\\Logging_2025-09-19_07-07-52.blf', 'C:\\\\Users\\\\henry\\\\OneDrive\\\\Documentos\\\\2. Formación\\\\Maestria\\\\Proyecto Integrador\\\\Datos\\\\Logs_Recorrido2_19092025\\\\Logging_2025-09-19_07-25-15.blf', 'C:\\\\Users\\\\henry\\\\OneDrive\\\\Documentos\\\\2. Formación\\\\Maestria\\\\Proyecto Integrador\\\\Datos\\\\Logs_Recorrido2_19092025\\\\Logging_2025-09-19_07-27-46.blf', 'C:\\\\Users\\\\henry\\\\OneDrive\\\\Documentos\\\\2. Formación\\\\Maestria\\\\Proyecto Integrador\\\\Datos\\\\Logs_Recorrido2_19092025\\\\Logging_2025-09-19_08-12-51.blf', 'C:\\\\Users\\\\henry\\\\OneDrive\\\\Documentos\\\\2. Formación\\\\Maestria\\\\Proyecto Integrador\\\\Datos\\\\Logs_Recorrido2_19092025\\\\Logging_2025-09-19_08-50-30.blf', 'C:\\\\Users\\\\henry\\\\OneDrive\\\\Documentos\\\\2. Formación\\\\Maestria\\\\Proyecto Integrador\\\\Datos\\\\Logs_Recorrido2_19092025\\\\Logging_2025-09-19_09-22-22.blf', 'C:\\\\Users\\\\henry\\\\OneDrive\\\\Documentos\\\\2. Formación\\\\Maestria\\\\Proyecto Integrador\\\\Datos\\\\Logs_Recorrido2_19092025\\\\Logging_2025-09-19_10-07-32.blf', 'C:\\\\Users\\\\henry\\\\OneDrive\\\\Documentos\\\\2. Formación\\\\Maestria\\\\Proyecto Integrador\\\\Datos\\\\Logs_Recorrido2_19092025\\\\Logging_2025-09-19_10-37-05.blf', 'C:\\\\Users\\\\henry\\\\OneDrive\\\\Documentos\\\\2. Formación\\\\Maestria\\\\Proyecto Integrador\\\\Datos\\\\Logs_Recorrido2_19092025\\\\Logging_2025-09-19_11-07-51.blf']\n",
      "  Salida JSONL: C:/Users/henry/OneDrive/Documentos/2. Formación/Maestria/Proyecto Integrador/Datos/Salida_Optimizada.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Asignación directa de rutas (opcional)\n",
    "# Si las variables siguen vacías, puedes asignarlas aquí manualmente y ejecutar esta celda.\n",
    "try:\n",
    "    PATHS_DBC\n",
    "except NameError:\n",
    "    PATHS_DBC = []\n",
    "try:\n",
    "    PATHS_BLF\n",
    "except NameError:\n",
    "    PATHS_BLF = []\n",
    "try:\n",
    "    PATH_JSON_DEF\n",
    "except NameError:\n",
    "    PATH_JSON_DEF = \"\"\n",
    "try:\n",
    "    OUTPUT_JSONL\n",
    "except NameError:\n",
    "    OUTPUT_JSONL = \"\"\n",
    "\n",
    "print(\"Valores actuales:\")\n",
    "print(\"  DBCs:\", PATHS_DBC)\n",
    "print(\"  JSON definiciones:\", PATH_JSON_DEF or \"<no>\")\n",
    "print(\"  BLFs:\", PATHS_BLF)\n",
    "print(\"  Salida JSONL:\", OUTPUT_JSONL or \"<no>\")\n",
    "\n",
    "# Ejemplos (descomenta y ajusta):\n",
    "# PATHS_DBC = [r\"C:\\\\ruta\\\\a\\\\red1.dbc\", r\"C:\\\\ruta\\\\a\\\\red2.dbc\"]\n",
    "# PATHS_BLF = [r\"C:\\\\ruta\\\\a\\\\log1.blf\", r\"C:\\\\ruta\\\\a\\\\log2.blf\"]\n",
    "# PATH_JSON_DEF = r\"C:\\\\ruta\\\\a\\\\definiciones.json\"  # opcional\n",
    "# OUTPUT_JSONL = r\"C:\\\\ruta\\\\salida\\\\dataset_rag_decode_ev.jsonl\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "97f9afe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reglas auto-generadas: {'WhBasedVehSpeed': {'op': '>', 'valor': 11.851}, 'Brake_Application_Pressure': {'op': '>', 'valor': 0.2}, 'EngTorqueMode_SPN899': {'op': '>', 'valor': 0.0}, 'Param_MaxCurrentPerTray': {'op': '>', 'valor': 0.0}}\n",
      "C:\\Users\\henry\\OneDrive\\Documentos\\2. Formación\\Maestria\\Proyecto Integrador\\Datos\\Logs_Recorrido2_19092025\\Logging_2025-09-19_07-07-52.blf: 0 eventos\n",
      "C:\\Users\\henry\\OneDrive\\Documentos\\2. Formación\\Maestria\\Proyecto Integrador\\Datos\\Logs_Recorrido2_19092025\\Logging_2025-09-19_07-25-15.blf: 0 eventos\n",
      "C:\\Users\\henry\\OneDrive\\Documentos\\2. Formación\\Maestria\\Proyecto Integrador\\Datos\\Logs_Recorrido2_19092025\\Logging_2025-09-19_07-25-15.blf: 0 eventos\n",
      "Error procesando C:\\Users\\henry\\OneDrive\\Documentos\\2. Formación\\Maestria\\Proyecto Integrador\\Datos\\Logs_Recorrido2_19092025\\Logging_2025-09-19_07-27-46.blf: Unable to allocate 9.76 GiB for an array with shape (470, 2787399) and data type float64\n",
      "Error procesando C:\\Users\\henry\\OneDrive\\Documentos\\2. Formación\\Maestria\\Proyecto Integrador\\Datos\\Logs_Recorrido2_19092025\\Logging_2025-09-19_07-27-46.blf: Unable to allocate 9.76 GiB for an array with shape (470, 2787399) and data type float64\n",
      "Error procesando C:\\Users\\henry\\OneDrive\\Documentos\\2. Formación\\Maestria\\Proyecto Integrador\\Datos\\Logs_Recorrido2_19092025\\Logging_2025-09-19_08-12-51.blf: Unable to allocate 3.77 GiB for an array with shape (219, 2309840) and data type object\n",
      "No se generaron entradas. Revisa reglas auto-generadas/umbrales de eventos o la cobertura del DBC.\n",
      "Error procesando C:\\Users\\henry\\OneDrive\\Documentos\\2. Formación\\Maestria\\Proyecto Integrador\\Datos\\Logs_Recorrido2_19092025\\Logging_2025-09-19_08-12-51.blf: Unable to allocate 3.77 GiB for an array with shape (219, 2309840) and data type object\n",
      "No se generaron entradas. Revisa reglas auto-generadas/umbrales de eventos o la cobertura del DBC.\n"
     ]
    }
   ],
   "source": [
    "# Orquestación: decodificar BLF(s), construir dataset y exportar JSONL\n",
    "\n",
    "# Aseguramos dependencias mínimas en esta celda\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Variables de configuración rápida\n",
    "RED_CAN = \"CAN_1\"\n",
    "TOP_N_SENALES = 5\n",
    "MIN_DURACION_EVENTO = 10  # filas\n",
    "\n",
    "# Cargar definiciones combinadas (múltiples DBC + JSON opcional)\n",
    "if PATHS_DBC or PATH_JSON_DEF:\n",
    "    defs = cargar_definiciones_combinadas(paths_dbc=PATHS_DBC or [], path_json_def=PATH_JSON_DEF or None)\n",
    "else:\n",
    "    defs = {}\n",
    "\n",
    "if PATHS_BLF and not PATHS_DBC:\n",
    "    print(\"Advertencia: Para decodificar BLF se requiere al menos un DBC. Suministra PATHS_DBC o decodifica fuera del notebook.\")\n",
    "\n",
    "entries_totales = []\n",
    "detection_rules = None\n",
    "\n",
    "for idx_blf, path_blf in enumerate(PATHS_BLF, start=1):\n",
    "    try:\n",
    "        if PATHS_DBC:\n",
    "            df = decodificar_blf_a_dataframe_stream_multi(path_blf=path_blf, paths_dbc=PATHS_DBC)\n",
    "        else:\n",
    "            df = pd.DataFrame()\n",
    "        if df.empty:\n",
    "            print(f\"BLF vacío o no decodificable: {path_blf}\")\n",
    "            continue\n",
    "\n",
    "        # Construcción automática de reglas (solo una vez, usando el primer DF no vacío)\n",
    "        if detection_rules is None:\n",
    "            detection_rules = construir_reglas_automaticas(df, defs)\n",
    "            print(\"Reglas auto-generadas:\", detection_rules)\n",
    "            if not detection_rules:\n",
    "                print(\"No se pudieron inferir reglas desde DBC/DF; se usarán reglas por defecto internas.\")\n",
    "\n",
    "        constructor = ConstructorDatasetRAG(\n",
    "            definiciones_dbc=defs,\n",
    "            red_can=RED_CAN,\n",
    "            top_n_senales=TOP_N_SENALES,\n",
    "            detection_rules=detection_rules,\n",
    "            min_duracion_evento=MIN_DURACION_EVENTO,\n",
    "        )\n",
    "\n",
    "        # Construir entradas por eventos\n",
    "        entries = constructor.construir_dataset_completo(df, fuente=path_blf)\n",
    "        print(f\"{path_blf}: {len(entries)} eventos\")\n",
    "        entries_totales.extend(entries)\n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando {path_blf}: {e}\")\n",
    "\n",
    "# Escribir JSONL\n",
    "if entries_totales:\n",
    "    os.makedirs(os.path.dirname(OUTPUT_JSONL), exist_ok=True)\n",
    "    escribir_jsonl(entries_totales, OUTPUT_JSONL)\n",
    "    print(f\"Dataset JSONL escrito en: {OUTPUT_JSONL} ({len(entries_totales)} eventos totales)\")\n",
    "else:\n",
    "    print(\"No se generaron entradas. Revisa reglas auto-generadas/umbrales de eventos o la cobertura del DBC.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd480b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilidades de ajuste automático: duración mínima en filas y selección heurística de señales desde DBC\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Set\n",
    "\n",
    "def calcular_min_duracion_filas(df: pd.DataFrame, segundos: float = 2.0) -> int:\n",
    "    \"\"\"Convierte una duración en segundos a filas según la mediana de delta timestamp.\n",
    "    Devuelve al menos 3 filas para evitar falsos positivos.\n",
    "    \"\"\"\n",
    "    if df is None or df.empty or \"timestamp\" not in df.columns:\n",
    "        return max(3, int(segundos * 10))  # suponer ~10 Hz si no hay timestamp\n",
    "    ts = pd.to_numeric(df[\"timestamp\"], errors=\"coerce\").dropna().values\n",
    "    if len(ts) < 3:\n",
    "        return max(3, int(segundos * 10))\n",
    "    d = np.diff(ts)\n",
    "    d = d[(d > 0) & np.isfinite(d)]\n",
    "    if len(d) == 0:\n",
    "        return max(3, int(segundos * 10))\n",
    "    med = float(np.median(d))\n",
    "    if med <= 0 or not np.isfinite(med):\n",
    "        return max(3, int(segundos * 10))\n",
    "    filas = int(np.ceil(segundos / med))\n",
    "    return max(3, filas)\n",
    "\n",
    "\n",
    "def senales_interes_desde_defs(defs: Dict[str, Dict[str, Dict]]) -> Set[str]:\n",
    "    \"\"\"Construye una allowlist de señales típicas a partir de nombres en defs (normalizados).\n",
    "    Palabras clave: velocidad, brake/freno, torque, corriente, pedal, acelerador, rpm, soc, voltaje.\n",
    "    Limita a 40 señales para mantener memoria controlada.\n",
    "    \"\"\"\n",
    "    if not defs:\n",
    "        return set()\n",
    "    keywords = [\n",
    "        \"velocidad\", \"speed\", \"veh_speed\", \"vehicle_speed\",\n",
    "        \"brake\", \"freno\",\n",
    "        \"torque\",\n",
    "        \"corriente\", \"current\", \"ibatt\", \"battery\",\n",
    "        \"pedal\", \"acelerador\", \"throttle\",\n",
    "        \"rpm\", \"engine_speed\",\n",
    "        \"soc\", \"state_of_charge\",\n",
    "        \"volt\", \"voltage\",\n",
    "    ]\n",
    "    out: List[str] = []\n",
    "    for _, sigs in defs.items():\n",
    "        for raw_name in sigs.keys():\n",
    "            n = normalizar_nombre_senal(raw_name)\n",
    "            l = n.lower()\n",
    "            if any(k in l for k in keywords):\n",
    "                out.append(n)\n",
    "    # Quitar duplicados conservando orden\n",
    "    seen = set()\n",
    "    uniq = []\n",
    "    for s in out:\n",
    "        if s not in seen:\n",
    "            seen.add(s)\n",
    "            uniq.append(s)\n",
    "    return set(uniq[:40])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623a6946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Orquestación (robusta y memory-safe): filtrado por señales, remuestreo y duración adaptativa\n",
    "import os\n",
    "import pandas as pd\n",
    "from typing import Optional, Set\n",
    "\n",
    "# Configuración rápida\n",
    "RED_CAN = globals().get(\"RED_CAN\", \"CAN_1\")\n",
    "TOP_N_SENALES = int(globals().get(\"TOP_N_SENALES\", 5))\n",
    "RESAMPLE_MS = 100                 # remuestreo a 10 Hz aprox\n",
    "MAX_MENSAJES_SAMPLE = 200_000     # para inferir reglas sin agotar memoria\n",
    "MAX_MENSAJES_FULL: Optional[int] = None  # o fija, p.ej. 1_500_000\n",
    "MIN_DURACION_S = 2.0              # duración mínima de evento en segundos\n",
    "\n",
    "# Cargar definiciones combinadas si no existen\n",
    "if 'defs' not in globals() or not isinstance(defs, dict) or not defs:\n",
    "    try:\n",
    "        defs = cargar_definiciones_combinadas(paths_dbc=PATHS_DBC or [], path_json_def=PATH_JSON_DEF or None)\n",
    "    except Exception:\n",
    "        defs = {}\n",
    "\n",
    "if not PATHS_BLF:\n",
    "    print(\"No hay BLF para procesar. Revisa PATHS_BLF (usa la celda de selección anterior).\")\n",
    "else:\n",
    "    # 1) Construir allowlist heurística a partir del DBC\n",
    "    allow_heur: Set[str] = senales_interes_desde_defs(defs)\n",
    "    if allow_heur:\n",
    "        print(f\"Allowlist heurística desde DBC: {len(allow_heur)} señales\")\n",
    "    else:\n",
    "        print(\"No se pudo derivar allowlist desde DBC; se continuará sin filtro en la muestra (riesgo de memoria).\")\n",
    "\n",
    "    # 2) Decodificar una muestra del primer BLF con filtro para proponer reglas\n",
    "    reglas_auto = None\n",
    "    df_sample = pd.DataFrame()\n",
    "    try:\n",
    "        df_sample = decodificar_blf_a_dataframe_stream_multi_filtrado(\n",
    "            path_blf=PATHS_BLF[0],\n",
    "            paths_dbc=PATHS_DBC,\n",
    "            signals_allow=allow_heur if allow_heur else None,\n",
    "            max_mensajes=MAX_MENSAJES_SAMPLE,\n",
    "            resample_ms=RESAMPLE_MS,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\"Error al decodificar muestra:\", e)\n",
    "\n",
    "    if not df_sample.empty:\n",
    "        try:\n",
    "            reglas_auto = sugerir_reglas_basicas(df_sample)\n",
    "            print(\"Reglas auto-generadas:\", reglas_auto)\n",
    "            diagnosticar_reglas_eventos(df_sample, reglas_auto, min_duracion=10)\n",
    "        except Exception as e:\n",
    "            print(\"No se pudieron sugerir/diagnosticar reglas:\", e)\n",
    "    else:\n",
    "        print(\"Muestra vacía; no se pudieron sugerir reglas. Se usará un conjunto básico.\")\n",
    "        reglas_auto = reglas_auto or {}\n",
    "\n",
    "    # 3) Señales finales a conservar (unión reglas + heurística)\n",
    "    allow_final: Set[str] = set(reglas_auto.keys()) | allow_heur\n",
    "    if allow_final:\n",
    "        print(f\"Allowlist final: {len(allow_final)} señales\")\n",
    "\n",
    "    # 4) Procesar todos los BLF con filtrado, remuestreo y duración adaptativa\n",
    "    entries_totales = []\n",
    "    for path_blf in PATHS_BLF:\n",
    "        try:\n",
    "            df = decodificar_blf_a_dataframe_stream_multi_filtrado(\n",
    "                path_blf=path_blf,\n",
    "                paths_dbc=PATHS_DBC,\n",
    "                signals_allow=allow_final if allow_final else None,\n",
    "                max_mensajes=MAX_MENSAJES_FULL,\n",
    "                resample_ms=RESAMPLE_MS,\n",
    "            )\n",
    "            if df.empty:\n",
    "                print(f\"BLF vacío o no decodificable (filtrado): {path_blf}\")\n",
    "                continue\n",
    "\n",
    "            min_filas = calcular_min_duracion_filas(df, segundos=MIN_DURACION_S)\n",
    "            constructor = ConstructorDatasetRAG(\n",
    "                definiciones_dbc=defs,\n",
    "                red_can=RED_CAN,\n",
    "                top_n_senales=TOP_N_SENALES,\n",
    "                detection_rules=reglas_auto if reglas_auto else None,\n",
    "                min_duracion_evento=min_filas,\n",
    "            )\n",
    "            entries = constructor.construir_dataset_completo(df, fuente=path_blf)\n",
    "            print(f\"{path_blf}: {len(entries)} eventos (min {min_filas} filas)\")\n",
    "            entries_totales.extend(entries)\n",
    "        except Exception as e:\n",
    "            print(f\"Error procesando {path_blf}: {e}\")\n",
    "\n",
    "    # 5) Guardar JSONL\n",
    "    if entries_totales:\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(OUTPUT_JSONL), exist_ok=True)\n",
    "            escribir_jsonl(entries_totales, OUTPUT_JSONL)\n",
    "            print(f\"Dataset JSONL escrito en: {OUTPUT_JSONL} ({len(entries_totales)} eventos totales)\")\n",
    "        except Exception as e:\n",
    "            print(\"No fue posible escribir el JSONL:\", e)\n",
    "    else:\n",
    "        print(\"No se generaron entradas. Revisa las reglas, la allowlist y el remuestreo.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
