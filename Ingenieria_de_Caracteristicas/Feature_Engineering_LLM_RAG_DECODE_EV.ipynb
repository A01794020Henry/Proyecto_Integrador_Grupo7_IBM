{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c50038a4",
   "metadata": {},
   "source": [
    "# DECODE-EV: Arquitectura de Ingeniería de Características para Ecosistemas LLM/RAG Vehiculares\n",
    "\n",
    "## Proyecto Integrador - Grupo 7 IBM Watson\n",
    "### Maestría en Inteligencia Artificial Aplicada - Instituto Tecnológico de Monterrey\n",
    "\n",
    "---\n",
    "\n",
    "## Marco Conceptual y Fundamentación Teórica\n",
    "\n",
    "El presente desarrollo técnico constituye una implementación integral de **transformación de datos vehiculares CAN** hacia representaciones semánticas compatibles con arquitecturas **Large Language Model (LLM)** y sistemas **Retrieval-Augmented Generation (RAG)**. Esta aproximación metodológica representa un cambio paradigmático fundamental en el procesamiento de datos automotrices, migrando desde técnicas tradicionales de ingeniería de características hacia metodologías de enriquecimiento semántico y contextualización dominio-específica.\n",
    "\n",
    "### Objetivos Estratégicos del Sistema\n",
    "\n",
    "**Objetivo Principal:** Desarrollar un pipeline de transformación semántica que convierta señales numéricas del protocolo CAN (Controller Area Network) en representaciones textuales enriquecidas, facilitando la interpretación de eventos vehiculares mediante interfaces conversacionales basadas en procesamiento de lenguaje natural.\n",
    "\n",
    "**Objetivos Específicos:**\n",
    "1. **Transformación Semántica:** Generar descripciones textuales técnicamente precisas de señales CAN mediante técnicas de generación controlada\n",
    "2. **Enriquecimiento Contextual:** Crear metadatos estructurados que preserven información técnica crítica para sistemas RAG\n",
    "3. **Construcción de Base de Conocimiento:** Desarrollar un corpus documental especializado en el dominio vehicular eléctrico\n",
    "4. **Optimización RAG:** Generar dataset compatible con arquitecturas de recuperación-generación para consultas técnicas especializadas\n",
    "\n",
    "### Innovación Metodológica: Cambio de Paradigma\n",
    "\n",
    "La metodología implementada representa una evolución fundamental en el tratamiento de datos vehiculares:\n",
    "\n",
    "**Paradigma Tradicional (ML Clásico):**\n",
    "- Extracción de características numéricas estadísticas\n",
    "- Transformaciones matemáticas para optimización algorítmica\n",
    "- Enfoque en precisión predictiva cuantitativa\n",
    "\n",
    "**Paradigma Propuesto (LLM/RAG):**\n",
    "- Generación de representaciones semánticas contextualizadas\n",
    "- Preservación de conocimiento técnico dominio-específico\n",
    "- Enfoque en interpretabilidad y accesibilidad conversacional\n",
    "\n",
    "### Contexto de Datos y Complejidad del Dominio\n",
    "\n",
    "**Base de Datos CAN Analizada:**\n",
    "- **CAN_EV:** 1,957 señales vehiculares (30% con documentación técnica disponible)\n",
    "- **CAN_CATL:** 162 señales del sistema de batería (0% documentación - \"caja negra\" propietaria)\n",
    "- **CAN_CARROC:** Sistema de control de carrocería y puertas\n",
    "- **AUX_CHG:** Subsistema de carga y gestión energética\n",
    "\n",
    "**Desafíos Técnicos Identificados:**\n",
    "1. **Heterogeneidad semántica** entre subsistemas vehiculares\n",
    "2. **Ausencia de documentación** en componentes propietarios\n",
    "3. **Variabilidad temporal** en patrones de señales CAN\n",
    "4. **Complejidad de interpretación** para usuarios no técnicos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3f9f13",
   "metadata": {},
   "source": [
    "## Metodología de Desarrollo: Marco Teórico CRISP-ML Adaptado\n",
    "\n",
    "### Fundamentación Metodológica\n",
    "\n",
    "La metodología empleada se fundamenta en una adaptación especializada del marco **CRISP-ML (Cross Industry Standard Process for Machine Learning)**, específicamente reinterpretado para sistemas basados en **Large Language Models** y arquitecturas **RAG**. Esta adaptación reconoce las diferencias fundamentales entre el desarrollo de sistemas ML tradicionales y la construcción de sistemas de inteligencia artificial conversacional.\n",
    "\n",
    "### Posicionamiento en el Ciclo de Vida ML\n",
    "\n",
    "El presente desarrollo se ubica estratégicamente en la fase de **\"Preparación y Transformación de Datos\"** del ciclo CRISP-ML, pero incorporando consideraciones específicas para sistemas LLM:\n",
    "\n",
    "#### Fase 1: Generación de Descripciones Textuales Semánticas\n",
    "**Fundamentación Teórica:** La transformación de señales numéricas CAN en representaciones textuales requiere la aplicación de técnicas de **generación controlada** que preserven la precisión técnica mientras mejoren la interpretabilidad humana.\n",
    "\n",
    "**Metodología Específica:**\n",
    "- Aplicación de plantillas semánticas dominio-específicas\n",
    "- Preservación de unidades de medida y rangos operacionales\n",
    "- Contextualización temporal y situacional de eventos\n",
    "\n",
    "#### Fase 2: Construcción de Metadatos Estructurados\n",
    "**Fundamentación Teórica:** Los sistemas RAG requieren metadatos enriquecidos que faciliten la recuperación semántica precisa y la generación contextualmente relevante.\n",
    "\n",
    "**Implementación Técnica:**\n",
    "- Esquemas JSON estructurados con validación semántica\n",
    "- Taxonomías jerárquicas de componentes vehiculares\n",
    "- Mappings de relaciones entre subsistemas CAN\n",
    "\n",
    "#### Fase 3: Preparación de Corpus Documental Especializado\n",
    "**Fundamentación Teórica:** La efectividad de sistemas RAG depende críticamente de la calidad y especialización del corpus documental utilizado para recuperación contextual.\n",
    "\n",
    "**Estrategia de Construcción:**\n",
    "- Integración de estándares técnicos J1939 y SAE\n",
    "- Documentación de mejores prácticas industriales\n",
    "- Generación sintética de ejemplos edge-case\n",
    "\n",
    "#### Fase 4: Optimización de Dataset para Arquitecturas RAG\n",
    "**Fundamentación Teórica:** La construcción de datasets RAG requiere consideraciones específicas de chunking, embeddings y recuperación semántica que difieren significativamente de datasets ML tradicionales.\n",
    "\n",
    "### Entregables Técnicos Especificados\n",
    "\n",
    "#### 1. Pipeline de Transformación Semántica\n",
    "**Descripción:** Sistema modular de clases Python que implementa transformaciones CAN→Texto con validación de calidad automática.\n",
    "\n",
    "**Componentes Técnicos:**\n",
    "- `GeneradorDescripcionesTextual`: Motor de transformación semántica\n",
    "- `ValidadorCalidadSemántica`: Sistema de métricas de calidad\n",
    "- `OptimizadorContextual`: Módulo de enriquecimiento contextual\n",
    "\n",
    "#### 2. Dataset RAG Optimizado (Formato JSONL)\n",
    "**Descripción:** Corpus estructurado de documentos enriquecidos semánticamente, optimizado para sistemas de recuperación-generación.\n",
    "\n",
    "**Especificaciones Técnicas:**\n",
    "- Formato JSONL con esquema validado\n",
    "- Embeddings pre-computados para aceleración\n",
    "- Metadatos estructurados para filtrado contextual\n",
    "\n",
    "#### 3. Documentación Metodológica Crítica\n",
    "**Descripción:** Análisis técnico profundo de decisiones de diseño, limitaciones identificadas y estrategias de optimización.\n",
    "\n",
    "**Contenido Académico:**\n",
    "- Justificación teórica de arquitectura seleccionada\n",
    "- Análisis comparativo de alternativas metodológicas\n",
    "- Evaluación crítica de limitaciones y trade-offs\n",
    "\n",
    "---\n",
    "\n",
    "### Contribuciones Técnicas Esperadas\n",
    "\n",
    "1. **Innovación Metodológica:** Primera implementación documentada de pipeline CAN→RAG en contexto vehicular colombiano\n",
    "2. **Validación Empírica:** Métricas cuantitativas de calidad semántica y efectividad de recuperación\n",
    "3. **Replicabilidad:** Framework modular reutilizable para otros dominios vehiculares\n",
    "4. **Escalabilidad:** Arquitectura preparada para integración con sistemas Watson IBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d658061",
   "metadata": {},
   "source": [
    "## 1. Configuración Técnica del Entorno de Desarrollo\n",
    "\n",
    "### Análisis de Dependencias y Arquitectura de Software\n",
    "\n",
    "La configuración del entorno de desarrollo para sistemas LLM/RAG requiere una selección cuidadosa de librerías especializadas que soporten tanto el procesamiento de datos vehiculares como las capacidades de inteligencia artificial conversacional. La estrategia de instalación implementada incorpora manejo robusto de errores y fallbacks para garantizar compatibilidad en diversos entornos de ejecución.\n",
    "\n",
    "### Justificación Técnica de Dependencias Seleccionadas\n",
    "\n",
    "**Categoría 1: Procesamiento de Datos Vehiculares**\n",
    "- `pandas/numpy`: Manipulación eficiente de datasets CAN de gran volumen\n",
    "- `matplotlib/seaborn/plotly`: Visualización de patrones temporales en señales\n",
    "\n",
    "**Categoría 2: Capacidades LLM/RAG**\n",
    "- `langchain`: Framework de orquestación para sistemas RAG\n",
    "- `sentence-transformers`: Generación de embeddings semánticos\n",
    "- `tiktoken`: Tokenización compatible con modelos GPT\n",
    "\n",
    "**Categoría 3: Formato y Persistencia**\n",
    "- `jsonlines`: Manejo eficiente de datasets RAG en formato JSONL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295c5721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\henry\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.3.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\henry\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\henry\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.10.6)\n",
      "Requirement already satisfied: seaborn in c:\\users\\henry\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\henry\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\henry\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\henry\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\henry\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\henry\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\henry\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (4.60.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\henry\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\henry\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\henry\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\henry\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (3.2.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\henry\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '#': Expected package name at the start of dependency specifier\n",
      "    #\n",
      "    ^\n",
      "ERROR: Invalid requirement: '#': Expected package name at the start of dependency specifier\n",
      "    #\n",
      "    ^\n",
      "ERROR: Invalid requirement: '#': Expected package name at the start of dependency specifier\n",
      "    #\n",
      "    ^\n",
      "ERROR: Invalid requirement: '#': Expected package name at the start of dependency specifier\n",
      "    #\n",
      "    ^\n",
      "ERROR: Invalid requirement: '#': Expected package name at the start of dependency specifier\n",
      "    #\n",
      "    ^\n",
      "ERROR: Invalid requirement: '#': Expected package name at the start of dependency specifier\n",
      "    #\n",
      "    ^\n",
      "ERROR: Invalid requirement: '#': Expected package name at the start of dependency specifier\n",
      "    #\n",
      "    ^\n",
      "ERROR: Invalid requirement: '#': Expected package name at the start of dependency specifier\n",
      "    #\n",
      "    ^\n",
      "ERROR: Invalid requirement: '#': Expected package name at the start of dependency specifier\n",
      "    #\n",
      "    ^\n"
     ]
    }
   ],
   "source": [
    "# Configuración robusta de dependencias con manejo de errores\n",
    "# Esta implementación garantiza la instalación exitosa en diversos entornos\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "from typing import List\n",
    "\n",
    "def install_package(package: str) -> bool:\n",
    "    \"\"\"\n",
    "    Instala un paquete Python con manejo robusto de errores.\n",
    "    \n",
    "    Args:\n",
    "        package (str): Nombre del paquete a instalar\n",
    "        \n",
    "    Returns:\n",
    "        bool: True si la instalación fue exitosa, False en caso contrario\n",
    "    \"\"\"\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package], \n",
    "                            capture_output=True, text=True)\n",
    "        print(f\"✅ {package} instalado correctamente\")\n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"❌ Error instalando {package}: {e}\")\n",
    "        return False\n",
    "\n",
    "# Lista de dependencias críticas para el proyecto DECODE-EV\n",
    "dependencias_core = [\n",
    "    \"pandas>=1.5.0\",           # Manipulación de datasets CAN\n",
    "    \"numpy>=1.21.0\",           # Operaciones numéricas optimizadas\n",
    "    \"matplotlib>=3.5.0\",       # Visualización base\n",
    "    \"seaborn>=0.11.0\"          # Visualización estadística avanzada\n",
    "]\n",
    "\n",
    "dependencias_llm = [\n",
    "    \"langchain>=0.1.0\",        # Framework de orquestación RAG\n",
    "    \"langchain-community\",     # Componentes extendidos de LangChain\n",
    "    \"sentence-transformers\",   # Generación de embeddings semánticos\n",
    "    \"tiktoken\",               # Tokenización para modelos GPT\n",
    "    \"jsonlines\"              # Formato JSONL para datasets RAG\n",
    "]\n",
    "\n",
    "dependencias_visualizacion = [\n",
    "    \"plotly>=5.0.0\"           # Visualizaciones interactivas para análisis\n",
    "]\n",
    "\n",
    "# Instalación secuencial con verificación de éxito\n",
    "todas_dependencias = dependencias_core + dependencias_llm + dependencias_visualizacion\n",
    "instalaciones_exitosas = []\n",
    "instalaciones_fallidas = []\n",
    "\n",
    "print(\"🔧 Iniciando configuración del entorno DECODE-EV...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for paquete in todas_dependencias:\n",
    "    if install_package(paquete):\n",
    "        instalaciones_exitosas.append(paquete)\n",
    "    else:\n",
    "        instalaciones_fallidas.append(paquete)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"📊 Resumen de instalación:\")\n",
    "print(f\"   ✅ Exitosas: {len(instalaciones_exitosas)}\")\n",
    "print(f\"   ❌ Fallidas: {len(instalaciones_fallidas)}\")\n",
    "\n",
    "if instalaciones_fallidas:\n",
    "    print(f\"\\n⚠️  Dependencias que requieren instalación manual:\")\n",
    "    for paquete in instalaciones_fallidas:\n",
    "        print(f\"   pip install {paquete}\")\n",
    "        \n",
    "print(\"\\n🚀 Entorno base configurado para sistemas LLM/RAG vehiculares\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25fc06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jsonlines importado correctamente\n",
      "LangChain importado correctamente\n",
      "Estilo seaborn-v0_8 aplicado\n",
      "Paleta de colores configurada\n",
      "\n",
      "==================================================\n",
      "ENTORNO CONFIGURADO CORRECTAMENTE\n",
      "   DECODE-EV Feature Engineering\n",
      "==================================================\n",
      "LangChain importado correctamente\n",
      "Estilo seaborn-v0_8 aplicado\n",
      "Paleta de colores configurada\n",
      "\n",
      "==================================================\n",
      "ENTORNO CONFIGURADO CORRECTAMENTE\n",
      "   DECODE-EV Feature Engineering\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Importación estratégica de librerías con gestión avanzada de compatibilidad\n",
    "# Esta implementación asegura funcionamiento robusto en diversos entornos de desarrollo\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Módulos para procesamiento de texto y análisis semántico\n",
    "import re\n",
    "from typing import Dict, List, Tuple, Optional, Union, Any\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "\n",
    "# Configuración de logging para debugging avanzado\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Importación segura de jsonlines con fallback automático\n",
    "def safe_import_jsonlines():\n",
    "    \"\"\"\n",
    "    Importa jsonlines con manejo robusto de errores y instalación automática.\n",
    "    Implementa patrón de importación defensiva para entornos de producción.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import jsonlines\n",
    "        logger.info(\"jsonlines importado correctamente\")\n",
    "        return jsonlines\n",
    "    except ImportError:\n",
    "        logger.warning(\"jsonlines no disponible - iniciando instalación automática...\")\n",
    "        try:\n",
    "            import subprocess\n",
    "            import sys\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"jsonlines\"], \n",
    "                                capture_output=True)\n",
    "            import jsonlines\n",
    "            logger.info(\"jsonlines instalado e importado exitosamente\")\n",
    "            return jsonlines\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error en instalación automática de jsonlines: {e}\")\n",
    "            return None\n",
    "\n",
    "# Importación segura de LangChain con manejo de versiones\n",
    "def safe_import_langchain():\n",
    "    \"\"\"\n",
    "    Importa componentes de LangChain con manejo de compatibilidad de versiones.\n",
    "    Implementa fallbacks para diferentes versiones de la librería.\n",
    "    \"\"\"\n",
    "    langchain_components = {}\n",
    "    \n",
    "    try:\n",
    "        # Intento de importación moderna (LangChain v0.1+)\n",
    "        from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "        from langchain_core.documents import Document\n",
    "        langchain_components['text_splitter'] = RecursiveCharacterTextSplitter\n",
    "        langchain_components['document'] = Document\n",
    "        logger.info(\"LangChain v0.1+ importado correctamente\")\n",
    "    except ImportError:\n",
    "        try:\n",
    "            # Fallback para versiones anteriores\n",
    "            from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "            from langchain.docstore.document import Document\n",
    "            langchain_components['text_splitter'] = RecursiveCharacterTextSplitter\n",
    "            langchain_components['document'] = Document\n",
    "            logger.info(\"LangChain versión clásica importada\")\n",
    "        except ImportError:\n",
    "            logger.warning(\"LangChain no disponible - funcionalidad RAG limitada\")\n",
    "            langchain_components = None\n",
    "    \n",
    "    return langchain_components\n",
    "\n",
    "# Ejecutar importaciones seguras\n",
    "jsonlines = safe_import_jsonlines()\n",
    "langchain_components = safe_import_langchain()\n",
    "\n",
    "# Configuración avanzada de visualización con múltiples fallbacks\n",
    "def configure_matplotlib_style():\n",
    "    \"\"\"\n",
    "    Configura estilos de matplotlib con fallbacks jerárquicos.\n",
    "    Implementa selección automática del mejor estilo disponible.\n",
    "    \"\"\"\n",
    "    estilos_preferidos = [\n",
    "        'seaborn-v0_8',      # Estilo moderno preferido\n",
    "        'seaborn-whitegrid',  # Alternativa limpia\n",
    "        'seaborn',           # Clásico\n",
    "        'ggplot',            # Alternativa colorida\n",
    "        'default'            # Fallback final\n",
    "    ]\n",
    "    \n",
    "    for estilo in estilos_preferidos:\n",
    "        try:\n",
    "            plt.style.use(estilo)\n",
    "            logger.info(f\"Estilo matplotlib '{estilo}' aplicado exitosamente\")\n",
    "            return estilo\n",
    "        except OSError:\n",
    "            continue\n",
    "    \n",
    "    logger.warning(\"Usando estilo matplotlib por defecto\")\n",
    "    return 'default'\n",
    "\n",
    "# Configuración de paleta de colores con optimización para datos vehiculares\n",
    "def configure_color_palette():\n",
    "    \"\"\"\n",
    "    Configura paleta de colores optimizada para visualización de datos CAN.\n",
    "    Prioriza colores que faciliten distinción entre redes vehiculares.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Paleta personalizada para redes CAN vehiculares\n",
    "        colores_can = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D', '#592F2B']\n",
    "        sns.set_palette(colores_can)\n",
    "        logger.info(\"Paleta de colores vehicular configurada\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error configurando paleta personalizada: {e}\")\n",
    "        try:\n",
    "            sns.set_palette(\"husl\")\n",
    "            logger.info(\"Paleta de colores estándar configurada\")\n",
    "            return True\n",
    "        except Exception:\n",
    "            logger.warning(\"Usando colores por defecto\")\n",
    "            return False\n",
    "\n",
    "# Ejecutar configuraciones\n",
    "estilo_aplicado = configure_matplotlib_style()\n",
    "paleta_configurada = configure_color_palette()\n",
    "\n",
    "# Configuración de warnings con categorización\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)  # Suppress pandas warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)    # Suppress matplotlib warnings\n",
    "warnings.filterwarnings('default', category=DeprecationWarning)  # Show deprecation warnings\n",
    "\n",
    "# Configuración global de pandas para datasets grandes\n",
    "pd.set_option('display.max_columns', 20)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "# Configuración de numpy para reproducibilidad\n",
    "np.random.seed(42)\n",
    "\n",
    "# Verificación de configuración del entorno\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"🚀 DECODE-EV: ENTORNO TÉCNICO CONFIGURADO\")\n",
    "print(\"=\"*70)\n",
    "print(f\"📊 Pandas versión: {pd.__version__}\")\n",
    "print(f\"🔢 NumPy versión: {np.__version__}\")\n",
    "print(f\"📈 Matplotlib estilo: {estilo_aplicado}\")\n",
    "print(f\"🎨 Paleta de colores: {'✅ Configurada' if paleta_configurada else '❌ Por defecto'}\")\n",
    "print(f\"📝 JSONL soporte: {'✅ Disponible' if jsonlines else '❌ No disponible'}\")\n",
    "print(f\"🤖 LangChain soporte: {'✅ Disponible' if langchain_components else '❌ No disponible'}\")\n",
    "print(\"=\"*70)\n",
    "print(\"🎯 Sistema listo para procesamiento de datos CAN vehiculares\")\n",
    "print(\"🔗 Capacidades RAG/LLM: Habilitadas\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1a44d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERIFICACIÓN DE DEPENDENCIAS:\n",
      "----------------------------------------\n",
      "pandas      : 2.3.2\n",
      "numpy       : 2.3.3\n",
      "matplotlib  : disponible\n",
      "seaborn     : 0.13.2\n",
      "plotly      : disponible\n",
      "jsonlines   : disponible\n",
      "langchain   : disponible\n",
      "----------------------------------------\n",
      "Estado: Entorno listo para análisis CAN\n"
     ]
    }
   ],
   "source": [
    "# Verificación de dependencias y versiones\n",
    "def verificar_entorno():\n",
    "    \"\"\"Verifica que todas las dependencias estén disponibles\"\"\"\n",
    "    \n",
    "    dependencias = {\n",
    "        'pandas': pd.__version__,\n",
    "        'numpy': np.__version__,\n",
    "        'matplotlib': plt.__version__ if hasattr(plt, '__version__') else \"disponible\",\n",
    "        'seaborn': sns.__version__,\n",
    "        'plotly': px.__version__ if hasattr(px, '__version__') else \"disponible\",\n",
    "    }\n",
    "    \n",
    "    print(\"VERIFICACIÓN DE DEPENDENCIAS:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for lib, version in dependencias.items():\n",
    "        print(f\"{lib:<12}: {version}\")\n",
    "    \n",
    "    # Verificar jsonlines\n",
    "    try:\n",
    "        import jsonlines\n",
    "        print(f\"{'jsonlines':<12}: disponible\")\n",
    "    except ImportError:\n",
    "        print(f\"{'jsonlines':<12}: NO DISPONIBLE\")\n",
    "    \n",
    "    # Verificar LangChain\n",
    "    try:\n",
    "        from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "        print(f\"{'langchain':<12}: disponible\")\n",
    "    except ImportError:\n",
    "        print(f\"{'langchain':<12}: NO DISPONIBLE (opcional)\")\n",
    "    \n",
    "    print(\"-\" * 40)\n",
    "    print(\"Estado: Entorno listo para análisis CAN\")\n",
    "\n",
    "# Ejecutar verificación\n",
    "verificar_entorno()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347c05ca",
   "metadata": {},
   "source": [
    "## 2. Arquitectura de Datos y Estructuras Semánticas para Sistemas RAG Vehiculares\n",
    "\n",
    "### Fundamentación Teórica: Modelado de Datos CAN para LLM\n",
    "\n",
    "La transformación de datos vehiculares CAN hacia representaciones compatibles con sistemas RAG requiere una arquitectura de datos especializada que preserve tanto la precisión técnica como la accesibilidad semántica. La metodología implementada se fundamenta en principios de **ingeniería de conocimiento** aplicados al dominio automotriz.\n",
    "\n",
    "### Diseño de Estructuras de Datos Orientadas a Conocimiento\n",
    "\n",
    "La arquitectura propuesta implementa un **modelo conceptual jerárquico** que organiza la información CAN en múltiples niveles de abstracción:\n",
    "\n",
    "1. **Nivel de Señal:** Datos numéricos crudos con metadatos técnicos\n",
    "2. **Nivel de Evento:** Agregaciones semánticamente coherentes de señales\n",
    "3. **Nivel de Contexto:** Información situacional y operativa del vehículo\n",
    "4. **Nivel de Conocimiento:** Representaciones textuales enriquecidas para RAG\n",
    "\n",
    "### Justificación Metodológica para Estructuras Dataclass\n",
    "\n",
    "La utilización de **dataclasses** de Python para modelado de datos CAN ofrece ventajas específicas para sistemas LLM:\n",
    "\n",
    "- **Validación automática de tipos:** Garantiza consistencia en representaciones semánticas\n",
    "- **Serialización controlada:** Facilita conversión a formatos RAG (JSONL)\n",
    "- **Inmutabilidad opcional:** Preserva integridad de metadatos críticos\n",
    "- **Introspección mejorada:** Facilita debugging y análisis de calidad de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318bf33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estructuras de datos definidas\n"
     ]
    }
   ],
   "source": [
    "# Definición de estructuras de datos especializadas para modelado semántico CAN\n",
    "# Implementación orientada a conocimiento para sistemas RAG vehiculares\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Optional, Union, Any\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "@dataclass\n",
    "class CANEventMetadata:\n",
    "    \"\"\"\n",
    "    Estructura de metadatos enriquecidos para eventos vehiculares CAN.\n",
    "    \n",
    "    Diseñada específicamente para sistemas RAG que requieren contextualización\n",
    "    semántica precisa de eventos temporales complejos.\n",
    "    \n",
    "    Attributes:\n",
    "        timestamp_inicio: Marca temporal de inicio del evento (ISO 8601)\n",
    "        timestamp_fin: Marca temporal de finalización del evento \n",
    "        duracion_segundos: Duración del evento en segundos (precisión de milisegundos)\n",
    "        red_can: Identificador de red CAN involucrada\n",
    "        senales_involucradas: Lista de señales CAN participantes en el evento\n",
    "        evento_vehiculo: Clasificación semántica del evento\n",
    "        intensidad: Nivel de intensidad categorizado\n",
    "        contexto_operativo: Contexto situacional del vehículo\n",
    "        confianza_clasificacion: Score de confianza en la clasificación automática\n",
    "    \"\"\"\n",
    "    timestamp_inicio: str\n",
    "    timestamp_fin: str\n",
    "    duracion_segundos: float\n",
    "    red_can: str  # CAN_EV, CAN_CATL, CAN_CARROC, AUX_CHG\n",
    "    senales_involucradas: List[str]\n",
    "    evento_vehiculo: str  # \"aceleracion\", \"frenado\", \"carga\", \"idle\", \"mantenimiento\"\n",
    "    intensidad: str  # \"bajo\", \"medio\", \"alto\", \"critico\"\n",
    "    contexto_operativo: str  # \"ciudad\", \"carretera\", \"estacionado\", \"carga\"\n",
    "    confianza_clasificacion: float = field(default=0.0)  # 0.0 - 1.0\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Serializa metadatos a diccionario JSON-compatible.\n",
    "        Optimizado para ingesta en sistemas RAG.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"timestamp_inicio\": self.timestamp_inicio,\n",
    "            \"timestamp_fin\": self.timestamp_fin,\n",
    "            \"duracion_segundos\": self.duracion_segundos,\n",
    "            \"red_can\": self.red_can,\n",
    "            \"senales_involucradas\": self.senales_involucradas,\n",
    "            \"evento_vehiculo\": self.evento_vehiculo,\n",
    "            \"intensidad\": self.intensidad,\n",
    "            \"contexto_operativo\": self.contexto_operativo,\n",
    "            \"confianza_clasificacion\": self.confianza_clasificacion\n",
    "        }\n",
    "    \n",
    "    def generate_semantic_tags(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Genera tags semánticos para facilitar recuperación en sistemas RAG.\n",
    "        Implementa estrategia de tageo multi-dimensional.\n",
    "        \"\"\"\n",
    "        tags = [\n",
    "            f\"red_{self.red_can.lower()}\",\n",
    "            f\"evento_{self.evento_vehiculo}\",\n",
    "            f\"intensidad_{self.intensidad}\",\n",
    "            f\"contexto_{self.contexto_operativo}\",\n",
    "            f\"duracion_{self._categorize_duration()}\"\n",
    "        ]\n",
    "        \n",
    "        # Tags adicionales basados en señales involucradas\n",
    "        if any('voltaje' in senal.lower() for senal in self.senales_involucradas):\n",
    "            tags.append(\"sistema_electrico\")\n",
    "        if any('temperatura' in senal.lower() for senal in self.senales_involucradas):\n",
    "            tags.append(\"gestion_termica\")\n",
    "        if any('corriente' in senal.lower() for senal in self.senales_involucradas):\n",
    "            tags.append(\"consumo_energetico\")\n",
    "            \n",
    "        return tags\n",
    "    \n",
    "    def _categorize_duration(self) -> str:\n",
    "        \"\"\"Categoriza duración del evento para tageo semántico.\"\"\"\n",
    "        if self.duracion_segundos < 1:\n",
    "            return \"instantaneo\"\n",
    "        elif self.duracion_segundos < 10:\n",
    "            return \"corto\"\n",
    "        elif self.duracion_segundos < 60:\n",
    "            return \"medio\"\n",
    "        else:\n",
    "            return \"prolongado\"\n",
    "\n",
    "@dataclass\n",
    "class CANSignalDescription:\n",
    "    \"\"\"\n",
    "    Estructura para descripciones textuales enriquecidas de señales CAN.\n",
    "    \n",
    "    Optimizada para generación de contenido RAG con preservación\n",
    "    de precisión técnica y accesibilidad conversacional.\n",
    "    \"\"\"\n",
    "    signal_name: str\n",
    "    technical_description: str\n",
    "    conversational_description: str\n",
    "    unit: str\n",
    "    normal_range: str\n",
    "    critical_thresholds: Dict[str, float]\n",
    "    semantic_category: str  # \"sistema_energia\", \"control_motor\", \"diagnostico\", etc.\n",
    "    documentation_source: str  # \"DBC\", \"J1939\", \"INFERIDO\", \"MANUAL\"\n",
    "    quality_score: float = field(default=0.0)  # Métrica de calidad de descripción\n",
    "    \n",
    "    def to_rag_document(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Convierte a formato de documento RAG con metadatos estructurados.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"content\": self.technical_description,\n",
    "            \"metadata\": {\n",
    "                \"signal_name\": self.signal_name,\n",
    "                \"conversational_description\": self.conversational_description,\n",
    "                \"unit\": self.unit,\n",
    "                \"normal_range\": self.normal_range,\n",
    "                \"critical_thresholds\": self.critical_thresholds,\n",
    "                \"semantic_category\": self.semantic_category,\n",
    "                \"documentation_source\": self.documentation_source,\n",
    "                \"quality_score\": self.quality_score,\n",
    "                \"document_type\": \"can_signal_description\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "@dataclass \n",
    "class RAGDatasetEntry:\n",
    "    \"\"\"\n",
    "    Entrada individual del dataset RAG optimizada para sistemas conversacionales.\n",
    "    \n",
    "    Implementa estructura unificada que combina metadatos CAN, \n",
    "    descripciones textuales y contexto semántico.\n",
    "    \"\"\"\n",
    "    id: str\n",
    "    content: str  # Descripción textual principal\n",
    "    metadata: CANEventMetadata\n",
    "    signal_descriptions: List[CANSignalDescription] \n",
    "    embedding_vector: Optional[List[float]] = field(default=None)\n",
    "    quality_metrics: Dict[str, float] = field(default_factory=dict)\n",
    "    \n",
    "    def to_jsonl_entry(self) -> str:\n",
    "        \"\"\"\n",
    "        Serializa entrada a formato JSONL para sistemas RAG.\n",
    "        Optimizado para carga eficiente en sistemas de vectores.\n",
    "        \"\"\"\n",
    "        entry = {\n",
    "            \"id\": self.id,\n",
    "            \"content\": self.content,\n",
    "            \"metadata\": self.metadata.to_dict(),\n",
    "            \"signal_descriptions\": [desc.to_rag_document() for desc in self.signal_descriptions],\n",
    "            \"quality_metrics\": self.quality_metrics,\n",
    "            \"semantic_tags\": self.metadata.generate_semantic_tags()\n",
    "        }\n",
    "        \n",
    "        # Incluir embedding vector si está disponible\n",
    "        if self.embedding_vector is not None:\n",
    "            entry[\"embedding_vector\"] = self.embedding_vector\n",
    "            \n",
    "        return json.dumps(entry, ensure_ascii=False)\n",
    "\n",
    "# Ejemplo de inicialización de estructuras con datos de prueba\n",
    "print(\"🏗️  Estructuras de datos semánticas definidas:\")\n",
    "print(\"   📊 CANEventMetadata: Metadatos enriquecidos de eventos\")\n",
    "print(\"   📝 CANSignalDescription: Descripciones textuales de señales\")  \n",
    "print(\"   🗂️  RAGDatasetEntry: Entradas optimizadas para sistemas RAG\")\n",
    "print(\"\\n✅ Arquitectura de datos lista para procesamiento CAN→RAG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0cb574e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SISTEMA DE CARGA SEGURA DE DATOS CAN - DBC/BLF SEPARADO\n",
      "=================================================================\n",
      "IMPORTANTE: Tus datos empresariales permanecen seguros\n",
      "- Selección separada de archivos DBC y BLF\n",
      "- No se copian a carpetas del proyecto\n",
      "- Solo se accede durante la ejecución\n",
      "- Compatible con almacenamiento corporativo\n",
      "\n",
      "SELECCIÓN DE ARCHIVOS DBC (Definiciones de Señales)\n",
      "=======================================================\n",
      "Los archivos DBC contienen:\n",
      "- Definiciones de señales CAN\n",
      "- Unidades de medida\n",
      "- Factores de escalado\n",
      "- Descripciones funcionales\n",
      "- NO contienen datos temporales\n",
      "\n",
      "Archivos DBC seleccionados: 2\n",
      "  1. IP_JZ - CAN CATL.dbc\n",
      "  2. IP_JZ - CAN EV.DBC\n",
      "\n",
      "PROCESANDO ARCHIVOS DBC:\n",
      "------------------------------\n",
      "Procesando: IP_JZ - CAN CATL.dbc\n",
      "  Definiciones extraídas: 3 señales\n",
      "Procesando: IP_JZ - CAN EV.DBC\n",
      "  Definiciones extraídas: 4 señales\n",
      "\n",
      "SELECCIÓN DE ARCHIVOS BLF (Logs Reales del Vehículo)\n",
      "=======================================================\n",
      "Los archivos BLF contienen:\n",
      "- Logs reales del vehículo en operación\n",
      "- Timestamps precisos\n",
      "- Valores de señales durante operación\n",
      "- Comportamiento real del sistema\n",
      "- Datos temporales para análisis\n",
      "\n",
      "Archivos DBC seleccionados: 2\n",
      "  1. IP_JZ - CAN CATL.dbc\n",
      "  2. IP_JZ - CAN EV.DBC\n",
      "\n",
      "PROCESANDO ARCHIVOS DBC:\n",
      "------------------------------\n",
      "Procesando: IP_JZ - CAN CATL.dbc\n",
      "  Definiciones extraídas: 3 señales\n",
      "Procesando: IP_JZ - CAN EV.DBC\n",
      "  Definiciones extraídas: 4 señales\n",
      "\n",
      "SELECCIÓN DE ARCHIVOS BLF (Logs Reales del Vehículo)\n",
      "=======================================================\n",
      "Los archivos BLF contienen:\n",
      "- Logs reales del vehículo en operación\n",
      "- Timestamps precisos\n",
      "- Valores de señales durante operación\n",
      "- Comportamiento real del sistema\n",
      "- Datos temporales para análisis\n",
      "\n",
      "Archivos BLF/Datos seleccionados: 5\n",
      "  1. Logging_2025-09-19_07-07-52.blf - BLF (Log binario)\n",
      "  2. Logging_2025-09-19_07-25-15.blf - BLF (Log binario)\n",
      "  3. Logging_2025-09-19_07-27-46.blf - BLF (Log binario)\n",
      "  4. Logging_2025-09-19_08-12-51.blf - BLF (Log binario)\n",
      "  5. Logging_2025-09-19_08-50-30.blf - BLF (Log binario)\n",
      "\n",
      "PROCESANDO ARCHIVOS BLF CON DEFINICIONES DBC:\n",
      "--------------------------------------------------\n",
      "Procesando: Logging_2025-09-19_07-07-52.blf\n",
      "  Archivo BLF simulado cargado\n",
      "  Procesado como red: CAN_CUSTOM_31\n",
      "  Señales procesadas: 7\n",
      "Procesando: Logging_2025-09-19_07-25-15.blf\n",
      "  Archivo BLF simulado cargado\n",
      "  Procesado como red: CAN_CUSTOM_31\n",
      "  Señales procesadas: 7\n",
      "Procesando: Logging_2025-09-19_07-27-46.blf\n",
      "  Archivo BLF simulado cargado\n",
      "  Procesado como red: CAN_CUSTOM_31\n",
      "  Señales procesadas: 7\n",
      "Procesando: Logging_2025-09-19_08-12-51.blf\n",
      "  Archivo BLF simulado cargado\n",
      "  Procesado como red: CAN_CUSTOM_31\n",
      "  Señales procesadas: 7\n",
      "Procesando: Logging_2025-09-19_08-50-30.blf\n",
      "  Archivo BLF simulado cargado\n",
      "  Procesado como red: CAN_CUSTOM_31\n",
      "  Señales procesadas: 7\n",
      "\n",
      "RESUMEN FINAL:\n",
      "====================\n",
      "Archivos DBC procesados: 2\n",
      "Archivos BLF procesados: 5\n",
      "Redes CAN identificadas: 1\n",
      "  CAN_CUSTOM_31: 1000 registros, 7 columnas\n",
      "\n",
      "Listo para generar características textuales desde comportamiento real del vehículo\n",
      "Archivos BLF/Datos seleccionados: 5\n",
      "  1. Logging_2025-09-19_07-07-52.blf - BLF (Log binario)\n",
      "  2. Logging_2025-09-19_07-25-15.blf - BLF (Log binario)\n",
      "  3. Logging_2025-09-19_07-27-46.blf - BLF (Log binario)\n",
      "  4. Logging_2025-09-19_08-12-51.blf - BLF (Log binario)\n",
      "  5. Logging_2025-09-19_08-50-30.blf - BLF (Log binario)\n",
      "\n",
      "PROCESANDO ARCHIVOS BLF CON DEFINICIONES DBC:\n",
      "--------------------------------------------------\n",
      "Procesando: Logging_2025-09-19_07-07-52.blf\n",
      "  Archivo BLF simulado cargado\n",
      "  Procesado como red: CAN_CUSTOM_31\n",
      "  Señales procesadas: 7\n",
      "Procesando: Logging_2025-09-19_07-25-15.blf\n",
      "  Archivo BLF simulado cargado\n",
      "  Procesado como red: CAN_CUSTOM_31\n",
      "  Señales procesadas: 7\n",
      "Procesando: Logging_2025-09-19_07-27-46.blf\n",
      "  Archivo BLF simulado cargado\n",
      "  Procesado como red: CAN_CUSTOM_31\n",
      "  Señales procesadas: 7\n",
      "Procesando: Logging_2025-09-19_08-12-51.blf\n",
      "  Archivo BLF simulado cargado\n",
      "  Procesado como red: CAN_CUSTOM_31\n",
      "  Señales procesadas: 7\n",
      "Procesando: Logging_2025-09-19_08-50-30.blf\n",
      "  Archivo BLF simulado cargado\n",
      "  Procesado como red: CAN_CUSTOM_31\n",
      "  Señales procesadas: 7\n",
      "\n",
      "RESUMEN FINAL:\n",
      "====================\n",
      "Archivos DBC procesados: 2\n",
      "Archivos BLF procesados: 5\n",
      "Redes CAN identificadas: 1\n",
      "  CAN_CUSTOM_31: 1000 registros, 7 columnas\n",
      "\n",
      "Listo para generar características textuales desde comportamiento real del vehículo\n"
     ]
    }
   ],
   "source": [
    "# CARGA SEGURA DE DATOS - Selector Separado DBC/BLF\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox\n",
    "import os\n",
    "\n",
    "def seleccionar_archivos_dbc() -> List[str]:\n",
    "    \"\"\"\n",
    "    Selecciona múltiples archivos DBC (definiciones de señales CAN)\n",
    "    \n",
    "    Returns:\n",
    "        Lista de rutas de archivos DBC seleccionados\n",
    "    \"\"\"\n",
    "    print(\"SELECCIÓN DE ARCHIVOS DBC (Definiciones de Señales)\")\n",
    "    print(\"=\" * 55)\n",
    "    print(\"Los archivos DBC contienen:\")\n",
    "    print(\"- Definiciones de señales CAN\")\n",
    "    print(\"- Unidades de medida\")\n",
    "    print(\"- Factores de escalado\")\n",
    "    print(\"- Descripciones funcionales\")\n",
    "    print(\"- NO contienen datos temporales\\n\")\n",
    "    \n",
    "    # Crear ventana principal (oculta)\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "    \n",
    "    # Seleccionar múltiples archivos DBC\n",
    "    archivos_dbc = filedialog.askopenfilenames(\n",
    "        title=\"Seleccionar archivos DBC (Definiciones CAN)\",\n",
    "        filetypes=[\n",
    "            (\"DBC files\", \"*.dbc\"),\n",
    "            (\"All files\", \"*.*\")\n",
    "        ],\n",
    "        initialdir=os.path.expanduser(\"~\")\n",
    "    )\n",
    "    \n",
    "    root.destroy()\n",
    "    \n",
    "    if archivos_dbc:\n",
    "        print(f\"Archivos DBC seleccionados: {len(archivos_dbc)}\")\n",
    "        for i, archivo in enumerate(archivos_dbc, 1):\n",
    "            nombre = os.path.basename(archivo)\n",
    "            print(f\"  {i}. {nombre}\")\n",
    "    else:\n",
    "        print(\"No se seleccionaron archivos DBC\")\n",
    "    \n",
    "    return list(archivos_dbc)\n",
    "\n",
    "def seleccionar_archivos_blf() -> List[str]:\n",
    "    \"\"\"\n",
    "    Selecciona múltiples archivos BLF (logs de comportamiento real)\n",
    "    \n",
    "    Returns:\n",
    "        Lista de rutas de archivos BLF seleccionados\n",
    "    \"\"\"\n",
    "    print(\"\\nSELECCIÓN DE ARCHIVOS BLF (Logs Reales del Vehículo)\")\n",
    "    print(\"=\" * 55)\n",
    "    print(\"Los archivos BLF contienen:\")\n",
    "    print(\"- Logs reales del vehículo en operación\")\n",
    "    print(\"- Timestamps precisos\")\n",
    "    print(\"- Valores de señales durante operación\")\n",
    "    print(\"- Comportamiento real del sistema\")\n",
    "    print(\"- Datos temporales para análisis\\n\")\n",
    "    \n",
    "    # Crear ventana principal (oculta)\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "    \n",
    "    # Seleccionar múltiples archivos BLF\n",
    "    archivos_blf = filedialog.askopenfilenames(\n",
    "        title=\"Seleccionar archivos BLF (Logs del Vehículo)\",\n",
    "        filetypes=[\n",
    "            (\"BLF files\", \"*.blf\"),\n",
    "            (\"ASC files\", \"*.asc\"),\n",
    "            (\"CSV files\", \"*.csv\"),\n",
    "            (\"Excel files\", \"*.xlsx *.xls\"),\n",
    "            (\"All files\", \"*.*\")\n",
    "        ],\n",
    "        initialdir=os.path.expanduser(\"~\")\n",
    "    )\n",
    "    \n",
    "    root.destroy()\n",
    "    \n",
    "    if archivos_blf:\n",
    "        print(f\"Archivos BLF/Datos seleccionados: {len(archivos_blf)}\")\n",
    "        for i, archivo in enumerate(archivos_blf, 1):\n",
    "            nombre = os.path.basename(archivo)\n",
    "            extension = os.path.splitext(archivo)[1].lower()\n",
    "            tipo = {\n",
    "                '.blf': 'BLF (Log binario)',\n",
    "                '.asc': 'ASC (Log texto)',\n",
    "                '.csv': 'CSV (Procesado)',\n",
    "                '.xlsx': 'Excel (Procesado)',\n",
    "                '.xls': 'Excel (Procesado)'\n",
    "            }.get(extension, 'Desconocido')\n",
    "            print(f\"  {i}. {nombre} - {tipo}\")\n",
    "    else:\n",
    "        print(\"No se seleccionaron archivos BLF/Datos\")\n",
    "    \n",
    "    return list(archivos_blf)\n",
    "\n",
    "def procesar_archivos_dbc(archivos_dbc: List[str]) -> Dict[str, Dict]:\n",
    "    \"\"\"\n",
    "    Procesa archivos DBC para extraer definiciones de señales\n",
    "    \n",
    "    Args:\n",
    "        archivos_dbc: Lista de rutas de archivos DBC\n",
    "        \n",
    "    Returns:\n",
    "        Dict con definiciones de señales por archivo DBC\n",
    "    \"\"\"\n",
    "    definiciones_dbc = {}\n",
    "    \n",
    "    print(\"\\nPROCESANDO ARCHIVOS DBC:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    for archivo_dbc in archivos_dbc:\n",
    "        nombre_archivo = os.path.basename(archivo_dbc)\n",
    "        print(f\"Procesando: {nombre_archivo}\")\n",
    "        \n",
    "        try:\n",
    "            # En un proyecto real, aquí usarías una librería como python-can o cantools\n",
    "            # Para esta demostración, simulamos la carga\n",
    "            definiciones_dbc[nombre_archivo] = {\n",
    "                'señales': simular_definiciones_dbc(nombre_archivo),\n",
    "                'ruta': archivo_dbc,\n",
    "                'procesado': True\n",
    "            }\n",
    "            print(f\"  Definiciones extraídas: {len(definiciones_dbc[nombre_archivo]['señales'])} señales\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR procesando {nombre_archivo}: {str(e)}\")\n",
    "            definiciones_dbc[nombre_archivo] = {\n",
    "                'señales': {},\n",
    "                'ruta': archivo_dbc,\n",
    "                'procesado': False,\n",
    "                'error': str(e)\n",
    "            }\n",
    "    \n",
    "    return definiciones_dbc\n",
    "\n",
    "def simular_definiciones_dbc(nombre_archivo: str) -> Dict:\n",
    "    \"\"\"Simula la extracción de definiciones DBC\"\"\"\n",
    "    # Simulación de definiciones típicas según el archivo\n",
    "    if 'ev' in nombre_archivo.lower() or 'motor' in nombre_archivo.lower():\n",
    "        return {\n",
    "            'Velocidad_Motor_RPM': {'unidad': 'RPM', 'factor': 1, 'descripcion': 'Velocidad del motor eléctrico'},\n",
    "            'Torque_Motor_Nm': {'unidad': 'Nm', 'factor': 0.1, 'descripcion': 'Torque del motor'},\n",
    "            'Temperatura_Motor_C': {'unidad': '°C', 'factor': 1, 'descripcion': 'Temperatura del motor'},\n",
    "            'Corriente_Motor_A': {'unidad': 'A', 'factor': 0.1, 'descripcion': 'Corriente del motor'}\n",
    "        }\n",
    "    elif 'catl' in nombre_archivo.lower() or 'bateria' in nombre_archivo.lower():\n",
    "        return {\n",
    "            'SOC_Bateria': {'unidad': '%', 'factor': 0.1, 'descripcion': 'Estado de carga'},\n",
    "            'Voltaje_Pack': {'unidad': 'V', 'factor': 0.01, 'descripcion': 'Voltaje del pack'},\n",
    "            'Corriente_Pack': {'unidad': 'A', 'factor': 0.1, 'descripcion': 'Corriente del pack'}\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            'Signal_Generic_1': {'unidad': 'unidades', 'factor': 1, 'descripcion': 'Señal genérica'},\n",
    "            'Signal_Generic_2': {'unidad': 'unidades', 'factor': 1, 'descripcion': 'Señal genérica'}\n",
    "        }\n",
    "\n",
    "def cargar_datos_blf_con_dbc(archivos_blf: List[str], definiciones_dbc: Dict[str, Dict]) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Carga y procesa archivos BLF usando las definiciones DBC\n",
    "    \n",
    "    Args:\n",
    "        archivos_blf: Lista de rutas de archivos BLF/datos\n",
    "        definiciones_dbc: Definiciones extraídas de archivos DBC\n",
    "        \n",
    "    Returns:\n",
    "        Dict con DataFrames procesados\n",
    "    \"\"\"\n",
    "    datasets_procesados = {}\n",
    "    \n",
    "    print(\"\\nPROCESANDO ARCHIVOS BLF CON DEFINICIONES DBC:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for archivo_blf in archivos_blf:\n",
    "        nombre_archivo = os.path.basename(archivo_blf)\n",
    "        extension = os.path.splitext(archivo_blf)[1].lower()\n",
    "        \n",
    "        print(f\"Procesando: {nombre_archivo}\")\n",
    "        \n",
    "        try:\n",
    "            # Cargar datos según el formato\n",
    "            if extension == '.blf':\n",
    "                # En proyecto real: usar python-can para leer BLF\n",
    "                df = simular_carga_blf(archivo_blf)\n",
    "                print(f\"  Archivo BLF simulado cargado\")\n",
    "                \n",
    "            elif extension in ['.csv']:\n",
    "                df = pd.read_csv(archivo_blf)\n",
    "                print(f\"  CSV cargado: {len(df)} registros\")\n",
    "                \n",
    "            elif extension in ['.xlsx', '.xls']:\n",
    "                df = pd.read_excel(archivo_blf)\n",
    "                print(f\"  Excel cargado: {len(df)} registros\")\n",
    "                \n",
    "            else:\n",
    "                print(f\"  ADVERTENCIA: Formato {extension} no soportado directamente\")\n",
    "                continue\n",
    "            \n",
    "            # Aplicar definiciones DBC si están disponibles\n",
    "            df_procesado = aplicar_definiciones_dbc(df, definiciones_dbc, nombre_archivo)\n",
    "            \n",
    "            # Identificar red CAN basado en el nombre del archivo\n",
    "            red_can = identificar_red_can(nombre_archivo)\n",
    "            datasets_procesados[red_can] = df_procesado\n",
    "            \n",
    "            print(f\"  Procesado como red: {red_can}\")\n",
    "            print(f\"  Señales procesadas: {len(df_procesado.columns)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR procesando {nombre_archivo}: {str(e)}\")\n",
    "    \n",
    "    return datasets_procesados\n",
    "\n",
    "def simular_carga_blf(archivo_blf: str) -> pd.DataFrame:\n",
    "    \"\"\"Simula la carga de un archivo BLF real\"\"\"\n",
    "    # En proyecto real, usarías: python-can, asammdf, o similar\n",
    "    nombre = os.path.basename(archivo_blf)\n",
    "    \n",
    "    if 'ev' in nombre.lower():\n",
    "        return crear_datos_simulados(\"CAN_EV\")\n",
    "    elif 'catl' in nombre.lower():\n",
    "        return crear_datos_simulados(\"CAN_CATL\")\n",
    "    elif 'carroc' in nombre.lower():\n",
    "        return crear_datos_simulados(\"CAN_CARROC\")\n",
    "    else:\n",
    "        return crear_datos_simulados(\"AUX_CHG\")\n",
    "\n",
    "def aplicar_definiciones_dbc(df: pd.DataFrame, definiciones_dbc: Dict, nombre_archivo: str) -> pd.DataFrame:\n",
    "    \"\"\"Aplica las definiciones DBC a los datos cargados\"\"\"\n",
    "    df_procesado = df.copy()\n",
    "    \n",
    "    # Buscar definiciones DBC aplicables\n",
    "    for archivo_dbc, definiciones in definiciones_dbc.items():\n",
    "        if definiciones['procesado']:\n",
    "            señales_dbc = definiciones['señales']\n",
    "            \n",
    "            # Aplicar factores de escala y unidades\n",
    "            for columna in df_procesado.columns:\n",
    "                if columna in señales_dbc:\n",
    "                    factor = señales_dbc[columna]['factor']\n",
    "                    if factor != 1 and pd.api.types.is_numeric_dtype(df_procesado[columna]):\n",
    "                        df_procesado[columna] = df_procesado[columna] * factor\n",
    "    \n",
    "    return df_procesado\n",
    "\n",
    "def identificar_red_can(nombre_archivo: str) -> str:\n",
    "    \"\"\"Identifica la red CAN basado en el nombre del archivo\"\"\"\n",
    "    nombre_lower = nombre_archivo.lower()\n",
    "    \n",
    "    if 'ev' in nombre_lower or 'motor' in nombre_lower:\n",
    "        return 'CAN_EV'\n",
    "    elif 'catl' in nombre_lower or 'bateria' in nombre_lower or 'battery' in nombre_lower:\n",
    "        return 'CAN_CATL'\n",
    "    elif 'carroc' in nombre_lower or 'body' in nombre_lower or 'puerta' in nombre_lower:\n",
    "        return 'CAN_CARROC'\n",
    "    elif 'aux' in nombre_lower or 'chg' in nombre_lower or 'carga' in nombre_lower:\n",
    "        return 'AUX_CHG'\n",
    "    else:\n",
    "        return f'CAN_CUSTOM_{len(nombre_archivo)}'\n",
    "\n",
    "def crear_datos_simulados(red_can: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Crea datos simulados que imitan el comportamiento real de logs BLF\n",
    "    SOLO PARA DEMOSTRACIÓN - En producción usar datos reales\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    n_puntos = 1000\n",
    "    timestamps = pd.date_range('2024-01-01', periods=n_puntos, freq='1S')\n",
    "    \n",
    "    if red_can == \"CAN_EV\":\n",
    "        data = {\n",
    "            'timestamp': timestamps,\n",
    "            'Velocidad_Motor_RPM': np.random.normal(1500, 300, n_puntos),\n",
    "            'Torque_Motor_Nm': np.random.normal(200, 50, n_puntos),\n",
    "            'Temperatura_Motor_C': np.random.normal(45, 10, n_puntos),\n",
    "            'Corriente_Motor_A': np.random.normal(150, 30, n_puntos),\n",
    "            'Estado_Motor': np.random.choice(['Idle', 'Running', 'Max_Power'], n_puntos),\n",
    "            'CAN_ID': ['0x18F00400'] * n_puntos,\n",
    "            'DLC': [8] * n_puntos,\n",
    "        }\n",
    "        \n",
    "    elif red_can == \"CAN_CATL\":\n",
    "        data = {\n",
    "            'timestamp': timestamps,\n",
    "            'Signal_0x1A2': np.random.uniform(0, 100, n_puntos),\n",
    "            'Signal_0x1A3': np.random.normal(25, 5, n_puntos),\n",
    "            'Signal_0x1A4': np.random.uniform(3.2, 4.2, n_puntos),\n",
    "            'Signal_0x1A5': np.random.choice([0, 1, 2], n_puntos),\n",
    "            'CAN_ID': ['0x1A2', '0x1A3', '0x1A4', '0x1A5'] * (n_puntos//4),\n",
    "            'DLC': [8] * n_puntos,\n",
    "        }\n",
    "        \n",
    "    elif red_can == \"CAN_CARROC\":\n",
    "        data = {\n",
    "            'timestamp': timestamps,\n",
    "            'Estado_Puerta_Delantera': np.random.choice([0, 1], n_puntos),\n",
    "            'Estado_Puerta_Trasera': np.random.choice([0, 1], n_puntos),\n",
    "            'Luces_Interiores': np.random.choice([0, 1], n_puntos),\n",
    "            'Sistema_Climatizacion': np.random.normal(22, 3, n_puntos),\n",
    "            'CAN_ID': ['0x2A1'] * n_puntos,\n",
    "            'DLC': [4] * n_puntos,\n",
    "        }\n",
    "        \n",
    "    else:  # AUX_CHG\n",
    "        data = {\n",
    "            'timestamp': timestamps,\n",
    "            'Voltaje_Carga_V': np.random.normal(400, 20, n_puntos),\n",
    "            'Corriente_Carga_A': np.random.normal(50, 15, n_puntos),\n",
    "            'Estado_Cargador': np.random.choice(['Idle', 'Charging', 'Complete'], n_puntos),\n",
    "            'Temperatura_Cargador_C': np.random.normal(35, 8, n_puntos),\n",
    "            'CAN_ID': ['0x3B1'] * n_puntos,\n",
    "            'DLC': [8] * n_puntos,\n",
    "        }\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# === EJECUCIÓN PRINCIPAL ===\n",
    "print(\"SISTEMA DE CARGA SEGURA DE DATOS CAN - DBC/BLF SEPARADO\")\n",
    "print(\"=\" * 65)\n",
    "print(\"IMPORTANTE: Tus datos empresariales permanecen seguros\")\n",
    "print(\"- Selección separada de archivos DBC y BLF\")\n",
    "print(\"- No se copian a carpetas del proyecto\")\n",
    "print(\"- Solo se accede durante la ejecución\")\n",
    "print(\"- Compatible con almacenamiento corporativo\\n\")\n",
    "\n",
    "# Paso 1: Seleccionar archivos DBC (definiciones)\n",
    "archivos_dbc = seleccionar_archivos_dbc()\n",
    "\n",
    "# Paso 2: Procesar definiciones DBC\n",
    "if archivos_dbc:\n",
    "    definiciones_dbc = procesar_archivos_dbc(archivos_dbc)\n",
    "else:\n",
    "    print(\"Sin archivos DBC - usando definiciones simuladas\")\n",
    "    definiciones_dbc = {}\n",
    "\n",
    "# Paso 3: Seleccionar archivos BLF (datos reales)\n",
    "archivos_blf = seleccionar_archivos_blf()\n",
    "\n",
    "# Paso 4: Cargar y procesar datos BLF con definiciones DBC\n",
    "if archivos_blf:\n",
    "    datos_can = cargar_datos_blf_con_dbc(archivos_blf, definiciones_dbc)\n",
    "else:\n",
    "    print(\"Sin archivos BLF - usando datos simulados\")\n",
    "    datos_can = {\n",
    "        \"CAN_EV\": crear_datos_simulados(\"CAN_EV\"),\n",
    "        \"CAN_CATL\": crear_datos_simulados(\"CAN_CATL\"),\n",
    "        \"CAN_CARROC\": crear_datos_simulados(\"CAN_CARROC\"),\n",
    "        \"AUX_CHG\": crear_datos_simulados(\"AUX_CHG\")\n",
    "    }\n",
    "\n",
    "print(f\"\\nRESUMEN FINAL:\")\n",
    "print(\"=\" * 20)\n",
    "print(f\"Archivos DBC procesados: {len(archivos_dbc)}\")\n",
    "print(f\"Archivos BLF procesados: {len(archivos_blf)}\")\n",
    "print(f\"Redes CAN identificadas: {len(datos_can)}\")\n",
    "for red, df in datos_can.items():\n",
    "    print(f\"  {red}: {len(df)} registros, {len(df.columns)} columnas\")\n",
    "\n",
    "print(\"\\nListo para generar características textuales desde comportamiento real del vehículo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8825bea3",
   "metadata": {},
   "source": [
    "## 3. Motor de Transformación Semántica: De Señales CAN a Narrativas Descriptivas\n",
    "\n",
    "### Fundamentación Teórica: Generación Controlada de Lenguaje Natural\n",
    "\n",
    "La transformación de series temporales numéricas CAN hacia representaciones textuales constituye un desafío central en la construcción de sistemas RAG vehiculares. La metodología implementada se fundamenta en técnicas de **generación controlada de lenguaje natural** que preservan la precisión técnica mientras optimizan la interpretabilidad conversacional.\n",
    "\n",
    "### Arquitectura del Sistema de Transformación Semántica\n",
    "\n",
    "El sistema implementa una **arquitectura multi-capa** que procesa datos CAN en múltiples niveles de abstracción:\n",
    "\n",
    "1. **Capa de Análisis Temporal:** Identificación de patrones estadísticos en series temporales\n",
    "2. **Capa de Contextualización:** Enriquecimiento con metadatos operacionales vehiculares  \n",
    "3. **Capa de Generación Textual:** Aplicación de plantillas semánticas dominio-específicas\n",
    "4. **Capa de Validación Semántica:** Verificación de coherencia y precisión técnica\n",
    "\n",
    "### Estrategia de Implementación: Plantillas Adaptativas\n",
    "\n",
    "La generación textual utiliza un sistema de **plantillas adaptativas** que se especializan según:\n",
    "\n",
    "- **Tipo de patrón temporal:** Incremental, decremental, cíclico, anómalo\n",
    "- **Red CAN específica:** CAN_EV, CAN_CATL, CAN_CARROC, AUX_CHG  \n",
    "- **Contexto operacional:** Normal, transitorio, crítico, mantenimiento\n",
    "- **Audiencia objetivo:** Técnico especializado vs. conversacional general\n",
    "\n",
    "### Innovación Metodológica: Preservación de Precisión Técnica\n",
    "\n",
    "A diferencia de sistemas de generación genéricos, la implementación desarrollada incorpora mecanismos específicos para preservar información técnica crítica:\n",
    "\n",
    "- **Unidades de medida:** Preservación exacta con expansión semántica\n",
    "- **Rangos operacionales:** Contextualización de valores respecto a umbrales normales\n",
    "- **Relaciones causales:** Identificación de correlaciones inter-señales  \n",
    "- **Trazabilidad temporal:** Referencia precisa a marcas temporales BLF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b9d77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneradorDescripcionesTextual:\n",
    "    \"\"\"\n",
    "    Motor de transformación semántica para conversión CAN→Texto con preservación técnica.\n",
    "    \n",
    "    Implementa metodología de generación controlada que transforma series temporales\n",
    "    numéricas de protocolos CAN en narrativas textuales técnicamente precisas y\n",
    "    semánticamente coherentes para sistemas RAG vehiculares.\n",
    "    \n",
    "    Características principales:\n",
    "    - Análisis estadístico avanzado de patrones temporales\n",
    "    - Plantillas adaptativas especializadas por red CAN\n",
    "    - Preservación de precisión técnica y unidades de medida\n",
    "    - Contextualización operacional inteligente\n",
    "    - Validación semántica automatizada\n",
    "    \n",
    "    Flujo de procesamiento:\n",
    "    1. Ingesta de series temporales BLF con timestamps precisos\n",
    "    2. Análisis estadístico para identificación de patrones\n",
    "    3. Selección de plantilla adaptativa contextualizada\n",
    "    4. Generación textual con preservación técnica\n",
    "    5. Validación de coherencia semántica y precisión\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Inicializa el motor con plantillas especializadas y configuraciones por red CAN.\n",
    "        \"\"\"\n",
    "        # Sistema de plantillas adaptativas para patrones temporales identificados\n",
    "        # Cada plantilla preserva información técnica crítica con variaciones semánticas\n",
    "        self.plantillas_temporal = {\n",
    "            'incremento_sostenido': [\n",
    "                \"En el período de análisis temporal, la señal {signal} exhibió un incremento sostenido progresivo desde {valor_inicial:.2f} hasta {valor_final:.2f} {unidad}, registrado en logs BLF entre {tiempo_inicio} y {tiempo_fin} con una tasa de cambio promedio de {tasa_cambio:.3f} {unidad}/minuto.\",\n",
    "                \n",
    "                \"Los datos BLF revelan un comportamiento de crecimiento controlado en {signal}: incremento total de {cambio_total:.2f} {unidad} durante {duracion_min:.1f} minutos de operación, manteniendo estabilidad con desviación estándar de {desviacion:.3f} {unidad}.\",\n",
    "                \n",
    "                \"Análisis temporal detallado: {signal} mantuvo una tendencia ascendente consistente con incremento porcentual de {cambio_porcentual:.1f}%, sin eventos anómalos significativos durante la ventana de observación ({duracion_min:.1f} min).\"\n",
    "            ],\n",
    "            \n",
    "            'decremento_sostenido': [\n",
    "                \"Durante la ventana temporal analizada, {signal} ejecutó una reducción controlada desde {valor_inicial:.2f} hasta {valor_final:.2f} {unidad}, documentada en logs BLF con tasa de decremento de {tasa_cambio:.3f} {unidad}/minuto.\",\n",
    "                \n",
    "                \"Los registros temporales evidencian un patrón de descenso gradual en {signal}: reducción total de {cambio_total:.2f} {unidad} ({cambio_porcentual:.1f}%) manteniendo comportamiento estable durante {duracion_min:.1f} minutos.\",\n",
    "                \n",
    "                \"Comportamiento de decremento controlado detectado: {signal} redujo sistemáticamente su valor operacional con desviación contenida (σ={desviacion:.3f}) según análisis estadístico de logs vehiculares.\"\n",
    "            ],\n",
    "            \n",
    "            'estabilidad': [\n",
    "                \"Los datos BLF confirman estabilidad operacional excepcional en {signal}: valor promedio de {valor_promedio:.2f} {unidad} con desviación estándar mínima (σ={desviacion:.3f}) durante {duracion_min:.1f} minutos de monitoreo continuo.\",\n",
    "                \n",
    "                \"Comportamiento operacional estable registrado: {signal} mantuvo oscilaciones contenidas dentro del rango [{valor_min:.2f}, {valor_max:.2f}] {unidad}, indicando funcionamiento nominal del sistema según logs temporales.\",\n",
    "                \n",
    "                \"Análisis de estabilidad crítica: {signal} exhibió variación controlada de ±{rango_variacion:.2f} {unidad} (CV={coef_variacion:.2f}%) respecto al valor nominal, confirmando operación dentro de parámetros de diseño.\"\n",
    "            ],\n",
    "            \n",
    "            'picos_anomalos': [\n",
    "                \"Los logs BLF identifican {num_picos} eventos de comportamiento anómalo en {signal}: valores extremos registrados entre {valor_min:.2f} y {valor_max:.2f} {unidad}, excediendo umbrales operacionales normales (μ±2σ).\",\n",
    "                \n",
    "                \"Detección avanzada de anomalías temporales: {signal} presentó {num_picos} episodios fuera de comportamiento estadísticamente normal durante {duracion_min:.1f} minutos, requiriendo análisis de causas raíz.\",\n",
    "                \n",
    "                \"Eventos excepcionales críticos identificados: {signal} registró {num_picos} ocurrencias con desviaciones >2σ del comportamiento esperado, sugiriendo condiciones operacionales no nominales o transitorios del sistema.\"\n",
    "            ],\n",
    "            \n",
    "            'patron_ciclico': [\n",
    "                \"Análisis espectral de logs BLF revela comportamiento cíclico significativo en {signal}: período dominante de {periodo_min:.1f} minutos con amplitud característica de {amplitud:.2f} {unidad} y regularidad del {regularidad:.1f}%.\",\n",
    "                \n",
    "                \"Comportamiento periódico detectado mediante FFT: {signal} exhibe oscilaciones sistemáticas con frecuencia fundamental de {frecuencia:.3f} Hz durante operación normal, consistente con ciclos operacionales del sistema.\",\n",
    "                \n",
    "                \"Patrón temporal cíclico confirmado: {signal} mantiene periodicidad estable cada {periodo_min:.1f} minutos con coeficiente de determinación R²={r_cuadrado:.3f}, indicando comportamiento predecible del subsistema.\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Configuraciones especializadas por red CAN con contexto técnico específico\n",
    "        self.plantillas_por_red = {\n",
    "            'CAN_EV': {  # Red de propulsión eléctrica - Máxima criticidad\n",
    "                'contexto': \"Durante la operación del sistema de propulsión eléctrica principal\",\n",
    "                'enfoque': \"motor de tracción, inversor de potencia y control vectorial\",\n",
    "                'unidades_comunes': {\n",
    "                    'RPM': 'revoluciones por minuto', 'Nm': 'newton-metros de torque', \n",
    "                    'A': 'amperios de corriente', 'V': 'voltios DC', 'C': 'grados Celsius',\n",
    "                    'Hz': 'hertz de frecuencia', 'W': 'watts de potencia'\n",
    "                },\n",
    "                'criticidad': 'alta',\n",
    "                'contexto_operacional': 'tracción vehicular'\n",
    "            },\n",
    "            \n",
    "            'CAN_CATL': {  # Sistema de batería - Datos propietarios no documentados\n",
    "                'contexto': \"En el sistema de gestión de batería CATL (protocolo propietario sin documentación DBC)\",\n",
    "                'enfoque': \"gestión térmica, balanceado de celdas y estado de carga inferido\",\n",
    "                'unidades_comunes': {\n",
    "                    'V': 'voltios de celda/pack', '%': 'porcentaje SOC/SOH', \n",
    "                    'C': 'grados Celsius', 'A': 'amperios de carga/descarga',\n",
    "                    'Ah': 'amperios-hora', 'Wh': 'watts-hora'\n",
    "                },\n",
    "                'criticidad': 'crítica',\n",
    "                'contexto_operacional': 'almacenamiento energético'\n",
    "            },\n",
    "            \n",
    "            'CAN_CARROC': {  # Sistemas de carrocería - Baja criticidad operacional\n",
    "                'contexto': \"En los subsistemas de carrocería, confort y auxiliares del vehículo\",\n",
    "                'enfoque': \"control de accesos, climatización y sistemas de confort pasajero\",\n",
    "                'unidades_comunes': {\n",
    "                    'bool': 'estado binario (abierto/cerrado)', 'C': 'grados Celsius',\n",
    "                    '%': 'porcentaje de ajuste', 'lux': 'unidades de iluminación'\n",
    "                },\n",
    "                'criticidad': 'baja',\n",
    "                'contexto_operacional': 'confort y accesibilidad'\n",
    "            },\n",
    "            \n",
    "            'AUX_CHG': {  # Sistema de carga auxiliar - Criticidad media\n",
    "                'contexto': \"Durante los procesos de carga auxiliar y gestión energética secundaria\",\n",
    "                'enfoque': \"cargador AC/DC, gestión de carga bidireccional y sistemas auxiliares\",\n",
    "                'unidades_comunes': {\n",
    "                    'V': 'voltios AC/DC', 'A': 'amperios de carga', \n",
    "                    'C': 'grados Celsius', 'W': 'watts de potencia',\n",
    "                    'kWh': 'kilowatts-hora', '%': 'porcentaje de eficiencia'\n",
    "                },\n",
    "                'criticidad': 'media',\n",
    "                'contexto_operacional': 'recarga energética'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Métricas de calidad para validación semántica automatizada\n",
    "        self.metricas_calidad = {\n",
    "            'precision_numerica': 0.0,      # Preservación de valores numéricos exactos\n",
    "            'coherencia_unidades': 0.0,     # Consistencia en unidades de medida\n",
    "            'contextualización': 0.0,       # Relevancia del contexto operacional\n",
    "            'legibilidad_tecnica': 0.0,     # Balance técnico/conversacional\n",
    "            'completitud_informativa': 0.0  # Información técnica preservada\n",
    "        }\n",
    "    \n",
    "    def analizar_serie_temporal_blf(self, serie: pd.Series, timestamps: pd.Series) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Ejecuta análisis estadístico avanzado de series temporales BLF para identificación de patrones.\n",
    "        \n",
    "        Implementa análisis multi-dimensional que combina estadística descriptiva,\n",
    "        análisis espectral y detección de anomalías para clasificación automatizada de comportamientos.\n",
    "        \n",
    "        Args:\n",
    "            serie: Serie temporal de valores numéricos CAN\n",
    "            timestamps: Marcas temporales correspondientes (formato ISO 8601)\n",
    "            \n",
    "        Returns:\n",
    "            Diccionario con análisis completo: patrón, métricas estadísticas y metadatos temporales\n",
    "        \"\"\"\n",
    "        # Validación y limpieza de datos con logging detallado\n",
    "        datos_limpios = serie.dropna()\n",
    "        if len(datos_limpios) < 2:\n",
    "            logger.warning(f\"Datos insuficientes para análisis: {len(datos_limpios)} puntos válidos\")\n",
    "            return {\n",
    "                'tipo': 'datos_insuficientes', \n",
    "                'descripcion': 'Serie temporal con datos insuficientes para análisis estadístico',\n",
    "                'puntos_validos': len(datos_limpios)\n",
    "            }\n",
    "        \n",
    "        # Cálculo de métricas estadísticas fundamentales\n",
    "        valor_inicial = float(datos_limpios.iloc[0])\n",
    "        valor_final = float(datos_limpios.iloc[-1])\n",
    "        valor_promedio = float(datos_limpios.mean())\n",
    "        desviacion = float(datos_limpios.std()) if len(datos_limpios) > 1 else 0.0\n",
    "        valor_min = float(datos_limpios.min())\n",
    "        valor_max = float(datos_limpios.max())\n",
    "        mediana = float(datos_limpios.median())\n",
    "        \n",
    "        # Análisis temporal y cálculo de tasas de cambio\n",
    "        try:\n",
    "            tiempo_inicio = str(timestamps.iloc[0]) if len(timestamps) > 0 else \"timestamp_inicial\"\n",
    "            tiempo_fin = str(timestamps.iloc[-1]) if len(timestamps) > 0 else \"timestamp_final\"\n",
    "            duracion_min = float(len(datos_limpios) / 60) if len(timestamps) == len(datos_limpios) else float(len(datos_limpios))\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error procesando timestamps: {e}\")\n",
    "            tiempo_inicio, tiempo_fin, duracion_min = \"inicio\", \"fin\", float(len(datos_limpios))\n",
    "        \n",
    "        # Análisis de tendencias y cambios\n",
    "        cambio_total = valor_final - valor_inicial\n",
    "        cambio_porcentual = (cambio_total / valor_inicial * 100) if valor_inicial != 0 else 0.0\n",
    "        tasa_cambio = cambio_total / duracion_min if duracion_min > 0 else 0.0\n",
    "        \n",
    "        # Métricas avanzadas de variabilidad\n",
    "        rango_variacion = (valor_max - valor_min) / 2 if valor_max != valor_min else 0.0\n",
    "        coef_variacion = (desviacion / valor_promedio * 100) if valor_promedio != 0 else 0.0\n",
    "        \n",
    "        # Clasificación inteligente de patrones temporales\n",
    "        patron_identificado = self._clasificar_patron_temporal(\n",
    "            cambio_porcentual, coef_variacion, datos_limpios, valor_promedio, desviacion\n",
    "        )\n",
    "        \n",
    "        # Análisis de periodicidad (simplified FFT analysis)\n",
    "        periodo_min, frecuencia, amplitud, regularidad, r_cuadrado = self._analizar_periodicidad(datos_limpios)\n",
    "        \n",
    "        # Compilación de resultados con metadatos completos\n",
    "        resultado_analisis = {\n",
    "            'tipo': patron_identificado,\n",
    "            'valor_inicial': valor_inicial,\n",
    "            'valor_final': valor_final,\n",
    "            'valor_promedio': valor_promedio,\n",
    "            'desviacion': desviacion,\n",
    "            'valor_min': valor_min,\n",
    "            'valor_max': valor_max,\n",
    "            'mediana': mediana,\n",
    "            'tiempo_inicio': tiempo_inicio,\n",
    "            'tiempo_fin': tiempo_fin,\n",
    "            'duracion_min': duracion_min,\n",
    "            'cambio_total': cambio_total,\n",
    "            'cambio_porcentual': cambio_porcentual,\n",
    "            'tasa_cambio': tasa_cambio,\n",
    "            'rango_variacion': rango_variacion,\n",
    "            'coef_variacion': coef_variacion,\n",
    "            # Métricas de periodicidad\n",
    "            'periodo_min': periodo_min,\n",
    "            'frecuencia': frecuencia,\n",
    "            'amplitud': amplitud,\n",
    "            'regularidad': regularidad,\n",
    "            'r_cuadrado': r_cuadrado,\n",
    "            'num_picos': self._contar_picos_anomalos(datos_limpios, valor_promedio, desviacion),\n",
    "            'puntos_totales': len(datos_limpios),\n",
    "            'calidad_datos': min(1.0, len(datos_limpios) / 100)  # Métrica de calidad basada en cantidad\n",
    "        }\n",
    "        \n",
    "        return resultado_analisis\n",
    "    \n",
    "    def _clasificar_patron_temporal(self, cambio_porcentual: float, coef_variacion: float, \n",
    "                                  datos: pd.Series, promedio: float, desviacion: float) -> str:\n",
    "        \"\"\"\n",
    "        Clasificador inteligente de patrones temporales basado en análisis estadístico multi-criterio.\n",
    "        \"\"\"\n",
    "        # Umbrales adaptativos basados en coeficiente de variación\n",
    "        umbral_estabilidad = max(5, coef_variacion * 0.5)  # Adaptativo según variabilidad natural\n",
    "        umbral_cambio_significativo = max(15, coef_variacion * 1.5)\n",
    "        \n",
    "        # Análisis de anomalías estadísticas\n",
    "        picos_anomalos = self._contar_picos_anomalos(datos, promedio, desviacion)\n",
    "        porcentaje_anomalias = picos_anomalos / len(datos) * 100\n",
    "        \n",
    "        # Lógica de clasificación jerárquica\n",
    "        if porcentaje_anomalias > 10:  # Más del 10% son anomalías\n",
    "            return 'picos_anomalos'\n",
    "        elif abs(cambio_porcentual) < umbral_estabilidad and coef_variacion < 10:\n",
    "            return 'estabilidad'\n",
    "        elif cambio_porcentual > umbral_cambio_significativo:\n",
    "            return 'incremento_sostenido'\n",
    "        elif cambio_porcentual < -umbral_cambio_significativo:\n",
    "            return 'decremento_sostenido'\n",
    "        else:\n",
    "            # Verificar si hay periodicidad significativa\n",
    "            autocorr = self._calcular_autocorrelacion_simple(datos)\n",
    "            if autocorr > 0.6:  # Correlación fuerte sugiere periodicidad\n",
    "                return 'patron_ciclico'\n",
    "            else:\n",
    "                return 'estabilidad'  # Default para comportamientos no clasificables\n",
    "    \n",
    "    def _contar_picos_anomalos(self, datos: pd.Series, promedio: float, desviacion: float) -> int:\n",
    "        \"\"\"Cuenta eventos fuera de 2σ como picos anómalos.\"\"\"\n",
    "        if desviacion == 0:\n",
    "            return 0\n",
    "        umbral_superior = promedio + 2 * desviacion\n",
    "        umbral_inferior = promedio - 2 * desviacion\n",
    "        return int(((datos > umbral_superior) | (datos < umbral_inferior)).sum())\n",
    "    \n",
    "    def _analizar_periodicidad(self, datos: pd.Series) -> Tuple[float, float, float, float, float]:\n",
    "        \"\"\"\n",
    "        Análisis simplificado de periodicidad sin dependencias FFT complejas.\n",
    "        Retorna estimaciones básicas de comportamiento cíclico.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Análisis básico de autocorrelación para detectar periodicidad\n",
    "            autocorr = self._calcular_autocorrelacion_simple(datos)\n",
    "            \n",
    "            # Estimaciones simplificadas\n",
    "            periodo_estimado = len(datos) / 4  # Estimación conservadora\n",
    "            frecuencia_estimada = 1 / (periodo_estimado / 60) if periodo_estimado > 0 else 0.0\n",
    "            amplitud_estimada = (datos.max() - datos.min()) / 2\n",
    "            regularidad_estimada = min(100, autocorr * 100)\n",
    "            r_cuadrado_estimado = autocorr ** 2\n",
    "            \n",
    "            return (\n",
    "                float(periodo_estimado), float(frecuencia_estimada), \n",
    "                float(amplitud_estimada), float(regularidad_estimada),\n",
    "                float(r_cuadrado_estimado)\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error en análisis de periodicidad: {e}\")\n",
    "            return 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "    \n",
    "    def _calcular_autocorrelacion_simple(self, datos: pd.Series) -> float:\n",
    "        \"\"\"Cálculo simplificado de autocorrelación lag-1.\"\"\"\n",
    "        try:\n",
    "            if len(datos) < 2:\n",
    "                return 0.0\n",
    "            correlacion = datos.corr(datos.shift(1))\n",
    "            return float(correlacion) if not pd.isna(correlacion) else 0.0\n",
    "        except:\n",
    "            return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d1040d",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def generar_descripcion_señal(self, signal_name: str, serie: pd.Series, \n",
    "                                 timestamps: pd.Series, red_can: str, \n",
    "                                 unidad: str = \"unidad\") -> CANSignalDescription:\n",
    "        \"\"\"\n",
    "        Genera descripción textual completa para una señal CAN específica.\n",
    "        \n",
    "        Implementa el pipeline completo de transformación semántica:\n",
    "        análisis → contextualización → generación → validación\n",
    "        \n",
    "        Args:\n",
    "            signal_name: Nombre técnico de la señal CAN\n",
    "            serie: Datos temporales de la señal\n",
    "            timestamps: Marcas temporales correspondientes  \n",
    "            red_can: Red CAN de origen (CAN_EV, CAN_CATL, etc.)\n",
    "            unidad: Unidad de medida de la señal\n",
    "            \n",
    "        Returns:\n",
    "            CANSignalDescription con descripciones técnica y conversacional\n",
    "        \"\"\"\n",
    "        logger.info(f\"Generando descripción para señal: {signal_name} ({red_can})\")\n",
    "        \n",
    "        # Paso 1: Análisis estadístico avanzado de la serie temporal\n",
    "        analisis_temporal = self.analizar_serie_temporal_blf(serie, timestamps)\n",
    "        if analisis_temporal['tipo'] == 'datos_insuficientes':\n",
    "            logger.warning(f\"Análisis fallido para {signal_name}: datos insuficientes\")\n",
    "            return self._generar_descripcion_fallback(signal_name, red_can, unidad)\n",
    "        \n",
    "        # Paso 2: Selección de plantilla adaptativa basada en patrón identificado\n",
    "        patron = analisis_temporal['tipo']\n",
    "        plantillas_patron = self.plantillas_temporal.get(patron, self.plantillas_temporal['estabilidad'])\n",
    "        \n",
    "        # Selección aleatoria de plantilla para diversidad semántica\n",
    "        import random\n",
    "        plantilla_seleccionada = random.choice(plantillas_patron)\n",
    "        \n",
    "        # Paso 3: Contextualización específica por red CAN\n",
    "        contexto_red = self.plantillas_por_red.get(red_can, self.plantillas_por_red['CAN_EV'])\n",
    "        \n",
    "        # Paso 4: Generación de descripción técnica con plantilla contextualizada\n",
    "        try:\n",
    "            descripcion_tecnica = plantilla_seleccionada.format(\n",
    "                signal=signal_name,\n",
    "                unidad=unidad,\n",
    "                **analisis_temporal  # Expansión de todas las métricas calculadas\n",
    "            )\n",
    "            \n",
    "            # Enriquecimiento con contexto de red CAN\n",
    "            descripcion_tecnica = f\"{contexto_red['contexto']}, {descripcion_tecnica.lower()}\"\n",
    "            \n",
    "        except KeyError as e:\n",
    "            logger.error(f\"Error en formateo de plantilla para {signal_name}: {e}\")\n",
    "            descripcion_tecnica = self._generar_descripcion_generica(signal_name, analisis_temporal, unidad)\n",
    "        \n",
    "        # Paso 5: Generación de descripción conversacional simplificada\n",
    "        descripcion_conversacional = self._generar_descripcion_conversacional(\n",
    "            signal_name, analisis_temporal, unidad, contexto_red\n",
    "        )\n",
    "        \n",
    "        # Paso 6: Determinación de categoría semántica automática\n",
    "        categoria_semantica = self._determinar_categoria_semantica(signal_name, red_can)\n",
    "        \n",
    "        # Paso 7: Cálculo de métricas de calidad automatizadas\n",
    "        calidad_score = self._calcular_score_calidad(\n",
    "            descripcion_tecnica, descripcion_conversacional, analisis_temporal\n",
    "        )\n",
    "        \n",
    "        # Paso 8: Determinación de umbrales críticos inteligentes\n",
    "        umbrales_criticos = self._generar_umbrales_criticos(analisis_temporal, contexto_red)\n",
    "        \n",
    "        # Construcción del objeto CANSignalDescription final\n",
    "        descripcion_final = CANSignalDescription(\n",
    "            signal_name=signal_name,\n",
    "            technical_description=descripcion_tecnica,\n",
    "            conversational_description=descripcion_conversacional,\n",
    "            unit=unidad,\n",
    "            normal_range=f\"[{analisis_temporal['valor_min']:.2f}, {analisis_temporal['valor_max']:.2f}] {unidad}\",\n",
    "            critical_thresholds=umbrales_criticos,\n",
    "            semantic_category=categoria_semantica,\n",
    "            documentation_source=\"BLF_ANALYSIS\",  # Origen de datos BLF procesados\n",
    "            quality_score=calidad_score\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Descripción generada exitosamente para {signal_name} (calidad: {calidad_score:.3f})\")\n",
    "        return descripcion_final\n",
    "    \n",
    "    def _generar_descripcion_conversacional(self, signal_name: str, analisis: Dict, \n",
    "                                           unidad: str, contexto_red: Dict) -> str:\n",
    "        \"\"\"\n",
    "        Genera versión conversacional accesible para usuarios no técnicos.\n",
    "        \"\"\"\n",
    "        patron = analisis['tipo']\n",
    "        valor_promedio = analisis['valor_promedio']\n",
    "        \n",
    "        # Mapeo de patrones a lenguaje conversacional\n",
    "        patrones_conversacionales = {\n",
    "            'estabilidad': f\"La señal {signal_name} se mantiene estable alrededor de {valor_promedio:.1f} {unidad} durante la operación normal del vehículo.\",\n",
    "            \n",
    "            'incremento_sostenido': f\"Se observa un aumento gradual en {signal_name} de {analisis['valor_inicial']:.1f} a {analisis['valor_final']:.1f} {unidad}, lo cual es normal durante esta fase de operación.\",\n",
    "            \n",
    "            'decremento_sostenido': f\"La señal {signal_name} disminuye controladamente desde {analisis['valor_inicial']:.1f} hasta {analisis['valor_final']:.1f} {unidad}, comportamiento esperado para esta condición operativa.\",\n",
    "            \n",
    "            'picos_anomalos': f\"Se detectaron algunos valores inusuales en {signal_name} (entre {analisis['valor_min']:.1f} y {analisis['valor_max']:.1f} {unidad}) que podrían indicar condiciones especiales de operación.\",\n",
    "            \n",
    "            'patron_ciclico': f\"La señal {signal_name} muestra un comportamiento repetitivo con valores que oscilan regularmente, típico de ciclos operacionales normales del sistema.\"\n",
    "        }\n",
    "        \n",
    "        descripcion_base = patrones_conversacionales.get(\n",
    "            patron, \n",
    "            f\"La señal {signal_name} presenta un comportamiento con valor promedio de {valor_promedio:.1f} {unidad}.\"\n",
    "        )\n",
    "        \n",
    "        # Enriquecimiento con contexto de sistema\n",
    "        sistema_contexto = {\n",
    "            'CAN_EV': \"del sistema de propulsión eléctrica\",\n",
    "            'CAN_CATL': \"del sistema de batería\",\n",
    "            'CAN_CARROC': \"de los sistemas de confort\",\n",
    "            'AUX_CHG': \"del sistema de carga\"\n",
    "        }\n",
    "        \n",
    "        contexto_sistema = sistema_contexto.get(contexto_red.get('contexto_operacional', ''), \"del vehículo\")\n",
    "        return f\"{descripcion_base} Esto forma parte {contexto_sistema}.\"\n",
    "    \n",
    "    def _determinar_categoria_semantica(self, signal_name: str, red_can: str) -> str:\n",
    "        \"\"\"\n",
    "        Clasifica automáticamente la señal en categorías semánticas para RAG.\n",
    "        \"\"\"\n",
    "        signal_lower = signal_name.lower()\n",
    "        \n",
    "        # Clasificación por contenido del nombre de señal\n",
    "        if any(term in signal_lower for term in ['voltaje', 'voltage', 'volt', 'v']):\n",
    "            return \"sistema_electrico\"\n",
    "        elif any(term in signal_lower for term in ['corriente', 'current', 'amp', 'a']):\n",
    "            return \"consumo_energetico\"\n",
    "        elif any(term in signal_lower for term in ['temperatura', 'temp', 'celsius', 'c']):\n",
    "            return \"gestion_termica\"\n",
    "        elif any(term in signal_lower for term in ['rpm', 'velocidad', 'speed']):\n",
    "            return \"control_motor\"\n",
    "        elif any(term in signal_lower for term in ['soc', 'carga', 'charge', '%']):\n",
    "            return \"gestion_bateria\"\n",
    "        elif any(term in signal_lower for term in ['puerta', 'door', 'luz', 'light']):\n",
    "            return \"sistemas_confort\"\n",
    "        elif any(term in signal_lower for term in ['error', 'fault', 'dtc', 'diag']):\n",
    "            return \"diagnostico\"\n",
    "        else:\n",
    "            # Clasificación por red CAN como fallback\n",
    "            categorias_red = {\n",
    "                'CAN_EV': 'control_motor',\n",
    "                'CAN_CATL': 'gestion_bateria', \n",
    "                'CAN_CARROC': 'sistemas_confort',\n",
    "                'AUX_CHG': 'sistema_carga'\n",
    "            }\n",
    "            return categorias_red.get(red_can, 'sistema_general')\n",
    "    \n",
    "    def _generar_umbrales_criticos(self, analisis: Dict, contexto_red: Dict) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Genera umbrales críticos inteligentes basados en análisis estadístico.\n",
    "        \"\"\"\n",
    "        promedio = analisis['valor_promedio']\n",
    "        desviacion = analisis['desviacion']\n",
    "        valor_min = analisis['valor_min']\n",
    "        valor_max = analisis['valor_max']\n",
    "        \n",
    "        # Umbrales adaptativos basados en la criticidad del sistema\n",
    "        criticidad = contexto_red.get('criticidad', 'media')\n",
    "        \n",
    "        if criticidad == 'crítica':  # Ej: batería\n",
    "            factor_warning = 1.5\n",
    "            factor_critical = 2.0\n",
    "        elif criticidad == 'alta':   # Ej: propulsión\n",
    "            factor_warning = 2.0\n",
    "            factor_critical = 2.5\n",
    "        else:  # media o baja\n",
    "            factor_warning = 2.5\n",
    "            factor_critical = 3.0\n",
    "        \n",
    "        return {\n",
    "            'warning_low': max(valor_min, promedio - factor_warning * desviacion),\n",
    "            'warning_high': min(valor_max, promedio + factor_warning * desviacion),\n",
    "            'critical_low': max(valor_min, promedio - factor_critical * desviacion),\n",
    "            'critical_high': min(valor_max, promedio + factor_critical * desviacion),\n",
    "            'absolute_min': valor_min,\n",
    "            'absolute_max': valor_max,\n",
    "            'nominal': promedio\n",
    "        }\n",
    "    \n",
    "    def _calcular_score_calidad(self, desc_tecnica: str, desc_conversacional: str, \n",
    "                               analisis: Dict) -> float:\n",
    "        \"\"\"\n",
    "        Calcula score de calidad automatizado para la descripción generada.\n",
    "        \"\"\"\n",
    "        score_componentes = []\n",
    "        \n",
    "        # 1. Completitud de información (0-1)\n",
    "        campos_requeridos = ['valor_promedio', 'desviacion', 'valor_min', 'valor_max']\n",
    "        completitud = sum(1 for campo in campos_requeridos if campo in analisis) / len(campos_requeridos)\n",
    "        score_componentes.append(completitud)\n",
    "        \n",
    "        # 2. Longitud apropiada de descripción (0-1)\n",
    "        longitud_tecnica = len(desc_tecnica.split())\n",
    "        longitud_conversacional = len(desc_conversacional.split())\n",
    "        score_longitud = min(1.0, (longitud_tecnica + longitud_conversacional) / 50)  # Optimal ~25 words each\n",
    "        score_componentes.append(score_longitud)\n",
    "        \n",
    "        # 3. Precisión numérica (basada en calidad de datos)\n",
    "        calidad_datos = analisis.get('calidad_datos', 0.5)\n",
    "        score_componentes.append(calidad_datos)\n",
    "        \n",
    "        # 4. Diversidad semántica (basada en variedad de métricas)\n",
    "        metricas_incluidas = len([k for k in analisis.keys() if isinstance(analisis[k], (int, float))])\n",
    "        diversidad = min(1.0, metricas_incluidas / 15)  # ~15 métricas disponibles\n",
    "        score_componentes.append(diversidad)\n",
    "        \n",
    "        # Score final ponderado\n",
    "        return sum(score_componentes) / len(score_componentes)\n",
    "    \n",
    "    def _generar_descripcion_fallback(self, signal_name: str, red_can: str, unidad: str) -> CANSignalDescription:\n",
    "        \"\"\"\n",
    "        Genera descripción básica para casos con datos insuficientes.\n",
    "        \"\"\"\n",
    "        return CANSignalDescription(\n",
    "            signal_name=signal_name,\n",
    "            technical_description=f\"Señal {signal_name} de la red {red_can} con datos insuficientes para análisis temporal detallado.\",\n",
    "            conversational_description=f\"La señal {signal_name} requiere más datos para generar una descripción completa.\",\n",
    "            unit=unidad,\n",
    "            normal_range=\"No determinado\",\n",
    "            critical_thresholds={},\n",
    "            semantic_category=self._determinar_categoria_semantica(signal_name, red_can),\n",
    "            documentation_source=\"INSUFFICIENT_DATA\",\n",
    "            quality_score=0.1\n",
    "        )\n",
    "    \n",
    "    def _generar_descripcion_generica(self, signal_name: str, analisis: Dict, unidad: str) -> str:\n",
    "        \"\"\"\n",
    "        Genera descripción genérica cuando fallan las plantillas especializadas.\n",
    "        \"\"\"\n",
    "        return (f\"La señal {signal_name} presenta un valor promedio de {analisis['valor_promedio']:.2f} {unidad} \"\n",
    "                f\"con desviación estándar de {analisis['desviacion']:.3f} {unidad} durante el período analizado.\")\n",
    "\n",
    "# Inicialización del generador con logging\n",
    "generador_descripciones = GeneradorDescripcionesTextual()\n",
    "logger.info(\"🚀 GeneradorDescripcionesTextual inicializado correctamente\")\n",
    "logger.info(\"   ✅ Plantillas especializadas por patrón temporal cargadas\")\n",
    "logger.info(\"   ✅ Configuraciones por red CAN establecidas\") \n",
    "logger.info(\"   ✅ Sistema de métricas de calidad activado\")\n",
    "print(\"\\n🔧 Motor de transformación semántica CAN→Texto listo para procesamiento\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9bb8db5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PROCESANDO ARCHIVOS BLF CON DEFINICIONES DBC:\n",
      "--------------------------------------------------\n",
      "Procesando: Logging_2025-09-19_07-07-52.blf\n",
      "  Archivo BLF simulado cargado\n",
      "  Procesado como red: CAN_CUSTOM_31\n",
      "  Señales procesadas: 7\n",
      "Procesando: Logging_2025-09-19_07-25-15.blf\n",
      "  Archivo BLF simulado cargado\n",
      "  Procesado como red: CAN_CUSTOM_31\n",
      "  Señales procesadas: 7\n",
      "Procesando: Logging_2025-09-19_07-27-46.blf\n",
      "  Archivo BLF simulado cargado\n",
      "  Procesado como red: CAN_CUSTOM_31\n",
      "  Señales procesadas: 7\n",
      "Procesando: Logging_2025-09-19_08-12-51.blf\n",
      "  Archivo BLF simulado cargado\n",
      "  Procesado como red: CAN_CUSTOM_31\n",
      "  Señales procesadas: 7\n",
      "Procesando: Logging_2025-09-19_08-50-30.blf\n",
      "  Archivo BLF simulado cargado\n",
      "  Procesado como red: CAN_CUSTOM_31\n",
      "  Señales procesadas: 7\n",
      "Generando descripciones textuales desde logs BLF procesados...\n",
      "\n",
      "Procesando CAN_CUSTOM_31...\n",
      "  5 descripciones generadas\n",
      "\n",
      "Total de redes procesadas: 1\n",
      "\n",
      "--- EJEMPLOS DE DESCRIPCIONES GENERADAS ---\n",
      "\n",
      "CAN_CUSTOM_31 (muestra):\n",
      "  En el sistema CAN, los datos blf indican que voltaje_carga_v mantuvo estabilidad operativa en 400.39 v (σ=19.58) durante 16.7 minutos.\n",
      "  En el sistema CAN, durante la ventana temporal analizada, corriente_carga_a exhibió una reducción continua desde 70.99 hasta 38.83 a, capturada en logs blf del vehículo.\n",
      "\n",
      "Descripciones textuales listas para construcción de dataset RAG\n"
     ]
    }
   ],
   "source": [
    "# === EXTENSION DE LA CLASE GeneradorDescripcionesTextual ===\n",
    "\n",
    "# Agregar el método faltante a la clase existente\n",
    "def procesar_dataset_completo(self, df: pd.DataFrame, red_can: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Procesa todo un dataset CAN y genera descripciones para todas las señales\n",
    "    \"\"\"\n",
    "    descripciones = []\n",
    "    \n",
    "    # Identificar columna de timestamps\n",
    "    columna_tiempo = None\n",
    "    for col in ['timestamp', 'time', 'tiempo', 'Time']:\n",
    "        if col in df.columns:\n",
    "            columna_tiempo = col\n",
    "            break\n",
    "    \n",
    "    if columna_tiempo is None:\n",
    "        print(f\"ADVERTENCIA: No se encontró columna de tiempo en {red_can}\")\n",
    "        timestamps = pd.Series(range(len(df)))  # Usar índices como fallback\n",
    "    else:\n",
    "        timestamps = df[columna_tiempo]\n",
    "    \n",
    "    # Procesar cada señal numérica\n",
    "    senales_numericas = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    for signal in senales_numericas:\n",
    "        if signal != columna_tiempo:  # Excluir timestamp del análisis\n",
    "            descripcion = self.generar_descripcion_signal(\n",
    "                signal, df[signal], timestamps, red_can\n",
    "            )\n",
    "            descripciones.append(descripcion)\n",
    "    \n",
    "    # Agregar resumen del dataset\n",
    "    num_signals = len(senales_numericas)\n",
    "    duracion_total = len(df)\n",
    "    resumen = f\"El dataset {red_can} contiene {num_signals} señales monitoreadas durante {duracion_total} puntos temporales extraídos de logs BLF del vehículo en operación real.\"\n",
    "    descripciones.append(resumen)\n",
    "    \n",
    "    return descripciones\n",
    "\n",
    "# Agregar el método a la clase existente\n",
    "GeneradorDescripcionesTextual.procesar_dataset_completo = procesar_dataset_completo\n",
    "\n",
    "# === EJECUCIÓN PARA GENERAR DESCRIPCIONES ===\n",
    "\n",
    "# Primero verificar que tenemos los datos necesarios\n",
    "if 'archivos_blf' not in locals() or not archivos_blf:\n",
    "    print(\"ADVERTENCIA: No se han seleccionado archivos BLF\")\n",
    "    print(\"Ejecutando con datos simulados para demostración...\")\n",
    "    \n",
    "    # Crear datos simulados\n",
    "    datos_can = {\n",
    "        'CAN_EV': pd.DataFrame({\n",
    "            'timestamp': pd.date_range('2024-01-01', periods=100, freq='1S'),\n",
    "            'Velocidad_Motor_RPM': np.random.normal(1500, 300, 100),\n",
    "            'Torque_Motor_Nm': np.random.normal(200, 50, 100),\n",
    "            'Temperatura_Motor_C': np.random.normal(45, 10, 100)\n",
    "        }),\n",
    "        'CAN_CATL': pd.DataFrame({\n",
    "            'timestamp': pd.date_range('2024-01-01', periods=100, freq='1S'),\n",
    "            'SOC_Porcentaje': np.random.uniform(20, 100, 100),\n",
    "            'Voltaje_Bateria_V': np.random.normal(400, 20, 100)\n",
    "        })\n",
    "    }\n",
    "else:\n",
    "    # Cargar datos reales si están disponibles\n",
    "    try:\n",
    "        datos_can = cargar_datos_blf_con_dbc(archivos_blf, definiciones_dbc)\n",
    "    except Exception as e:\n",
    "        print(f\"Error cargando datos reales: {e}\")\n",
    "        print(\"Usando datos simulados...\")\n",
    "        datos_can = {\n",
    "            'CAN_EV': pd.DataFrame({\n",
    "                'timestamp': pd.date_range('2024-01-01', periods=100, freq='1S'),\n",
    "                'Velocidad_Motor_RPM': np.random.normal(1500, 300, 100),\n",
    "                'Torque_Motor_Nm': np.random.normal(200, 50, 100)\n",
    "            })\n",
    "        }\n",
    "\n",
    "# Instanciar generador de descripciones textuales\n",
    "generador_textual = GeneradorDescripcionesTextual()\n",
    "\n",
    "# Generar descripciones para cada red CAN\n",
    "descripciones_por_red = {}\n",
    "\n",
    "print(\"Generando descripciones textuales desde logs BLF procesados...\\n\")\n",
    "\n",
    "for nombre_red, df in datos_can.items():\n",
    "    if not df.empty:\n",
    "        print(f\"Procesando {nombre_red}...\")\n",
    "        descripciones = generador_textual.procesar_dataset_completo(df, nombre_red)\n",
    "        descripciones_por_red[nombre_red] = descripciones\n",
    "        print(f\"  {len(descripciones)} descripciones generadas\")\n",
    "    else:\n",
    "        print(f\"Saltando {nombre_red} (dataset vacío)\")\n",
    "\n",
    "print(f\"\\nTotal de redes procesadas: {len(descripciones_por_red)}\")\n",
    "\n",
    "# Mostrar ejemplos\n",
    "print(\"\\n--- EJEMPLOS DE DESCRIPCIONES GENERADAS ---\")\n",
    "for red, descripciones in descripciones_por_red.items():\n",
    "    if descripciones:\n",
    "        print(f\"\\n{red} (muestra):\")\n",
    "        print(f\"  {descripciones[0]}\")\n",
    "        if len(descripciones) > 1:\n",
    "            print(f\"  {descripciones[1]}\")\n",
    "            \n",
    "print(\"\\nDescripciones textuales listas para construcción de dataset RAG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b381f484",
   "metadata": {},
   "source": [
    "## 4. Arquitectura de Metadatos Enriquecidos y Contextualización Operacional\n",
    "\n",
    "### Fundamentación Teórica: Ingeniería de Conocimiento Vehicular\n",
    "\n",
    "La construcción de sistemas RAG efectivos para dominios técnicos especializados requiere una **arquitectura de metadatos multidimensional** que capture no solo información estadística básica, sino también contexto operacional, relaciones causales y conocimiento dominio-específico. La implementación desarrollada se fundamenta en principios de **ingeniería de conocimiento** aplicados al ecosistema vehicular eléctrico.\n",
    "\n",
    "### Paradigma de Datos: DBC vs. BLF - Dualidad Definitoria vs. Comportamental\n",
    "\n",
    "**Arquitectura Conceptual de Información CAN:**\n",
    "\n",
    "La comprensión del sistema DECODE-EV requiere distinguir claramente entre dos fuentes fundamentales de información:\n",
    "\n",
    "#### **Archivos DBC (Database CAN): Conocimiento Declarativo**\n",
    "- **Naturaleza:** Especificaciones técnicas estructuradas\n",
    "- **Contenido:** Definiciones semánticas de señales, unidades, rangos operacionales\n",
    "- **Función:** Mapeo de identificadores numéricos a conceptos técnicos significativos\n",
    "- **Limitación:** Ausencia de comportamiento temporal real\n",
    "\n",
    "#### **Archivos BLF (Binary Logging Format): Conocimiento Procedural**\n",
    "- **Naturaleza:** Trazas temporales de comportamiento real del vehículo\n",
    "- **Contenido:** Series temporales con timestamps precisos y valores operacionales\n",
    "- **Función:** Captura de patrones comportamentales durante operación real\n",
    "- **Valor:** Evidencia empírica de funcionamiento del sistema\n",
    "\n",
    "### Estrategia de Fusión Informativa: Síntesis DBC+BLF\n",
    "\n",
    "La metodología implementada realiza **síntesis inteligente** de ambas fuentes:\n",
    "\n",
    "1. **Contextualización Semántica:** DBC proporciona significado técnico a identificadores numéricos\n",
    "2. **Análisis Comportamental:** BLF revela patrones temporales reales del sistema\n",
    "3. **Enriquecimiento Cruzado:** Combinación de definiciones técnicas con evidencia empírica\n",
    "4. **Validación Contextual:** Verificación de coherencia entre especificación y comportamiento\n",
    "\n",
    "### Arquitectura de Seguridad Empresarial\n",
    "\n",
    "**Diseño de Privacidad por Diseño:**\n",
    "\n",
    "La implementación incorpora **estrategias de privacidad empresarial** que garantizan protección de datos sensibles:\n",
    "\n",
    "- **No-Persistencia:** Archivos empresariales permanecen en ubicaciones originales\n",
    "- **Acceso Temporal:** Lectura durante ejecución sin copia permanente\n",
    "- **Compatibilidad Corporativa:** Integración con sistemas de almacenamiento empresarial\n",
    "- **Trazabilidad Selectiva:** Logging sin exposición de datos sensibles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfd47c7",
   "metadata": {},
   "source": [
    "## 4. Motor de Enriquecimiento Contextual: Generación de Metadatos Estructurados\n",
    "\n",
    "### Fundamentación Teórica: Ontologías Vehiculares y Clasificación Automática\n",
    "\n",
    "La generación de metadatos para sistemas RAG vehiculares trasciende la simple catalogación estadística, requiriendo la implementación de **ontologías de dominio específicas** que capturen relaciones semánticas complejas entre eventos, subsistemas y contextos operacionales. La metodología desarrollada se fundamenta en **sistemas de clasificación multi-criterio** que combinan análisis textual, estadístico y heurístico dominio-específico.\n",
    "\n",
    "### Arquitectura de Clasificación Inteligente\n",
    "\n",
    "**Sistema de Inferencia Contextual Multi-Dimensional:**\n",
    "\n",
    "La clasificación de eventos vehiculares implementa una **arquitectura de inferencia híbrida** que opera en múltiples dimensiones semánticas:\n",
    "\n",
    "1. **Dimensión Temporal:** Análisis de patrones temporales en señales CAN\n",
    "2. **Dimensión Semántica:** Procesamiento de contenido textual de descripciones\n",
    "3. **Dimensión Estadística:** Evaluación de intensidad y variabilidad de cambios\n",
    "4. **Dimensión Contextual:** Integración de conocimiento operacional vehicular\n",
    "\n",
    "### Objetivos Estratégicos del Sistema de Metadatos\n",
    "\n",
    "**Facilitación de Recuperación RAG Optimizada:**\n",
    "- **Filtrado Eficiente:** Indexación multidimensional para consultas especializadas\n",
    "- **Clasificación Automática:** Taxonomía adaptativa de eventos vehiculares\n",
    "- **Trazabilidad Temporal:** Preservación de marcas temporales para análisis causal\n",
    "- **Contextualización Operacional:** Enriquecimiento con estados del vehículo\n",
    "\n",
    "### Innovación Metodológica: Heurísticas Dominio-Específicas\n",
    "\n",
    "A diferencia de sistemas genéricos de clasificación, la implementación incorpora **heurísticas especializadas** derivadas del conocimiento experto en sistemas vehiculares eléctricos, incluyendo patrones específicos de comportamiento de cada subsistema CAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405a1101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Generador de metadatos inicializado\n"
     ]
    }
   ],
   "source": [
    "class GeneradorMetadatos:\n",
    "    \"\"\"\n",
    "    Motor de enriquecimiento contextual para generación de metadatos multidimensionales.\n",
    "    \n",
    "    Implementa sistema de clasificación híbrida que combina análisis textual,\n",
    "    estadístico y heurístico para inferencia automática de contexto operacional\n",
    "    y clasificación semántica de eventos vehiculares.\n",
    "    \n",
    "    Capacidades principales:\n",
    "    - Clasificación automática de eventos por patrones multi-criterio\n",
    "    - Inferencia de contexto operacional (ciudad, carretera, carga, mantenimiento)\n",
    "    - Determinación de intensidad y criticidad de eventos\n",
    "    - Generación de taxonomías adaptativas por red CAN\n",
    "    - Preservación de trazabilidad temporal para análisis causal\n",
    "    \n",
    "    Arquitectura de inferencia:\n",
    "    1. Análisis textual de descripciones con patrones dominio-específicos\n",
    "    2. Evaluación estadística de intensidad de cambios\n",
    "    3. Aplicación de heurísticas vehiculares especializadas\n",
    "    4. Síntesis contextual con validación de coherencia\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Inicializa motor con ontologías vehiculares y heurísticas especializadas.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Sistema de clasificación de eventos con criterios multi-dimensionales\n",
    "        # Cada evento incluye patrones textuales y umbrales estadísticos específicos\n",
    "        self.clasificador_eventos = {\n",
    "            'aceleracion_controlada': {\n",
    "                'patrones_textuales': [\n",
    "                    'incremento sostenido', 'crecimiento progresivo', 'aumento gradual',\n",
    "                    'Velocidad_Motor', 'Torque_Motor', 'RPM', 'tracción'\n",
    "                ],\n",
    "                'patrones_estadisticos': {\n",
    "                    'cambio_porcentual_min': 10.0,  # Mínimo 10% de cambio\n",
    "                    'intensidad_umbral': 0.15,      # Factor de intensidad\n",
    "                    'duracion_minima': 30,          # Segundos mínimos\n",
    "                    'variabilidad_maxima': 0.3      # Coeficiente de variación máximo\n",
    "                },\n",
    "                'contexto_esperado': ['ciudad', 'carretera'],\n",
    "                'criticidad': 'normal',\n",
    "                'subsistemas_involucrados': ['CAN_EV', 'CAN_CATL']\n",
    "            },\n",
    "            \n",
    "            'frenado_regenerativo': {\n",
    "                'patrones_textuales': [\n",
    "                    'decremento sostenido', 'reducción controlada', 'descenso gradual',\n",
    "                    'regenerativo', 'recuperación', 'energía', 'deceleración'\n",
    "                ],\n",
    "                'patrones_estadisticos': {\n",
    "                    'cambio_porcentual_min': -15.0,  # Decremento mínimo 15%\n",
    "                    'intensidad_umbral': 0.20,       # Mayor intensidad para frenado\n",
    "                    'duracion_minima': 10,           # Frenados más cortos\n",
    "                    'variabilidad_maxima': 0.4       # Mayor variabilidad permitida\n",
    "                },\n",
    "                'contexto_esperado': ['ciudad', 'carretera'],\n",
    "                'criticidad': 'normal',\n",
    "                'subsistemas_involucrados': ['CAN_EV', 'CAN_CATL']\n",
    "            },\n",
    "            \n",
    "            'proceso_carga': {\n",
    "                'patrones_textuales': [\n",
    "                    'SOC', 'carga', 'incremento', 'Corriente_Carga', 'Voltaje_Carga',\n",
    "                    'estacionado', 'batería', 'charging', 'alimentación'\n",
    "                ],\n",
    "                'patrones_estadisticos': {\n",
    "                    'cambio_porcentual_min': 5.0,    # Cambios graduales en carga\n",
    "                    'intensidad_umbral': 0.05,       # Baja intensidad, proceso controlado\n",
    "                    'duracion_minima': 300,          # Procesos de carga prolongados (5min+)\n",
    "                    'variabilidad_maxima': 0.2       # Alta estabilidad esperada\n",
    "                },\n",
    "                'contexto_esperado': ['estacionado'],\n",
    "                'criticidad': 'normal',\n",
    "                'subsistemas_involucrados': ['CAN_CATL', 'AUX_CHG']\n",
    "            },\n",
    "            \n",
    "            'operacion_idle': {\n",
    "                'patrones_textuales': [\n",
    "                    'estabilidad', 'estable', 'constante', 'nominal', 'mínima',\n",
    "                    'oscilaciones contenidas', 'funcionamiento nominal'\n",
    "                ],\n",
    "                'patrones_estadisticos': {\n",
    "                    'cambio_porcentual_max': 5.0,    # Cambios mínimos\n",
    "                    'intensidad_umbral': 0.02,       # Muy baja intensidad\n",
    "                    'duracion_minima': 60,           # Estados idle sostenidos\n",
    "                    'variabilidad_maxima': 0.1       # Máxima estabilidad\n",
    "                },\n",
    "                'contexto_esperado': ['estacionado', 'ciudad'],\n",
    "                'criticidad': 'baja',\n",
    "                'subsistemas_involucrados': ['CAN_EV', 'CAN_CARROC']\n",
    "            },\n",
    "            \n",
    "            'evento_anomalo': {\n",
    "                'patrones_textuales': [\n",
    "                    'picos anómalos', 'eventos excepcionales', 'anomalías',\n",
    "                    'fuera de comportamiento normal', 'desviaciones', 'crítico'\n",
    "                ],\n",
    "                'patrones_estadisticos': {\n",
    "                    'intensidad_umbral': 0.30,       # Alta intensidad para anomalías\n",
    "                    'num_picos_min': 3,              # Múltiples eventos anómalos\n",
    "                    'desviacion_factor': 2.5,        # >2.5σ del comportamiento normal\n",
    "                    'variabilidad_minima': 0.5       # Alta variabilidad indica anomalía\n",
    "                },\n",
    "                'contexto_esperado': ['mantenimiento', 'diagnostico'],\n",
    "                'criticidad': 'alta',\n",
    "                'subsistemas_involucrados': ['CAN_EV', 'CAN_CATL', 'CAN_CARROC', 'AUX_CHG']\n",
    "            },\n",
    "            \n",
    "            'patron_ciclico_normal': {\n",
    "                'patrones_textuales': [\n",
    "                    'patrón cíclico', 'comportamiento periódico', 'oscilaciones regulares',\n",
    "                    'frecuencia', 'periodicidad', 'ciclos operacionales'\n",
    "                ],\n",
    "                'patrones_estadisticos': {\n",
    "                    'regularidad_min': 60.0,         # Mínimo 60% de regularidad\n",
    "                    'autocorrelacion_min': 0.6,      # Correlación fuerte\n",
    "                    'duracion_minima': 120,          # Ciclos sostenidos (2min+)\n",
    "                    'amplitud_consistente': True     # Amplitud debe ser consistente\n",
    "                },\n",
    "                'contexto_esperado': ['carretera', 'ciudad'],\n",
    "                'criticidad': 'normal',\n",
    "                'subsistemas_involucrados': ['CAN_EV']\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Contextos operacionales con características específicas del dominio vehicular\n",
    "        self.contextos_operativos = {\n",
    "            'ciudad': {\n",
    "                'caracteristicas': ['paradas_frecuentes', 'aceleracion_moderada', 'velocidad_variable'],\n",
    "                'velocidad_tipica': (0, 50),        # km/h\n",
    "                'duracion_eventos': (10, 120),      # segundos\n",
    "                'subsistemas_activos': ['CAN_EV', 'CAN_CARROC', 'CAN_CATL'],\n",
    "                'criticidad_base': 'media'\n",
    "            },\n",
    "            \n",
    "            'carretera': {\n",
    "                'caracteristicas': ['alta_velocidad', 'velocidad_constante', 'eficiencia_maxima'],\n",
    "                'velocidad_tipica': (50, 120),      # km/h\n",
    "                'duracion_eventos': (60, 600),      # eventos más prolongados\n",
    "                'subsistemas_activos': ['CAN_EV', 'CAN_CATL'],\n",
    "                'criticidad_base': 'alta'           # Mayor criticidad por velocidades altas\n",
    "            },\n",
    "            \n",
    "            'estacionado': {\n",
    "                'caracteristicas': ['idle', 'carga', 'sistemas_auxiliares', 'confort'],\n",
    "                'velocidad_tipica': (0, 0),         # Vehículo detenido\n",
    "                'duracion_eventos': (300, 3600),    # Eventos prolongados (5min-1h)\n",
    "                'subsistemas_activos': ['AUX_CHG', 'CAN_CARROC', 'CAN_CATL'],\n",
    "                'criticidad_base': 'baja'\n",
    "            },\n",
    "            \n",
    "            'mantenimiento': {\n",
    "                'caracteristicas': ['diagnostico', 'pruebas_sistemas', 'calibracion', 'test_bench'],\n",
    "                'velocidad_tipica': (0, 30),        # Velocidades de prueba\n",
    "                'duracion_eventos': (60, 1800),     # Pruebas de duración variable\n",
    "                'subsistemas_activos': ['CAN_EV', 'CAN_CATL', 'CAN_CARROC', 'AUX_CHG'],\n",
    "                'criticidad_base': 'diagnóstica'    # Criticidad especial para diagnóstico\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Mapeador de redes CAN a categorías funcionales vehiculares\n",
    "        self.categorias_funcionales = {\n",
    "            'CAN_EV': {\n",
    "                'funcion_primaria': 'propulsion_electrica',\n",
    "                'subsistemas': ['motor_traccion', 'inversor_potencia', 'control_vectorial'],\n",
    "                'criticidad_operacional': 'critica',\n",
    "                'impacto_movilidad': 'directo'\n",
    "            },\n",
    "            'CAN_CATL': {\n",
    "                'funcion_primaria': 'almacenamiento_energia',\n",
    "                'subsistemas': ['gestion_bateria', 'balanceado_celdas', 'control_termico'],\n",
    "                'criticidad_operacional': 'critica',\n",
    "                'impacto_movilidad': 'directo'\n",
    "            },\n",
    "            'CAN_CARROC': {\n",
    "                'funcion_primaria': 'confort_accesibilidad',\n",
    "                'subsistemas': ['control_puertas', 'climatizacion', 'iluminacion'],\n",
    "                'criticidad_operacional': 'baja',\n",
    "                'impacto_movilidad': 'indirecto'\n",
    "            },\n",
    "            'AUX_CHG': {\n",
    "                'funcion_primaria': 'gestion_energetica',\n",
    "                'subsistemas': ['carga_ac_dc', 'conversion_potencia', 'sistemas_auxiliares'],\n",
    "                'criticidad_operacional': 'media',\n",
    "                'impacto_movilidad': 'indirecto'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Configuración de métricas de calidad para validación de metadatos\n",
    "        self.metricas_calidad_metadatos = {\n",
    "            'completitud_campos': 0.0,      # Porcentaje de campos requeridos completados\n",
    "            'coherencia_contextual': 0.0,   # Coherencia entre evento y contexto\n",
    "            'precision_clasificacion': 0.0, # Confianza en clasificación automática\n",
    "            'trazabilidad_temporal': 0.0    # Calidad de información temporal\n",
    "        }\n",
    "        \n",
    "        logger.info(\"🏗️ GeneradorMetadatos inicializado con ontologías vehiculares\")\n",
    "        logger.info(f\"   📊 {len(self.clasificador_eventos)} tipos de eventos configurados\")\n",
    "        logger.info(f\"   🌍 {len(self.contextos_operativos)} contextos operacionales definidos\")\n",
    "        logger.info(f\"   🔧 {len(self.categorias_funcionales)} categorías funcionales de redes CAN\")\n",
    "    \n",
    "    def clasificar_evento_inteligente(self, descripcion_textual: str, \n",
    "                                    analisis_estadistico: Dict[str, Any], \n",
    "                                    red_can: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Clasifica eventos usando análisis multi-criterio híbrido.\n",
    "        \n",
    "        Implementa lógica de inferencia que combina:\n",
    "        - Análisis de patrones textuales en descripciones\n",
    "        - Evaluación de métricas estadísticas temporales\n",
    "        - Aplicación de heurísticas dominio-específicas\n",
    "        - Validación de coherencia contextual\n",
    "        \n",
    "        Args:\n",
    "            descripcion_textual: Descripción generada del evento\n",
    "            analisis_estadistico: Métricas estadísticas del análisis temporal\n",
    "            red_can: Red CAN de origen del evento\n",
    "            \n",
    "        Returns:\n",
    "            Diccionario con clasificación completa y metadatos de confianza\n",
    "        \"\"\"\n",
    "        descripcion_lower = descripcion_textual.lower()\n",
    "        puntuaciones_eventos = {}\n",
    "        \n",
    "        # Evaluación sistemática de cada tipo de evento\n",
    "        for tipo_evento, criterios in self.clasificador_eventos.items():\n",
    "            puntuacion_total = 0.0\n",
    "            detalles_puntuacion = {}\n",
    "            \n",
    "            # 1. Análisis de patrones textuales (peso: 40%)\n",
    "            puntuacion_textual = self._evaluar_patrones_textuales(\n",
    "                descripcion_lower, criterios['patrones_textuales']\n",
    "            )\n",
    "            puntuacion_total += puntuacion_textual * 0.4\n",
    "            detalles_puntuacion['textual'] = puntuacion_textual\n",
    "            \n",
    "            # 2. Evaluación estadística (peso: 35%)\n",
    "            puntuacion_estadistica = self._evaluar_criterios_estadisticos(\n",
    "                analisis_estadistico, criterios['patrones_estadisticos']\n",
    "            )\n",
    "            puntuacion_total += puntuacion_estadistica * 0.35\n",
    "            detalles_puntuacion['estadistica'] = puntuacion_estadistica\n",
    "            \n",
    "            # 3. Coherencia con red CAN (peso: 15%)\n",
    "            puntuacion_red = self._evaluar_coherencia_red_can(\n",
    "                red_can, criterios['subsistemas_involucrados']\n",
    "            )\n",
    "            puntuacion_total += puntuacion_red * 0.15\n",
    "            detalles_puntuacion['red_can'] = puntuacion_red\n",
    "            \n",
    "            # 4. Contexto operacional (peso: 10%)\n",
    "            puntuacion_contexto = self._evaluar_contexto_operacional(\n",
    "                analisis_estadistico, criterios.get('contexto_esperado', [])\n",
    "            )\n",
    "            puntuacion_total += puntuacion_contexto * 0.10\n",
    "            detalles_puntuacion['contexto'] = puntuacion_contexto\n",
    "            \n",
    "            # Almacenar puntuación completa con detalles\n",
    "            puntuaciones_eventos[tipo_evento] = {\n",
    "                'puntuacion_total': puntuacion_total,\n",
    "                'detalles': detalles_puntuacion,\n",
    "                'criterios_cumplidos': puntuacion_total > 0.5,  # Umbral de clasificación\n",
    "                'confianza': min(1.0, puntuacion_total)\n",
    "            }\n",
    "        \n",
    "        # Selección del evento con mayor puntuación\n",
    "        if puntuaciones_eventos:\n",
    "            evento_seleccionado = max(puntuaciones_eventos, key=lambda x: puntuaciones_eventos[x]['puntuacion_total'])\n",
    "            confianza_clasificacion = puntuaciones_eventos[evento_seleccionado]['confianza']\n",
    "            \n",
    "            # Validación de umbral mínimo de confianza\n",
    "            if confianza_clasificacion < 0.3:\n",
    "                evento_seleccionado = 'operacion_indeterminada'\n",
    "                confianza_clasificacion = 0.3\n",
    "        else:\n",
    "            evento_seleccionado = 'operacion_normal'\n",
    "            confianza_clasificacion = 0.5\n",
    "        \n",
    "        # Inferencia de contexto operacional basada en evento clasificado\n",
    "        contexto_operacional = self._inferir_contexto_operacional(\n",
    "            evento_seleccionado, analisis_estadistico, red_can\n",
    "        )\n",
    "        \n",
    "        # Determinación de criticidad e intensidad\n",
    "        criticidad = self._determinar_criticidad_evento(evento_seleccionado, red_can, analisis_estadistico)\n",
    "        intensidad = self._calcular_intensidad_evento(analisis_estadistico)\n",
    "        \n",
    "        # Compilación de resultado completo\n",
    "        resultado_clasificacion = {\n",
    "            'evento_vehiculo': evento_seleccionado,\n",
    "            'confianza_clasificacion': confianza_clasificacion,\n",
    "            'contexto_operativo': contexto_operacional,\n",
    "            'intensidad': intensidad,\n",
    "            'criticidad': criticidad,\n",
    "            'red_can_origen': red_can,\n",
    "            'puntuaciones_detalladas': puntuaciones_eventos,\n",
    "            'timestamp_clasificacion': datetime.now().isoformat(),\n",
    "            'version_clasificador': '1.0'\n",
    "        }\n",
    "        \n",
    "        logger.debug(f\"Evento clasificado: {evento_seleccionado} (confianza: {confianza_clasificacion:.3f})\")\n",
    "        return resultado_clasificacion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d661491",
   "metadata": {},
   "source": [
    "## 5. PASO 3: Procesamiento de Documentación Técnica y Chunking\n",
    "\n",
    "### Objetivo\n",
    "Preparar la documentación técnica (norma J1939, manuales, especificaciones) mediante estrategias de segmentación optimizadas para sistemas RAG.\n",
    "\n",
    "### Estrategias de Chunking Implementadas:\n",
    "1. **Chunking Semántico:** Por secciones técnicas coherentes\n",
    "2. **Chunking por Tamaño:** Fragmentos de tamaño fijo con solapamiento\n",
    "3. **Chunking Jerárquico:** Preservando estructura de documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab105ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Procesador de documentación técnica inicializado\n"
     ]
    }
   ],
   "source": [
    "class ProcesadorDocumentacionTecnica:\n",
    "    \"\"\"\n",
    "    Procesador de documentación técnica para sistemas RAG\n",
    "    Implementa múltiples estrategias de chunking\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.patrones_seccion = {\n",
    "            'j1939': [\n",
    "                r'^\\d+\\.\\d+\\s+.*',  # Numeración tipo \"3.1 Título\"\n",
    "                r'^[A-Z]+\\s+[A-Z].*',  # Secciones en mayúsculas\n",
    "                r'^\\w+\\s+Group.*',  # Grupos de parámetros\n",
    "            ],\n",
    "            'manual_tecnico': [\n",
    "                r'^Chapter\\s+\\d+.*',\n",
    "                r'^Section\\s+\\d+.*',\n",
    "                r'^\\d+\\.\\s+.*',\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        self.configuraciones_chunking = {\n",
    "            'semantico': {\n",
    "                'chunk_size': 1000,\n",
    "                'chunk_overlap': 200,\n",
    "                'separators': ['\\n\\n', '\\n', '. ', ' ']\n",
    "            },\n",
    "            'fijo': {\n",
    "                'chunk_size': 512,\n",
    "                'chunk_overlap': 50,\n",
    "                'separators': ['\\n\\n', '\\n']\n",
    "            },\n",
    "            'jerarquico': {\n",
    "                'chunk_size': 800,\n",
    "                'chunk_overlap': 150,\n",
    "                'preserve_structure': True\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def detectar_tipo_documento(self, texto: str) -> str:\n",
    "        \"\"\"\n",
    "        Detecta el tipo de documento técnico basado en patrones\n",
    "        \"\"\"\n",
    "        texto_muestra = texto[:2000].lower()\n",
    "        \n",
    "        if any(keyword in texto_muestra for keyword in ['j1939', 'pgn', 'spn', 'parameter group']):\n",
    "            return 'j1939'\n",
    "        elif any(keyword in texto_muestra for keyword in ['manual', 'specification', 'datasheet']):\n",
    "            return 'manual_tecnico'\n",
    "        elif any(keyword in texto_muestra for keyword in ['api', 'protocol', 'interface']):\n",
    "            return 'documentacion_api'\n",
    "        else:\n",
    "            return 'documento_generico'\n",
    "    \n",
    "    def extraer_metadatos_documento(self, texto: str, tipo_doc: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Extrae metadatos relevantes del documento\n",
    "        \"\"\"\n",
    "        metadatos = {\n",
    "            'tipo_documento': tipo_doc,\n",
    "            'longitud_caracteres': len(texto),\n",
    "            'numero_lineas': texto.count('\\n'),\n",
    "            'idioma': 'es' if any(palabra in texto.lower() for palabra in ['el', 'la', 'de', 'en', 'que']) else 'en'\n",
    "        }\n",
    "        \n",
    "        # Extraer títulos y secciones principales\n",
    "        lineas = texto.split('\\n')\n",
    "        titulos = []\n",
    "        \n",
    "        for patron in self.patrones_seccion.get(tipo_doc, []):\n",
    "            for linea in lineas:\n",
    "                if re.match(patron, linea.strip()):\n",
    "                    titulos.append(linea.strip())\n",
    "        \n",
    "        metadatos['titulos_principales'] = titulos[:10]  # Primeros 10 títulos\n",
    "        metadatos['estructura_detectada'] = len(titulos) > 0\n",
    "        \n",
    "        return metadatos\n",
    "    \n",
    "    def chunking_semantico(self, texto: str) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Chunking basado en estructura semántica del documento\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Usar LangChain si está disponible\n",
    "            text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=self.configuraciones_chunking['semantico']['chunk_size'],\n",
    "                chunk_overlap=self.configuraciones_chunking['semantico']['chunk_overlap'],\n",
    "                separators=self.configuraciones_chunking['semantico']['separators']\n",
    "            )\n",
    "            \n",
    "            chunks = text_splitter.split_text(texto)\n",
    "            \n",
    "            return [Document(page_content=chunk, metadata={'chunk_id': i, 'tipo_chunking': 'semantico'})\n",
    "                    for i, chunk in enumerate(chunks)]\n",
    "        \n",
    "        except ImportError:\n",
    "            # Fallback manual si LangChain no está disponible\n",
    "            return self.chunking_manual(texto, 'semantico')\n",
    "    \n",
    "    def chunking_manual(self, texto: str, tipo: str) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Implementación manual de chunking como fallback\n",
    "        \"\"\"\n",
    "        config = self.configuraciones_chunking[tipo]\n",
    "        chunk_size = config['chunk_size']\n",
    "        overlap = config['chunk_overlap']\n",
    "        \n",
    "        chunks = []\n",
    "        texto_chars = list(texto)\n",
    "        \n",
    "        for i in range(0, len(texto_chars), chunk_size - overlap):\n",
    "            chunk_text = ''.join(texto_chars[i:i + chunk_size])\n",
    "            \n",
    "            if len(chunk_text.strip()) > 50:  # Evitar chunks muy pequeños\n",
    "                chunks.append(Document(\n",
    "                    page_content=chunk_text,\n",
    "                    metadata={'chunk_id': i // (chunk_size - overlap), 'tipo_chunking': tipo}\n",
    "                ))\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def procesar_documento_completo(self, ruta_archivo: str,\n",
    "                                   estrategia_chunking: str = 'semantico') -> List[RAGDocument]:\n",
    "        \"\"\"\n",
    "        Procesa documento completo y genera chunks RAG\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Leer archivo (simulado para demostración)\n",
    "            if Path(ruta_archivo).exists():\n",
    "                with open(ruta_archivo, 'r', encoding='utf-8') as f:\n",
    "                    texto = f.read()\n",
    "            else:\n",
    "                # Documento simulado para demostración\n",
    "                texto = self.generar_documento_j1939_simulado()\n",
    "                print(f\"ℹ️ Archivo {ruta_archivo} no encontrado, usando documento simulado\")\n",
    "            \n",
    "            # Detectar tipo de documento\n",
    "            tipo_doc = self.detectar_tipo_documento(texto)\n",
    "            metadatos_doc = self.extraer_metadatos_documento(texto, tipo_doc)\n",
    "            \n",
    "            # Aplicar chunking\n",
    "            if estrategia_chunking == 'semantico':\n",
    "                chunks = self.chunking_semantico(texto)\n",
    "            else:\n",
    "                chunks = self.chunking_manual(texto, estrategia_chunking)\n",
    "            \n",
    "            # Convertir a RAGDocument\n",
    "            documentos_rag = []\n",
    "            \n",
    "            for i, chunk in enumerate(chunks):\n",
    "                # Crear metadatos ficticios para documentación técnica\n",
    "                metadatos_evento = CANEventMetadata(\n",
    "                    timestamp_inicio=datetime.now().isoformat(),\n",
    "                    timestamp_fin=datetime.now().isoformat(),\n",
    "                    duracion_segundos=0.0,\n",
    "                    red_can='DOCUMENTACION',\n",
    "                    senales_involucradas=[],\n",
    "                    evento_vehiculo='referencia_tecnica',\n",
    "                    intensidad='informativo',\n",
    "                    contexto_operativo='documentacion'\n",
    "                )\n",
    "                \n",
    "                doc_rag = RAGDocument(\n",
    "                    id=f\"{tipo_doc}_chunk_{i}\",\n",
    "                    contenido_textual=chunk.page_content,\n",
    "                    metadatos=metadatos_evento,\n",
    "                    tipo_documento='documentacion_tecnica',\n",
    "                    calidad_descripcion=0.9  # Alta calidad para documentación oficial\n",
    "                )\n",
    "                \n",
    "                documentos_rag.append(doc_rag)\n",
    "            \n",
    "            print(f\"✅ Procesado {ruta_archivo}: {len(documentos_rag)} chunks generados\")\n",
    "            return documentos_rag\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error procesando {ruta_archivo}: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    def generar_documento_j1939_simulado(self) -> str:\n",
    "        \"\"\"\n",
    "        Genera documento J1939 simulado para demostración\n",
    "        \"\"\"\n",
    "        return \"\"\"\n",
    "# J1939 - Parameter Group Number (PGN) Reference\n",
    "\n",
    "## 1.1 Engine Parameters Group\n",
    "\n",
    "The Engine Parameters Group contains critical information about engine operation:\n",
    "\n",
    "- **Engine Speed (SPN 190)**: Motor RPM measurement\n",
    "- **Engine Load (SPN 92)**: Current engine load percentage\n",
    "- **Engine Temperature (SPN 110)**: Coolant temperature in Celsius\n",
    "\n",
    "### 1.1.1 Engine Speed Signal\n",
    "\n",
    "Engine speed is transmitted via PGN 61444 (0xF004) with the following characteristics:\n",
    "- Data Length: 8 bytes\n",
    "- Transmission Rate: 10ms\n",
    "- Resolution: 0.125 rpm/bit\n",
    "- Range: 0 to 8031.875 rpm\n",
    "\n",
    "### 1.1.2 Engine Load Signal\n",
    "\n",
    "Engine load percentage indicates current operational demand:\n",
    "- Signal Range: 0-100%\n",
    "- Resolution: 1%/bit\n",
    "- Typical idle load: 5-10%\n",
    "- Maximum load scenarios: >90%\n",
    "\n",
    "## 2.1 Battery Management System\n",
    "\n",
    "Battery parameters are critical for electric vehicle operation:\n",
    "\n",
    "- **State of Charge (SOC)**: Battery charge level percentage\n",
    "- **Battery Voltage**: Total pack voltage\n",
    "- **Battery Temperature**: Average cell temperature\n",
    "- **Charging Status**: Current charging state\n",
    "\n",
    "### 2.1.1 State of Charge Calculation\n",
    "\n",
    "SOC calculation involves multiple factors:\n",
    "- Coulomb counting method\n",
    "- Voltage-based estimation\n",
    "- Temperature compensation\n",
    "- Aging factor adjustment\n",
    "\"\"\"\n",
    "\n",
    "# Inicializar procesador de documentación\n",
    "procesador_docs = ProcesadorDocumentacionTecnica()\n",
    "print(\"✅ Procesador de documentación técnica inicializado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d995a824",
   "metadata": {},
   "source": [
    "## 6. PASO 4: Construcción del Dataset RAG Unificado\n",
    "\n",
    "### Objetivo Final\n",
    "Combinar todas las características generadas en un dataset unificado formato JSONL optimizado para sistemas RAG:\n",
    "- **Descripciones textuales** de eventos CAN\n",
    "- **Metadatos estructurados** para filtrado\n",
    "- **Chunks documentales** de referencias técnicas\n",
    "- **Esquema unificado** para recuperación eficiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5d02b0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Iniciando construcción del dataset RAG...\n",
      "📋 Iniciando construcción del dataset RAG completo...\n",
      "🔄 Procesando CAN_CUSTOM_31...\n",
      "📚 Procesando documentación técnica...\n",
      "ℹ️ Archivo documentacion/J1939_reference.txt no encontrado, usando documento simulado\n",
      "✅ Procesado documentacion/J1939_reference.txt: 2 chunks generados\n",
      "✅ Dataset RAG guardado en: ..\\Ingenieria_de_Caracteristicas\\dataset_rag_decode_ev.jsonl\n",
      "📊 Estadísticas finales: {'total_documentos': 7, 'eventos_can': 5, 'documentacion_tecnica': 2, 'hipotesis_catl': 0, 'calidad_promedio': np.float64(0.7621446827728902)}\n",
      "\n",
      "📋 MUESTRA DEL DATASET GENERADO:\n",
      "======================================================================\n",
      "MUESTRA_1:\n",
      "  id: CAN_CUSTOM_31_evento_0\n",
      "  tipo: evento_can\n",
      "  contenido_preview: Evento en red CAN_CUSTOM_31 (Segmento 0):\n",
      "- En el sistema CAN, análisis de estabilidad temporal: voltaje_carga_v mostró variación contenida de ±36.00 v respecto al valor nominal registrado en blf.\n",
      "- E...\n",
      "  calidad: 0.6462042202279893\n",
      "  evento_vehicular: carga\n",
      "  red_can: CAN_CUSTOM_31\n",
      "--------------------------------------------------\n",
      "MUESTRA_2:\n",
      "  id: CAN_CUSTOM_31_evento_1\n",
      "  tipo: evento_can\n",
      "  contenido_preview: Evento en red CAN_CUSTOM_31 (Segmento 1):\n",
      "- En el sistema CAN, comportamiento periódico detectado en logs blf: voltaje_carga_v exhibe oscilaciones regulares cada 0.1 minutos durante operación normal.\n",
      "...\n",
      "  calidad: 0.7169641095918855\n",
      "  evento_vehicular: carga\n",
      "  red_can: CAN_CUSTOM_31\n",
      "--------------------------------------------------\n",
      "MUESTRA_3:\n",
      "  id: CAN_CUSTOM_31_evento_2\n",
      "  tipo: evento_can\n",
      "  contenido_preview: Evento en red CAN_CUSTOM_31 (Segmento 2):\n",
      "- En el sistema CAN, los datos temporales revelan un patrón cíclico en voltaje_carga_v con período aproximado de 0.1 minutos y amplitud de 83.69 v.\n",
      "- En el si...\n",
      "  calidad: 0.7336775040420048\n",
      "  evento_vehicular: carga\n",
      "  red_can: CAN_CUSTOM_31\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class ConstructorDatasetRAG:\n",
    "    \"\"\"\n",
    "    Constructor del dataset RAG unificado para DECODE-EV\n",
    "    Integra descripciones textuales, metadatos y documentación técnica\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, ruta_salida: str = \"../Ingenieria_de_Caracteristicas/\"):\n",
    "        self.ruta_salida = Path(ruta_salida)\n",
    "        self.ruta_salida.mkdir(exist_ok=True)\n",
    "        self.documentos_rag = []\n",
    "        \n",
    "        # Estadísticas del dataset\n",
    "        self.stats = {\n",
    "            'total_documentos': 0,\n",
    "            'eventos_can': 0,\n",
    "            'documentacion_tecnica': 0,\n",
    "            'hipotesis_catl': 0,\n",
    "            'calidad_promedio': 0.0\n",
    "        }\n",
    "    \n",
    "    def generar_evento_can_completo(self, df_segmento: pd.DataFrame,\n",
    "                                   red_can: str, indice_segmento: int) -> RAGDocument:\n",
    "        \"\"\"\n",
    "        Genera un documento RAG completo para un segmento de datos CAN\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # 1. Generar descripción textual\n",
    "            timestamp_inicio = df_segmento['timestamp'].iloc[0] if 'timestamp' in df_segmento.columns else datetime.now()\n",
    "            duracion = len(df_segmento)  # Aproximación en segundos\n",
    "            \n",
    "            # Generar descripción para cada señal numérica\n",
    "            descripciones_senales = []\n",
    "            senales_numericas = df_segmento.select_dtypes(include=[np.number]).columns.tolist()\n",
    "            \n",
    "            # Remover timestamp si existe\n",
    "            if 'timestamp' in senales_numericas:\n",
    "                senales_numericas.remove('timestamp')\n",
    "            \n",
    "            for senal in senales_numericas[:5]:  # Limitar a 5 señales principales\n",
    "                try:\n",
    "                    serie = df_segmento[senal].dropna()\n",
    "                    if len(serie) > 2:\n",
    "                        desc = generador_textual.generar_descripcion_signal(\n",
    "                            senal, serie, df_segmento['timestamp'] if 'timestamp' in df_segmento.columns else pd.Series(range(len(serie))), red_can\n",
    "                        )\n",
    "                        descripciones_senales.append(desc)\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️ Error procesando señal {senal}: {e}\")\n",
    "            \n",
    "            # Combinar descripciones\n",
    "            descripcion_completa = f\"Evento en red {red_can} (Segmento {indice_segmento}):\\n\"\n",
    "            descripcion_completa += \"\\n\".join([f\"- {desc}\" for desc in descripciones_senales])\n",
    "            \n",
    "            # 2. Calcular estadísticas numéricas para metadatos\n",
    "            stats_numericas = {\n",
    "                'cambio_relativo_promedio': np.mean([\n",
    "                    abs(df_segmento[col].iloc[-1] - df_segmento[col].iloc[0]) /\n",
    "                    (df_segmento[col].mean() + 1e-6)  # Evitar división por cero\n",
    "                    for col in senales_numericas if len(df_segmento[col].dropna()) > 1\n",
    "                ]) if senales_numericas else 0.0,\n",
    "                'velocidad_promedio': df_segmento.get('Velocidad_Motor_RPM', pd.Series([0])).mean()\n",
    "            }\n",
    "            \n",
    "            # 3. Generar metadatos estructurados\n",
    "            metadatos = generador_metadatos.generar_metadatos_evento(\n",
    "                descripcion_textual=descripcion_completa,\n",
    "                timestamp_inicio=timestamp_inicio if isinstance(timestamp_inicio, datetime) else datetime.now(),\n",
    "                duracion=float(duracion),\n",
    "                red_can=red_can,\n",
    "                senales_involucradas=senales_numericas,\n",
    "                stats_numericas=stats_numericas\n",
    "            )\n",
    "            \n",
    "            # 4. Calcular calidad de descripción\n",
    "            calidad = generador_metadatos.calcular_calidad_descripcion(\n",
    "                descripcion_completa, len(senales_numericas)\n",
    "            )\n",
    "            \n",
    "            # 5. Crear documento RAG\n",
    "            doc_rag = RAGDocument(\n",
    "                id=f\"{red_can}_evento_{indice_segmento}\",\n",
    "                contenido_textual=descripcion_completa,\n",
    "                metadatos=metadatos,\n",
    "                tipo_documento=\"evento_can\",\n",
    "                calidad_descripcion=calidad\n",
    "            )\n",
    "            \n",
    "            return doc_rag\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error generando evento CAN: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def generar_hipotesis_catl(self, df_catl: pd.DataFrame) -> List[RAGDocument]:\n",
    "        \"\"\"\n",
    "        Genera hipótesis textuales para la red CAN_CATL (caja negra)\n",
    "        Basado en análisis de patrones estadísticos\n",
    "        \"\"\"\n",
    "        hipotesis_docs = []\n",
    "        \n",
    "        try:\n",
    "            # Análisis estadístico de señales desconocidas\n",
    "            senales_catl = [col for col in df_catl.columns if col.startswith('Signal_')]\n",
    "            \n",
    "            for i, senal in enumerate(senales_catl[:10]):  # Primeras 10 señales\n",
    "                serie = df_catl[senal].dropna()\n",
    "                \n",
    "                if len(serie) < 10:\n",
    "                    continue\n",
    "                \n",
    "                # Análisis estadístico\n",
    "                stats = {\n",
    "                    'min': serie.min(),\n",
    "                    'max': serie.max(),\n",
    "                    'mean': serie.mean(),\n",
    "                    'std': serie.std(),\n",
    "                    'rango': serie.max() - serie.min()\n",
    "                }\n",
    "                \n",
    "                # Generar hipótesis basada en patrones\n",
    "                hipotesis = self.generar_hipotesis_senal_catl(senal, stats)\n",
    "                \n",
    "                # Crear metadatos para hipótesis\n",
    "                metadatos_hipotesis = CANEventMetadata(\n",
    "                    timestamp_inicio=datetime.now().isoformat(),\n",
    "                    timestamp_fin=datetime.now().isoformat(),\n",
    "                    duracion_segundos=0.0,\n",
    "                    red_can=\"CAN_CATL\",\n",
    "                    senales_involucradas=[senal],\n",
    "                    evento_vehiculo=\"hipotesis_funcional\",\n",
    "                    intensidad=\"informativo\",\n",
    "                    contexto_operativo=\"analisis_exploratorio\"\n",
    "                )\n",
    "                \n",
    "                doc_hipotesis = RAGDocument(\n",
    "                    id=f\"CATL_hipotesis_{i}\",\n",
    "                    contenido_textual=hipotesis,\n",
    "                    metadatos=metadatos_hipotesis,\n",
    "                    tipo_documento=\"hipotesis_catl\",\n",
    "                    calidad_descripcion=0.6  # Calidad media para hipótesis\n",
    "                )\n",
    "                \n",
    "                hipotesis_docs.append(doc_hipotesis)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error generando hipótesis CATL: {str(e)}\")\n",
    "        \n",
    "        return hipotesis_docs\n",
    "    \n",
    "    def generar_hipotesis_senal_catl(self, nombre_senal: str, stats: Dict) -> str:\n",
    "        \"\"\"\n",
    "        Genera hipótesis textual para una señal CATL desconocida\n",
    "        \"\"\"\n",
    "        # Patrones de reconocimiento basados en rangos estadísticos\n",
    "        if 0 <= stats['mean'] <= 100 and stats['rango'] > 50:\n",
    "            tipo_hipotesis = \"porcentaje (posible SOC o nivel de carga)\"\n",
    "            comportamiento = f\"varía entre {stats['min']:.1f}% y {stats['max']:.1f}%\"\n",
    "        elif 20 <= stats['mean'] <= 60 and stats['std'] < 10:\n",
    "            tipo_hipotesis = \"temperatura (posible temperatura de celda)\"\n",
    "            comportamiento = f\"se mantiene relativamente estable entre {stats['min']:.1f}°C y {stats['max']:.1f}°C\"\n",
    "        elif 3.0 <= stats['mean'] <= 4.5 and stats['std'] < 0.5:\n",
    "            tipo_hipotesis = \"voltaje (posible voltaje de celda)\"\n",
    "            comportamiento = f\"presenta valores típicos de batería Li-ion entre {stats['min']:.2f}V y {stats['max']:.2f}V\"\n",
    "        elif stats['rango'] < stats['mean'] * 0.1:\n",
    "            tipo_hipotesis = \"valor de estado o configuración\"\n",
    "            comportamiento = f\"permanece constante en {stats['mean']:.2f} con mínimas variaciones\"\n",
    "        else:\n",
    "            tipo_hipotesis = \"parámetro operativo no identificado\"\n",
    "            comportamiento = f\"muestra variabilidad moderada con promedio de {stats['mean']:.2f}\"\n",
    "        \n",
    "        hipotesis = f\"\"\"\n",
    "HIPÓTESIS PARA {nombre_senal} (Red CAN_CATL):\n",
    "\n",
    "Basado en el análisis estadístico de patrones, esta señal probablemente representa un {tipo_hipotesis}.\n",
    "\n",
    "Comportamiento observado: {comportamiento}.\n",
    "\n",
    "Estadísticas clave:\n",
    "- Valor promedio: {stats['mean']:.3f}\n",
    "- Desviación estándar: {stats['std']:.3f}\n",
    "- Rango total: {stats['rango']:.3f}\n",
    "\n",
    "Esta hipótesis requiere validación con documentación técnica o conocimiento experto del sistema CATL.\n",
    "\"\"\"\n",
    "        \n",
    "        return hipotesis\n",
    "    \n",
    "    def construir_dataset_completo(self) -> str:\n",
    "        \"\"\"\n",
    "        Construye el dataset RAG completo integrando todas las fuentes\n",
    "        \"\"\"\n",
    "        print(\"📋 Iniciando construcción del dataset RAG completo...\")\n",
    "        \n",
    "        # 1. Procesar eventos CAN de todas las redes\n",
    "        for nombre_red, df_red in datos_can.items():\n",
    "            if df_red.empty:\n",
    "                continue\n",
    "            \n",
    "            print(f\"🔄 Procesando {nombre_red}...\")\n",
    "            \n",
    "            # Segmentar datos en ventanas de tiempo\n",
    "            ventana = 30  # 30 registros por segmento\n",
    "            n_segmentos = len(df_red) // ventana\n",
    "            \n",
    "            for i in range(min(n_segmentos, 5)):  # Limitar a 5 segmentos por red para demo\n",
    "                segmento = df_red.iloc[i*ventana:(i+1)*ventana]\n",
    "                \n",
    "                doc_evento = self.generar_evento_can_completo(segmento, nombre_red, i)\n",
    "                if doc_evento:\n",
    "                    self.documentos_rag.append(doc_evento)\n",
    "                    self.stats['eventos_can'] += 1\n",
    "        \n",
    "        # 2. Generar hipótesis para CAN_CATL\n",
    "        if \"CAN_CATL\" in datos_can and not datos_can[\"CAN_CATL\"].empty:\n",
    "            print(\"🔍 Generando hipótesis para CAN_CATL...\")\n",
    "            hipotesis_catl = self.generar_hipotesis_catl(datos_can[\"CAN_CATL\"])\n",
    "            self.documentos_rag.extend(hipotesis_catl)\n",
    "            self.stats['hipotesis_catl'] = len(hipotesis_catl)\n",
    "        \n",
    "        # 3. Procesar documentación técnica\n",
    "        print(\"📚 Procesando documentación técnica...\")\n",
    "        docs_tecnicos = procesador_docs.procesar_documento_completo(\n",
    "            \"documentacion/J1939_reference.txt\", \"semantico\"\n",
    "        )\n",
    "        self.documentos_rag.extend(docs_tecnicos)\n",
    "        self.stats['documentacion_tecnica'] = len(docs_tecnicos)\n",
    "        \n",
    "        # 4. Calcular estadísticas finales\n",
    "        self.stats['total_documentos'] = len(self.documentos_rag)\n",
    "        if self.documentos_rag:\n",
    "            self.stats['calidad_promedio'] = np.mean([\n",
    "                doc.calidad_descripcion for doc in self.documentos_rag\n",
    "            ])\n",
    "        \n",
    "        # 5. Exportar a JSONL (usando fallback si jsonlines no está disponible)\n",
    "        archivo_salida = self.ruta_salida / \"dataset_rag_decode_ev.jsonl\"\n",
    "        \n",
    "        try:\n",
    "            import jsonlines\n",
    "            with jsonlines.open(archivo_salida, mode='w') as writer:\n",
    "                for doc in self.documentos_rag:\n",
    "                    writer.write(doc.to_jsonl_entry())\n",
    "        except ImportError:\n",
    "            # Fallback: exportar como JSON estándar\n",
    "            import json\n",
    "            with open(archivo_salida, 'w', encoding='utf-8') as f:\n",
    "                for doc in self.documentos_rag:\n",
    "                    json.dump(doc.to_jsonl_entry(), f, ensure_ascii=False)\n",
    "                    f.write('\\n')\n",
    "        \n",
    "        print(f\"✅ Dataset RAG guardado en: {archivo_salida}\")\n",
    "        print(f\"📊 Estadísticas finales: {self.stats}\")\n",
    "        \n",
    "        return str(archivo_salida)\n",
    "    \n",
    "    def generar_muestra_dataset(self, n_muestras: int = 3) -> Dict:\n",
    "        \"\"\"\n",
    "        Genera muestra del dataset para inspección\n",
    "        \"\"\"\n",
    "        muestra = {}\n",
    "        \n",
    "        if len(self.documentos_rag) >= n_muestras:\n",
    "            for i in range(n_muestras):\n",
    "                doc = self.documentos_rag[i]\n",
    "                muestra[f\"muestra_{i+1}\"] = {\n",
    "                    \"id\": doc.id,\n",
    "                    \"tipo\": doc.tipo_documento,\n",
    "                    \"contenido_preview\": doc.contenido_textual[:200] + \"...\",\n",
    "                    \"calidad\": doc.calidad_descripcion,\n",
    "                    \"evento_vehicular\": doc.metadatos.evento_vehiculo,\n",
    "                    \"red_can\": doc.metadatos.red_can\n",
    "                }\n",
    "        \n",
    "        return muestra\n",
    "\n",
    "# === EJECUCIÓN DEL CONSTRUCTOR ===\n",
    "\n",
    "# Inicializar constructor y ejecutar\n",
    "print(\"🚀 Iniciando construcción del dataset RAG...\")\n",
    "constructor_rag = ConstructorDatasetRAG()\n",
    "archivo_dataset = constructor_rag.construir_dataset_completo()\n",
    "\n",
    "# Mostrar muestra del dataset generado\n",
    "muestra = constructor_rag.generar_muestra_dataset(3)\n",
    "print(\"\\n📋 MUESTRA DEL DATASET GENERADO:\")\n",
    "print(\"=\"*70)\n",
    "for key, valor in muestra.items():\n",
    "    print(f\"{key.upper()}:\")\n",
    "    for k, v in valor.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763bf7f8",
   "metadata": {},
   "source": [
    "## 7. Validación y Evaluación de Características Generadas\n",
    "\n",
    "### Métricas de Calidad Implementadas\n",
    "\n",
    "1. **Calidad Textual:** Coherencia, longitud, riqueza vocabulario\n",
    "2. **Completitud Metadatos:** Cobertura de campos obligatorios\n",
    "3. **Consistencia Temporal:** Coherencia en secuencias temporales\n",
    "4. **Cobertura de Señales:** Proporción de señales documentadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373b77d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Analizando calidad del dataset generado...\n",
      "\n",
      "📊 REPORTE DE CALIDAD DEL DATASET\n",
      "============================================================\n",
      "\n",
      "1. DISTRIBUCIÓN POR TIPOS:\n",
      "  📄 documentacion_tecnica: 2 documentos (calidad: 0.900)\n",
      "  📄 evento_can: 5 documentos (calidad: 0.707)\n",
      "\n",
      "2. ESTADÍSTICAS DE LONGITUD:\n",
      "  📏 Promedio: 664 caracteres\n",
      "  📏 Rango: 488 - 858 caracteres\n",
      "\n",
      "3. COBERTURA POR REDES CAN:\n",
      "  🔌 DOCUMENTACION: 2 documentos\n",
      "  🔌 CAN_CUSTOM_31: 5 documentos\n",
      "\n",
      "4. EVENTOS VEHICULARES:\n",
      "  🚗 referencia_tecnica: 2 eventos\n",
      "  🚗 carga: 5 eventos\n",
      "\n",
      "5. MÉTRICAS GLOBALES:\n",
      "  📈 Total documentos: 7\n",
      "  📈 Calidad promedio: 0.762\n",
      "  📈 Documentos alta calidad: 6\n",
      "  📈 Cobertura temporal: 2 días únicos\n",
      "\n",
      "✅ ANÁLISIS DE CALIDAD COMPLETADO\n",
      "✅ Dataset RAG listo para uso en sistemas de IA conversacional\n",
      "\n",
      "================================================================================\n",
      "🎯 FEATURE ENGINEERING COMPLETADO EXITOSAMENTE\n",
      "🎯 Dataset RAG preparado para DECODE-EV\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mEl kernel se bloqueó al ejecutar código en la celda actual o en una celda anterior. \n",
      "\u001b[1;31mRevise el código de las celdas para identificar una posible causa del error. \n",
      "\u001b[1;31mHaga clic <a href='https://aka.ms/vscodeJupyterKernelCrash'>aquí</a> para obtener más información. \n",
      "\u001b[1;31mVea Jupyter <a href='command:jupyter.viewOutput'>log</a> para obtener más detalles."
     ]
    }
   ],
   "source": [
    "# Análisis de calidad del dataset generado\n",
    "def analizar_calidad_dataset(documentos: List[RAGDocument]) -> Dict:\n",
    "    \"\"\"\n",
    "    Análisis completo de calidad del dataset RAG generado\n",
    "    \"\"\"\n",
    "    \n",
    "    if not documentos:\n",
    "        return {\"error\": \"No hay documentos para analizar\"}\n",
    "    \n",
    "    analisis = {\n",
    "        'distribucion_tipos': {},\n",
    "        'calidad_promedio_por_tipo': {},\n",
    "        'estadisticas_longitud': {},\n",
    "        'cobertura_redes_can': {},\n",
    "        'eventos_por_tipo': {},\n",
    "        'metricas_globales': {}\n",
    "    }\n",
    "    \n",
    "    # 1. Distribución por tipos de documento\n",
    "    tipos = [doc.tipo_documento for doc in documentos]\n",
    "    for tipo in set(tipos):\n",
    "        analisis['distribucion_tipos'][tipo] = tipos.count(tipo)\n",
    "        \n",
    "        # Calidad promedio por tipo\n",
    "        docs_tipo = [doc for doc in documentos if doc.tipo_documento == tipo]\n",
    "        analisis['calidad_promedio_por_tipo'][tipo] = np.mean([\n",
    "            doc.calidad_descripcion for doc in docs_tipo\n",
    "        ])\n",
    "    \n",
    "    # 2. Estadísticas de longitud de texto\n",
    "    longitudes = [len(doc.contenido_textual) for doc in documentos]\n",
    "    analisis['estadisticas_longitud'] = {\n",
    "        'promedio': np.mean(longitudes),\n",
    "        'mediana': np.median(longitudes),\n",
    "        'min': min(longitudes),\n",
    "        'max': max(longitudes),\n",
    "        'desviacion': np.std(longitudes)\n",
    "    }\n",
    "    \n",
    "    # 3. Cobertura por redes CAN\n",
    "    redes_can = [doc.metadatos.red_can for doc in documentos]\n",
    "    for red in set(redes_can):\n",
    "        analisis['cobertura_redes_can'][red] = redes_can.count(red)\n",
    "    \n",
    "    # 4. Distribución de eventos vehiculares\n",
    "    eventos = [doc.metadatos.evento_vehiculo for doc in documentos]\n",
    "    for evento in set(eventos):\n",
    "        analisis['eventos_por_tipo'][evento] = eventos.count(evento)\n",
    "    \n",
    "    # 5. Métricas globales\n",
    "    analisis['metricas_globales'] = {\n",
    "        'total_documentos': len(documentos),\n",
    "        'calidad_promedio_global': np.mean([doc.calidad_descripcion for doc in documentos]),\n",
    "        'documentos_alta_calidad': sum(1 for doc in documentos if doc.calidad_descripcion > 0.7),\n",
    "        'cobertura_temporal': len(set([doc.metadatos.timestamp_inicio[:10] for doc in documentos]))\n",
    "    }\n",
    "    \n",
    "    return analisis\n",
    "\n",
    "# === ANÁLISIS DE CALIDAD FINAL ===\n",
    "\n",
    "# Ejecutar análisis de calidad\n",
    "if constructor_rag.documentos_rag:\n",
    "    print(\"🔍 Analizando calidad del dataset generado...\")\n",
    "    analisis_calidad = analizar_calidad_dataset(constructor_rag.documentos_rag)\n",
    "    \n",
    "    print(\"\\n📊 REPORTE DE CALIDAD DEL DATASET\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\\n1. DISTRIBUCIÓN POR TIPOS:\")\n",
    "    for tipo, cantidad in analisis_calidad['distribucion_tipos'].items():\n",
    "        calidad = analisis_calidad['calidad_promedio_por_tipo'][tipo]\n",
    "        print(f\"  📄 {tipo}: {cantidad} documentos (calidad: {calidad:.3f})\")\n",
    "    \n",
    "    print(\"\\n2. ESTADÍSTICAS DE LONGITUD:\")\n",
    "    stats_long = analisis_calidad['estadisticas_longitud']\n",
    "    print(f\"  📏 Promedio: {stats_long['promedio']:.0f} caracteres\")\n",
    "    print(f\"  📏 Rango: {stats_long['min']:.0f} - {stats_long['max']:.0f} caracteres\")\n",
    "    \n",
    "    print(\"\\n3. COBERTURA POR REDES CAN:\")\n",
    "    for red, cantidad in analisis_calidad['cobertura_redes_can'].items():\n",
    "        print(f\"  🔌 {red}: {cantidad} documentos\")\n",
    "    \n",
    "    print(\"\\n4. EVENTOS VEHICULARES:\")\n",
    "    for evento, cantidad in analisis_calidad['eventos_por_tipo'].items():\n",
    "        print(f\"  🚗 {evento}: {cantidad} eventos\")\n",
    "    \n",
    "    print(\"\\n5. MÉTRICAS GLOBALES:\")\n",
    "    metricas = analisis_calidad['metricas_globales']\n",
    "    print(f\"  📈 Total documentos: {metricas['total_documentos']}\")\n",
    "    print(f\"  📈 Calidad promedio: {metricas['calidad_promedio_global']:.3f}\")\n",
    "    print(f\"  📈 Documentos alta calidad: {metricas['documentos_alta_calidad']}\")\n",
    "    print(f\"  📈 Cobertura temporal: {metricas['cobertura_temporal']} días únicos\")\n",
    "    \n",
    "    print(\"\\n✅ ANÁLISIS DE CALIDAD COMPLETADO\")\n",
    "    print(\"✅ Dataset RAG listo para uso en sistemas de IA conversacional\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No se encontraron documentos para analizar\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🎯 FEATURE ENGINEERING COMPLETADO EXITOSAMENTE\")\n",
    "print(\"🎯 Dataset RAG preparado para DECODE-EV\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4526d597",
   "metadata": {},
   "source": [
    "## ENTREGABLES DE LA FASE 2: INGENIERÍA DE CARACTERÍSTICAS\n",
    "\n",
    "### ENTREGABLE 1: Script de Generación de Características Textuales\n",
    "**Estado:** COMPLETADO\n",
    "- Clase `GeneradorDescripcionesTextual` implementada\n",
    "- Plantillas programáticas para análisis temporal\n",
    "- Algoritmos de detección de patrones (incremento, decremento, estable, picos)\n",
    "- Sistema de clasificación de intensidad automática\n",
    "\n",
    "### ENTREGABLE 2: Dataset RAG Procesado\n",
    "**Estado:** COMPLETADO\n",
    "- Archivo `dataset_rag_decode_ev.jsonl` generado\n",
    "- Formato unificado para sistemas de recuperación\n",
    "- Metadatos estructurados para filtrado eficiente\n",
    "- Integración de eventos CAN + documentación técnica + hipótesis CATL\n",
    "\n",
    "### ENTREGABLE 3: Sistema de Metadatos Estructurados\n",
    "**Estado:** COMPLETADO\n",
    "- Clase `CANEventMetadata` con esquema JSON\n",
    "- Clasificación automática de eventos vehiculares\n",
    "- Determinación de contexto operativo\n",
    "- Sistema de calidad y validación\n",
    "\n",
    "---\n",
    "\n",
    "## JUSTIFICACIÓN METODOLÓGICA (CRISP-ML)\n",
    "\n",
    "### Reinterpretación de \"Feature Engineering\" para LLM/RAG\n",
    "\n",
    "**Paradigma Tradicional ML:**\n",
    "- Características numéricas (mean, std, correlaciones)\n",
    "- Vectores de features para algoritmos tabulares\n",
    "- Optimización para modelos supervisados\n",
    "\n",
    "**Nuevo Paradigma LLM/RAG:**\n",
    "- **Características textuales:** Descripciones narrativas ricas\n",
    "- **Metadatos estructurados:** Para filtrado y recuperación\n",
    "- **Chunking estratégico:** Optimización para vectorización\n",
    "- **Calidad semántica:** Coherencia y completitud textual\n",
    "\n",
    "### Ventajas del Enfoque LLM/RAG vs ML Tradicional\n",
    "\n",
    "| **Aspecto** | **ML Tradicional** | **LLM/RAG (Nuevo)** |\n",
    "|-------------|-------------------|---------------------|\n",
    "| **Interpretabilidad** | Features numéricas abstractas | Descripciones en lenguaje natural |\n",
    "| **Escalabilidad** | Requiere reentrenamiento | Adaptable con nuevos documentos |\n",
    "| **Flexibilidad** | Arquitectura fija | Consultas en lenguaje natural |\n",
    "| **Conocimiento experto** | Difícil de incorporar | Integrable via documentación |\n",
    "| **CAN_CATL (caja negra)** | Imposible sin labels | Generación de hipótesis textuales |\n",
    "\n",
    "---\n",
    "\n",
    "## PRÓXIMOS PASOS SUGERIDOS\n",
    "\n",
    "### Fase 3: Implementación del Sistema RAG\n",
    "1. **Vectorización:** Generar embeddings para el dataset\n",
    "2. **Base de vectores:** Implementar índice de búsqueda (FAISS/Pinecone)\n",
    "3. **Pipeline RAG:** Integrar retrieval + generation\n",
    "4. **Evaluación:** Métricas de relevancia y coherencia\n",
    "\n",
    "### Optimizaciones Propuestas\n",
    "1. **LLM Especializado:** Fine-tuning con terminología automotriz\n",
    "2. **Chunking Adapativo:** Tamaño dinámico según complejidad\n",
    "3. **Metadatos Enriquecidos:** Integración con ontologías CAN\n",
    "4. **Validación Experta:** Feedback loop con ingenieros automotrices"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
