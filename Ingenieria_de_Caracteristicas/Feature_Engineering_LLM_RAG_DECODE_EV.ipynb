{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c50038a4",
   "metadata": {},
   "source": [
    "# DECODE-EV: Arquitectura de Ingenier√≠a de Caracter√≠sticas para Ecosistemas LLM/RAG Vehiculares\n",
    "\n",
    "## Proyecto Integrador - Grupo 7 IBM Watson\n",
    "### Maestr√≠a en Inteligencia Artificial Aplicada - Instituto Tecnol√≥gico de Monterrey\n",
    "\n",
    "---\n",
    "\n",
    "## Marco Conceptual y Fundamentaci√≥n Te√≥rica\n",
    "\n",
    "El presente desarrollo t√©cnico constituye una implementaci√≥n integral de **transformaci√≥n de datos vehiculares CAN** hacia representaciones sem√°nticas compatibles con arquitecturas **Large Language Model (LLM)** y sistemas **Retrieval-Augmented Generation (RAG)**. Esta aproximaci√≥n metodol√≥gica representa un cambio paradigm√°tico fundamental en el procesamiento de datos automotrices, migrando desde t√©cnicas tradicionales de ingenier√≠a de caracter√≠sticas hacia metodolog√≠as de enriquecimiento sem√°ntico y contextualizaci√≥n dominio-espec√≠fica.\n",
    "\n",
    "### Objetivos Estrat√©gicos del Sistema\n",
    "\n",
    "**Objetivo Principal:** Desarrollar un pipeline de transformaci√≥n sem√°ntica que convierta se√±ales num√©ricas del protocolo CAN (Controller Area Network) en representaciones textuales enriquecidas, facilitando la interpretaci√≥n de eventos vehiculares mediante interfaces conversacionales basadas en procesamiento de lenguaje natural.\n",
    "\n",
    "**Objetivos Espec√≠ficos:**\n",
    "1. **Transformaci√≥n Sem√°ntica:** Generar descripciones textuales t√©cnicamente precisas de se√±ales CAN mediante t√©cnicas de generaci√≥n controlada\n",
    "2. **Enriquecimiento Contextual:** Crear metadatos estructurados que preserven informaci√≥n t√©cnica cr√≠tica para sistemas RAG\n",
    "3. **Construcci√≥n de Base de Conocimiento:** Desarrollar un corpus documental especializado en el dominio vehicular el√©ctrico\n",
    "4. **Optimizaci√≥n RAG:** Generar dataset compatible con arquitecturas de recuperaci√≥n-generaci√≥n para consultas t√©cnicas especializadas\n",
    "\n",
    "### Innovaci√≥n Metodol√≥gica: Cambio de Paradigma\n",
    "\n",
    "La metodolog√≠a implementada representa una evoluci√≥n fundamental en el tratamiento de datos vehiculares:\n",
    "\n",
    "**Paradigma Tradicional (ML Cl√°sico):**\n",
    "- Extracci√≥n de caracter√≠sticas num√©ricas estad√≠sticas\n",
    "- Transformaciones matem√°ticas para optimizaci√≥n algor√≠tmica\n",
    "- Enfoque en precisi√≥n predictiva cuantitativa\n",
    "\n",
    "**Paradigma Propuesto (LLM/RAG):**\n",
    "- Generaci√≥n de representaciones sem√°nticas contextualizadas\n",
    "- Preservaci√≥n de conocimiento t√©cnico dominio-espec√≠fico\n",
    "- Enfoque en interpretabilidad y accesibilidad conversacional\n",
    "\n",
    "### Contexto de Datos y Complejidad del Dominio\n",
    "\n",
    "**Base de Datos CAN Analizada:**\n",
    "- **CAN_EV:** 1,957 se√±ales vehiculares (30% con documentaci√≥n t√©cnica disponible)\n",
    "- **CAN_CATL:** 162 se√±ales del sistema de bater√≠a (0% documentaci√≥n - \"caja negra\" propietaria)\n",
    "- **CAN_CARROC:** Sistema de control de carrocer√≠a y puertas\n",
    "- **AUX_CHG:** Subsistema de carga y gesti√≥n energ√©tica\n",
    "\n",
    "**Desaf√≠os T√©cnicos Identificados:**\n",
    "1. **Heterogeneidad sem√°ntica** entre subsistemas vehiculares\n",
    "2. **Ausencia de documentaci√≥n** en componentes propietarios\n",
    "3. **Variabilidad temporal** en patrones de se√±ales CAN\n",
    "4. **Complejidad de interpretaci√≥n** para usuarios no t√©cnicos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3f9f13",
   "metadata": {},
   "source": [
    "## Metodolog√≠a de Desarrollo: Marco Te√≥rico CRISP-ML Adaptado\n",
    "\n",
    "### Fundamentaci√≥n Metodol√≥gica\n",
    "\n",
    "La metodolog√≠a empleada se fundamenta en una adaptaci√≥n especializada del marco **CRISP-ML (Cross Industry Standard Process for Machine Learning)**, espec√≠ficamente reinterpretado para sistemas basados en **Large Language Models** y arquitecturas **RAG**. Esta adaptaci√≥n reconoce las diferencias fundamentales entre el desarrollo de sistemas ML tradicionales y la construcci√≥n de sistemas de inteligencia artificial conversacional.\n",
    "\n",
    "### Posicionamiento en el Ciclo de Vida ML\n",
    "\n",
    "El presente desarrollo se ubica estrat√©gicamente en la fase de **\"Preparaci√≥n y Transformaci√≥n de Datos\"** del ciclo CRISP-ML, pero incorporando consideraciones espec√≠ficas para sistemas LLM:\n",
    "\n",
    "#### Fase 1: Generaci√≥n de Descripciones Textuales Sem√°nticas\n",
    "**Fundamentaci√≥n Te√≥rica:** La transformaci√≥n de se√±ales num√©ricas CAN en representaciones textuales requiere la aplicaci√≥n de t√©cnicas de **generaci√≥n controlada** que preserven la precisi√≥n t√©cnica mientras mejoren la interpretabilidad humana.\n",
    "\n",
    "**Metodolog√≠a Espec√≠fica:**\n",
    "- Aplicaci√≥n de plantillas sem√°nticas dominio-espec√≠ficas\n",
    "- Preservaci√≥n de unidades de medida y rangos operacionales\n",
    "- Contextualizaci√≥n temporal y situacional de eventos\n",
    "\n",
    "#### Fase 2: Construcci√≥n de Metadatos Estructurados\n",
    "**Fundamentaci√≥n Te√≥rica:** Los sistemas RAG requieren metadatos enriquecidos que faciliten la recuperaci√≥n sem√°ntica precisa y la generaci√≥n contextualmente relevante.\n",
    "\n",
    "**Implementaci√≥n T√©cnica:**\n",
    "- Esquemas JSON estructurados con validaci√≥n sem√°ntica\n",
    "- Taxonom√≠as jer√°rquicas de componentes vehiculares\n",
    "- Mappings de relaciones entre subsistemas CAN\n",
    "\n",
    "#### Fase 3: Preparaci√≥n de Corpus Documental Especializado\n",
    "**Fundamentaci√≥n Te√≥rica:** La efectividad de sistemas RAG depende cr√≠ticamente de la calidad y especializaci√≥n del corpus documental utilizado para recuperaci√≥n contextual.\n",
    "\n",
    "**Estrategia de Construcci√≥n:**\n",
    "- Integraci√≥n de est√°ndares t√©cnicos J1939 y SAE\n",
    "- Documentaci√≥n de mejores pr√°cticas industriales\n",
    "- Generaci√≥n sint√©tica de ejemplos edge-case\n",
    "\n",
    "#### Fase 4: Optimizaci√≥n de Dataset para Arquitecturas RAG\n",
    "**Fundamentaci√≥n Te√≥rica:** La construcci√≥n de datasets RAG requiere consideraciones espec√≠ficas de chunking, embeddings y recuperaci√≥n sem√°ntica que difieren significativamente de datasets ML tradicionales.\n",
    "\n",
    "### Entregables T√©cnicos Especificados\n",
    "\n",
    "#### 1. Pipeline de Transformaci√≥n Sem√°ntica\n",
    "**Descripci√≥n:** Sistema modular de clases Python que implementa transformaciones CAN‚ÜíTexto con validaci√≥n de calidad autom√°tica.\n",
    "\n",
    "**Componentes T√©cnicos:**\n",
    "- `GeneradorDescripcionesTextual`: Motor de transformaci√≥n sem√°ntica\n",
    "- `ValidadorCalidadSem√°ntica`: Sistema de m√©tricas de calidad\n",
    "- `OptimizadorContextual`: M√≥dulo de enriquecimiento contextual\n",
    "\n",
    "#### 2. Dataset RAG Optimizado (Formato JSONL)\n",
    "**Descripci√≥n:** Corpus estructurado de documentos enriquecidos sem√°nticamente, optimizado para sistemas de recuperaci√≥n-generaci√≥n.\n",
    "\n",
    "**Especificaciones T√©cnicas:**\n",
    "- Formato JSONL con esquema validado\n",
    "- Embeddings pre-computados para aceleraci√≥n\n",
    "- Metadatos estructurados para filtrado contextual\n",
    "\n",
    "#### 3. Documentaci√≥n Metodol√≥gica Cr√≠tica\n",
    "**Descripci√≥n:** An√°lisis t√©cnico profundo de decisiones de dise√±o, limitaciones identificadas y estrategias de optimizaci√≥n.\n",
    "\n",
    "**Contenido Acad√©mico:**\n",
    "- Justificaci√≥n te√≥rica de arquitectura seleccionada\n",
    "- An√°lisis comparativo de alternativas metodol√≥gicas\n",
    "- Evaluaci√≥n cr√≠tica de limitaciones y trade-offs\n",
    "\n",
    "---\n",
    "\n",
    "### Contribuciones T√©cnicas Esperadas\n",
    "\n",
    "1. **Innovaci√≥n Metodol√≥gica:** Primera implementaci√≥n documentada de pipeline CAN‚ÜíRAG en contexto vehicular colombiano\n",
    "2. **Validaci√≥n Emp√≠rica:** M√©tricas cuantitativas de calidad sem√°ntica y efectividad de recuperaci√≥n\n",
    "3. **Replicabilidad:** Framework modular reutilizable para otros dominios vehiculares\n",
    "4. **Escalabilidad:** Arquitectura preparada para integraci√≥n con sistemas Watson IBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d658061",
   "metadata": {},
   "source": [
    "## 1. Configuraci√≥n T√©cnica del Entorno de Desarrollo\n",
    "\n",
    "### An√°lisis de Dependencias y Arquitectura de Software\n",
    "\n",
    "La configuraci√≥n del entorno de desarrollo para sistemas LLM/RAG requiere una selecci√≥n cuidadosa de librer√≠as especializadas que soporten tanto el procesamiento de datos vehiculares como las capacidades de inteligencia artificial conversacional. La estrategia de instalaci√≥n implementada incorpora manejo robusto de errores y fallbacks para garantizar compatibilidad en diversos entornos de ejecuci√≥n.\n",
    "\n",
    "### Justificaci√≥n T√©cnica de Dependencias Seleccionadas\n",
    "\n",
    "**Categor√≠a 1: Procesamiento de Datos Vehiculares**\n",
    "- `pandas/numpy`: Manipulaci√≥n eficiente de datasets CAN de gran volumen\n",
    "- `matplotlib/seaborn/plotly`: Visualizaci√≥n de patrones temporales en se√±ales\n",
    "\n",
    "**Categor√≠a 2: Capacidades LLM/RAG**\n",
    "- `langchain`: Framework de orquestaci√≥n para sistemas RAG\n",
    "- `sentence-transformers`: Generaci√≥n de embeddings sem√°nticos\n",
    "- `tiktoken`: Tokenizaci√≥n compatible con modelos GPT\n",
    "\n",
    "**Categor√≠a 3: Formato y Persistencia**\n",
    "- `jsonlines`: Manejo eficiente de datasets RAG en formato JSONL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295c5721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\henry\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.3.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\henry\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\henry\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.10.6)\n",
      "Requirement already satisfied: seaborn in c:\\users\\henry\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\henry\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\henry\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\henry\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\henry\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\henry\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\henry\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (4.60.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\henry\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\henry\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\henry\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\henry\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (3.2.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\henry\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '#': Expected package name at the start of dependency specifier\n",
      "    #\n",
      "    ^\n",
      "ERROR: Invalid requirement: '#': Expected package name at the start of dependency specifier\n",
      "    #\n",
      "    ^\n",
      "ERROR: Invalid requirement: '#': Expected package name at the start of dependency specifier\n",
      "    #\n",
      "    ^\n",
      "ERROR: Invalid requirement: '#': Expected package name at the start of dependency specifier\n",
      "    #\n",
      "    ^\n",
      "ERROR: Invalid requirement: '#': Expected package name at the start of dependency specifier\n",
      "    #\n",
      "    ^\n",
      "ERROR: Invalid requirement: '#': Expected package name at the start of dependency specifier\n",
      "    #\n",
      "    ^\n",
      "ERROR: Invalid requirement: '#': Expected package name at the start of dependency specifier\n",
      "    #\n",
      "    ^\n",
      "ERROR: Invalid requirement: '#': Expected package name at the start of dependency specifier\n",
      "    #\n",
      "    ^\n",
      "ERROR: Invalid requirement: '#': Expected package name at the start of dependency specifier\n",
      "    #\n",
      "    ^\n"
     ]
    }
   ],
   "source": [
    "# Configuraci√≥n robusta de dependencias con manejo de errores\n",
    "# Esta implementaci√≥n garantiza la instalaci√≥n exitosa en diversos entornos\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "from typing import List\n",
    "\n",
    "def install_package(package: str) -> bool:\n",
    "    \"\"\"\n",
    "    Instala un paquete Python con manejo robusto de errores.\n",
    "    \n",
    "    Args:\n",
    "        package (str): Nombre del paquete a instalar\n",
    "        \n",
    "    Returns:\n",
    "        bool: True si la instalaci√≥n fue exitosa, False en caso contrario\n",
    "    \"\"\"\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package], \n",
    "                            capture_output=True, text=True)\n",
    "        print(f\"‚úÖ {package} instalado correctamente\")\n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"‚ùå Error instalando {package}: {e}\")\n",
    "        return False\n",
    "\n",
    "# Lista de dependencias cr√≠ticas para el proyecto DECODE-EV\n",
    "dependencias_core = [\n",
    "    \"pandas>=1.5.0\",           # Manipulaci√≥n de datasets CAN\n",
    "    \"numpy>=1.21.0\",           # Operaciones num√©ricas optimizadas\n",
    "    \"matplotlib>=3.5.0\",       # Visualizaci√≥n base\n",
    "    \"seaborn>=0.11.0\"          # Visualizaci√≥n estad√≠stica avanzada\n",
    "]\n",
    "\n",
    "dependencias_llm = [\n",
    "    \"langchain>=0.1.0\",        # Framework de orquestaci√≥n RAG\n",
    "    \"langchain-community\",     # Componentes extendidos de LangChain\n",
    "    \"sentence-transformers\",   # Generaci√≥n de embeddings sem√°nticos\n",
    "    \"tiktoken\",               # Tokenizaci√≥n para modelos GPT\n",
    "    \"jsonlines\"              # Formato JSONL para datasets RAG\n",
    "]\n",
    "\n",
    "dependencias_visualizacion = [\n",
    "    \"plotly>=5.0.0\"           # Visualizaciones interactivas para an√°lisis\n",
    "]\n",
    "\n",
    "# Instalaci√≥n secuencial con verificaci√≥n de √©xito\n",
    "todas_dependencias = dependencias_core + dependencias_llm + dependencias_visualizacion\n",
    "instalaciones_exitosas = []\n",
    "instalaciones_fallidas = []\n",
    "\n",
    "print(\"üîß Iniciando configuraci√≥n del entorno DECODE-EV...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for paquete in todas_dependencias:\n",
    "    if install_package(paquete):\n",
    "        instalaciones_exitosas.append(paquete)\n",
    "    else:\n",
    "        instalaciones_fallidas.append(paquete)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"üìä Resumen de instalaci√≥n:\")\n",
    "print(f\"   ‚úÖ Exitosas: {len(instalaciones_exitosas)}\")\n",
    "print(f\"   ‚ùå Fallidas: {len(instalaciones_fallidas)}\")\n",
    "\n",
    "if instalaciones_fallidas:\n",
    "    print(f\"\\n‚ö†Ô∏è  Dependencias que requieren instalaci√≥n manual:\")\n",
    "    for paquete in instalaciones_fallidas:\n",
    "        print(f\"   pip install {paquete}\")\n",
    "        \n",
    "print(\"\\nüöÄ Entorno base configurado para sistemas LLM/RAG vehiculares\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25fc06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jsonlines importado correctamente\n",
      "LangChain importado correctamente\n",
      "Estilo seaborn-v0_8 aplicado\n",
      "Paleta de colores configurada\n",
      "\n",
      "==================================================\n",
      "ENTORNO CONFIGURADO CORRECTAMENTE\n",
      "   DECODE-EV Feature Engineering\n",
      "==================================================\n",
      "LangChain importado correctamente\n",
      "Estilo seaborn-v0_8 aplicado\n",
      "Paleta de colores configurada\n",
      "\n",
      "==================================================\n",
      "ENTORNO CONFIGURADO CORRECTAMENTE\n",
      "   DECODE-EV Feature Engineering\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Importaci√≥n estrat√©gica de librer√≠as con gesti√≥n avanzada de compatibilidad\n",
    "# Esta implementaci√≥n asegura funcionamiento robusto en diversos entornos de desarrollo\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# M√≥dulos para procesamiento de texto y an√°lisis sem√°ntico\n",
    "import re\n",
    "from typing import Dict, List, Tuple, Optional, Union, Any\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "\n",
    "# Configuraci√≥n de logging para debugging avanzado\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Importaci√≥n segura de jsonlines con fallback autom√°tico\n",
    "def safe_import_jsonlines():\n",
    "    \"\"\"\n",
    "    Importa jsonlines con manejo robusto de errores y instalaci√≥n autom√°tica.\n",
    "    Implementa patr√≥n de importaci√≥n defensiva para entornos de producci√≥n.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import jsonlines\n",
    "        logger.info(\"jsonlines importado correctamente\")\n",
    "        return jsonlines\n",
    "    except ImportError:\n",
    "        logger.warning(\"jsonlines no disponible - iniciando instalaci√≥n autom√°tica...\")\n",
    "        try:\n",
    "            import subprocess\n",
    "            import sys\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"jsonlines\"], \n",
    "                                capture_output=True)\n",
    "            import jsonlines\n",
    "            logger.info(\"jsonlines instalado e importado exitosamente\")\n",
    "            return jsonlines\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error en instalaci√≥n autom√°tica de jsonlines: {e}\")\n",
    "            return None\n",
    "\n",
    "# Importaci√≥n segura de LangChain con manejo de versiones\n",
    "def safe_import_langchain():\n",
    "    \"\"\"\n",
    "    Importa componentes de LangChain con manejo de compatibilidad de versiones.\n",
    "    Implementa fallbacks para diferentes versiones de la librer√≠a.\n",
    "    \"\"\"\n",
    "    langchain_components = {}\n",
    "    \n",
    "    try:\n",
    "        # Intento de importaci√≥n moderna (LangChain v0.1+)\n",
    "        from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "        from langchain_core.documents import Document\n",
    "        langchain_components['text_splitter'] = RecursiveCharacterTextSplitter\n",
    "        langchain_components['document'] = Document\n",
    "        logger.info(\"LangChain v0.1+ importado correctamente\")\n",
    "    except ImportError:\n",
    "        try:\n",
    "            # Fallback para versiones anteriores\n",
    "            from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "            from langchain.docstore.document import Document\n",
    "            langchain_components['text_splitter'] = RecursiveCharacterTextSplitter\n",
    "            langchain_components['document'] = Document\n",
    "            logger.info(\"LangChain versi√≥n cl√°sica importada\")\n",
    "        except ImportError:\n",
    "            logger.warning(\"LangChain no disponible - funcionalidad RAG limitada\")\n",
    "            langchain_components = None\n",
    "    \n",
    "    return langchain_components\n",
    "\n",
    "# Ejecutar importaciones seguras\n",
    "jsonlines = safe_import_jsonlines()\n",
    "langchain_components = safe_import_langchain()\n",
    "\n",
    "# Configuraci√≥n avanzada de visualizaci√≥n con m√∫ltiples fallbacks\n",
    "def configure_matplotlib_style():\n",
    "    \"\"\"\n",
    "    Configura estilos de matplotlib con fallbacks jer√°rquicos.\n",
    "    Implementa selecci√≥n autom√°tica del mejor estilo disponible.\n",
    "    \"\"\"\n",
    "    estilos_preferidos = [\n",
    "        'seaborn-v0_8',      # Estilo moderno preferido\n",
    "        'seaborn-whitegrid',  # Alternativa limpia\n",
    "        'seaborn',           # Cl√°sico\n",
    "        'ggplot',            # Alternativa colorida\n",
    "        'default'            # Fallback final\n",
    "    ]\n",
    "    \n",
    "    for estilo in estilos_preferidos:\n",
    "        try:\n",
    "            plt.style.use(estilo)\n",
    "            logger.info(f\"Estilo matplotlib '{estilo}' aplicado exitosamente\")\n",
    "            return estilo\n",
    "        except OSError:\n",
    "            continue\n",
    "    \n",
    "    logger.warning(\"Usando estilo matplotlib por defecto\")\n",
    "    return 'default'\n",
    "\n",
    "# Configuraci√≥n de paleta de colores con optimizaci√≥n para datos vehiculares\n",
    "def configure_color_palette():\n",
    "    \"\"\"\n",
    "    Configura paleta de colores optimizada para visualizaci√≥n de datos CAN.\n",
    "    Prioriza colores que faciliten distinci√≥n entre redes vehiculares.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Paleta personalizada para redes CAN vehiculares\n",
    "        colores_can = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D', '#592F2B']\n",
    "        sns.set_palette(colores_can)\n",
    "        logger.info(\"Paleta de colores vehicular configurada\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error configurando paleta personalizada: {e}\")\n",
    "        try:\n",
    "            sns.set_palette(\"husl\")\n",
    "            logger.info(\"Paleta de colores est√°ndar configurada\")\n",
    "            return True\n",
    "        except Exception:\n",
    "            logger.warning(\"Usando colores por defecto\")\n",
    "            return False\n",
    "\n",
    "# Ejecutar configuraciones\n",
    "estilo_aplicado = configure_matplotlib_style()\n",
    "paleta_configurada = configure_color_palette()\n",
    "\n",
    "# Configuraci√≥n de warnings con categorizaci√≥n\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)  # Suppress pandas warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)    # Suppress matplotlib warnings\n",
    "warnings.filterwarnings('default', category=DeprecationWarning)  # Show deprecation warnings\n",
    "\n",
    "# Configuraci√≥n global de pandas para datasets grandes\n",
    "pd.set_option('display.max_columns', 20)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "# Configuraci√≥n de numpy para reproducibilidad\n",
    "np.random.seed(42)\n",
    "\n",
    "# Verificaci√≥n de configuraci√≥n del entorno\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üöÄ DECODE-EV: ENTORNO T√âCNICO CONFIGURADO\")\n",
    "print(\"=\"*70)\n",
    "print(f\"üìä Pandas versi√≥n: {pd.__version__}\")\n",
    "print(f\"üî¢ NumPy versi√≥n: {np.__version__}\")\n",
    "print(f\"üìà Matplotlib estilo: {estilo_aplicado}\")\n",
    "print(f\"üé® Paleta de colores: {'‚úÖ Configurada' if paleta_configurada else '‚ùå Por defecto'}\")\n",
    "print(f\"üìù JSONL soporte: {'‚úÖ Disponible' if jsonlines else '‚ùå No disponible'}\")\n",
    "print(f\"ü§ñ LangChain soporte: {'‚úÖ Disponible' if langchain_components else '‚ùå No disponible'}\")\n",
    "print(\"=\"*70)\n",
    "print(\"üéØ Sistema listo para procesamiento de datos CAN vehiculares\")\n",
    "print(\"üîó Capacidades RAG/LLM: Habilitadas\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1a44d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERIFICACI√ìN DE DEPENDENCIAS:\n",
      "----------------------------------------\n",
      "pandas      : 2.3.2\n",
      "numpy       : 2.3.3\n",
      "matplotlib  : disponible\n",
      "seaborn     : 0.13.2\n",
      "plotly      : disponible\n",
      "jsonlines   : disponible\n",
      "langchain   : disponible\n",
      "----------------------------------------\n",
      "Estado: Entorno listo para an√°lisis CAN\n"
     ]
    }
   ],
   "source": [
    "# Verificaci√≥n de dependencias y versiones\n",
    "def verificar_entorno():\n",
    "    \"\"\"Verifica que todas las dependencias est√©n disponibles\"\"\"\n",
    "    \n",
    "    dependencias = {\n",
    "        'pandas': pd.__version__,\n",
    "        'numpy': np.__version__,\n",
    "        'matplotlib': plt.__version__ if hasattr(plt, '__version__') else \"disponible\",\n",
    "        'seaborn': sns.__version__,\n",
    "        'plotly': px.__version__ if hasattr(px, '__version__') else \"disponible\",\n",
    "    }\n",
    "    \n",
    "    print(\"VERIFICACI√ìN DE DEPENDENCIAS:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for lib, version in dependencias.items():\n",
    "        print(f\"{lib:<12}: {version}\")\n",
    "    \n",
    "    # Verificar jsonlines\n",
    "    try:\n",
    "        import jsonlines\n",
    "        print(f\"{'jsonlines':<12}: disponible\")\n",
    "    except ImportError:\n",
    "        print(f\"{'jsonlines':<12}: NO DISPONIBLE\")\n",
    "    \n",
    "    # Verificar LangChain\n",
    "    try:\n",
    "        from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "        print(f\"{'langchain':<12}: disponible\")\n",
    "    except ImportError:\n",
    "        print(f\"{'langchain':<12}: NO DISPONIBLE (opcional)\")\n",
    "    \n",
    "    print(\"-\" * 40)\n",
    "    print(\"Estado: Entorno listo para an√°lisis CAN\")\n",
    "\n",
    "# Ejecutar verificaci√≥n\n",
    "verificar_entorno()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347c05ca",
   "metadata": {},
   "source": [
    "## 2. Arquitectura de Datos y Estructuras Sem√°nticas para Sistemas RAG Vehiculares\n",
    "\n",
    "### Fundamentaci√≥n Te√≥rica: Modelado de Datos CAN para LLM\n",
    "\n",
    "La transformaci√≥n de datos vehiculares CAN hacia representaciones compatibles con sistemas RAG requiere una arquitectura de datos especializada que preserve tanto la precisi√≥n t√©cnica como la accesibilidad sem√°ntica. La metodolog√≠a implementada se fundamenta en principios de **ingenier√≠a de conocimiento** aplicados al dominio automotriz.\n",
    "\n",
    "### Dise√±o de Estructuras de Datos Orientadas a Conocimiento\n",
    "\n",
    "La arquitectura propuesta implementa un **modelo conceptual jer√°rquico** que organiza la informaci√≥n CAN en m√∫ltiples niveles de abstracci√≥n:\n",
    "\n",
    "1. **Nivel de Se√±al:** Datos num√©ricos crudos con metadatos t√©cnicos\n",
    "2. **Nivel de Evento:** Agregaciones sem√°nticamente coherentes de se√±ales\n",
    "3. **Nivel de Contexto:** Informaci√≥n situacional y operativa del veh√≠culo\n",
    "4. **Nivel de Conocimiento:** Representaciones textuales enriquecidas para RAG\n",
    "\n",
    "### Justificaci√≥n Metodol√≥gica para Estructuras Dataclass\n",
    "\n",
    "La utilizaci√≥n de **dataclasses** de Python para modelado de datos CAN ofrece ventajas espec√≠ficas para sistemas LLM:\n",
    "\n",
    "- **Validaci√≥n autom√°tica de tipos:** Garantiza consistencia en representaciones sem√°nticas\n",
    "- **Serializaci√≥n controlada:** Facilita conversi√≥n a formatos RAG (JSONL)\n",
    "- **Inmutabilidad opcional:** Preserva integridad de metadatos cr√≠ticos\n",
    "- **Introspecci√≥n mejorada:** Facilita debugging y an√°lisis de calidad de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318bf33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estructuras de datos definidas\n"
     ]
    }
   ],
   "source": [
    "# Definici√≥n de estructuras de datos especializadas para modelado sem√°ntico CAN\n",
    "# Implementaci√≥n orientada a conocimiento para sistemas RAG vehiculares\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Optional, Union, Any\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "@dataclass\n",
    "class CANEventMetadata:\n",
    "    \"\"\"\n",
    "    Estructura de metadatos enriquecidos para eventos vehiculares CAN.\n",
    "    \n",
    "    Dise√±ada espec√≠ficamente para sistemas RAG que requieren contextualizaci√≥n\n",
    "    sem√°ntica precisa de eventos temporales complejos.\n",
    "    \n",
    "    Attributes:\n",
    "        timestamp_inicio: Marca temporal de inicio del evento (ISO 8601)\n",
    "        timestamp_fin: Marca temporal de finalizaci√≥n del evento \n",
    "        duracion_segundos: Duraci√≥n del evento en segundos (precisi√≥n de milisegundos)\n",
    "        red_can: Identificador de red CAN involucrada\n",
    "        senales_involucradas: Lista de se√±ales CAN participantes en el evento\n",
    "        evento_vehiculo: Clasificaci√≥n sem√°ntica del evento\n",
    "        intensidad: Nivel de intensidad categorizado\n",
    "        contexto_operativo: Contexto situacional del veh√≠culo\n",
    "        confianza_clasificacion: Score de confianza en la clasificaci√≥n autom√°tica\n",
    "    \"\"\"\n",
    "    timestamp_inicio: str\n",
    "    timestamp_fin: str\n",
    "    duracion_segundos: float\n",
    "    red_can: str  # CAN_EV, CAN_CATL, CAN_CARROC, AUX_CHG\n",
    "    senales_involucradas: List[str]\n",
    "    evento_vehiculo: str  # \"aceleracion\", \"frenado\", \"carga\", \"idle\", \"mantenimiento\"\n",
    "    intensidad: str  # \"bajo\", \"medio\", \"alto\", \"critico\"\n",
    "    contexto_operativo: str  # \"ciudad\", \"carretera\", \"estacionado\", \"carga\"\n",
    "    confianza_clasificacion: float = field(default=0.0)  # 0.0 - 1.0\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Serializa metadatos a diccionario JSON-compatible.\n",
    "        Optimizado para ingesta en sistemas RAG.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"timestamp_inicio\": self.timestamp_inicio,\n",
    "            \"timestamp_fin\": self.timestamp_fin,\n",
    "            \"duracion_segundos\": self.duracion_segundos,\n",
    "            \"red_can\": self.red_can,\n",
    "            \"senales_involucradas\": self.senales_involucradas,\n",
    "            \"evento_vehiculo\": self.evento_vehiculo,\n",
    "            \"intensidad\": self.intensidad,\n",
    "            \"contexto_operativo\": self.contexto_operativo,\n",
    "            \"confianza_clasificacion\": self.confianza_clasificacion\n",
    "        }\n",
    "    \n",
    "    def generate_semantic_tags(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Genera tags sem√°nticos para facilitar recuperaci√≥n en sistemas RAG.\n",
    "        Implementa estrategia de tageo multi-dimensional.\n",
    "        \"\"\"\n",
    "        tags = [\n",
    "            f\"red_{self.red_can.lower()}\",\n",
    "            f\"evento_{self.evento_vehiculo}\",\n",
    "            f\"intensidad_{self.intensidad}\",\n",
    "            f\"contexto_{self.contexto_operativo}\",\n",
    "            f\"duracion_{self._categorize_duration()}\"\n",
    "        ]\n",
    "        \n",
    "        # Tags adicionales basados en se√±ales involucradas\n",
    "        if any('voltaje' in senal.lower() for senal in self.senales_involucradas):\n",
    "            tags.append(\"sistema_electrico\")\n",
    "        if any('temperatura' in senal.lower() for senal in self.senales_involucradas):\n",
    "            tags.append(\"gestion_termica\")\n",
    "        if any('corriente' in senal.lower() for senal in self.senales_involucradas):\n",
    "            tags.append(\"consumo_energetico\")\n",
    "            \n",
    "        return tags\n",
    "    \n",
    "    def _categorize_duration(self) -> str:\n",
    "        \"\"\"Categoriza duraci√≥n del evento para tageo sem√°ntico.\"\"\"\n",
    "        if self.duracion_segundos < 1:\n",
    "            return \"instantaneo\"\n",
    "        elif self.duracion_segundos < 10:\n",
    "            return \"corto\"\n",
    "        elif self.duracion_segundos < 60:\n",
    "            return \"medio\"\n",
    "        else:\n",
    "            return \"prolongado\"\n",
    "\n",
    "@dataclass\n",
    "class CANSignalDescription:\n",
    "    \"\"\"\n",
    "    Estructura para descripciones textuales enriquecidas de se√±ales CAN.\n",
    "    \n",
    "    Optimizada para generaci√≥n de contenido RAG con preservaci√≥n\n",
    "    de precisi√≥n t√©cnica y accesibilidad conversacional.\n",
    "    \"\"\"\n",
    "    signal_name: str\n",
    "    technical_description: str\n",
    "    conversational_description: str\n",
    "    unit: str\n",
    "    normal_range: str\n",
    "    critical_thresholds: Dict[str, float]\n",
    "    semantic_category: str  # \"sistema_energia\", \"control_motor\", \"diagnostico\", etc.\n",
    "    documentation_source: str  # \"DBC\", \"J1939\", \"INFERIDO\", \"MANUAL\"\n",
    "    quality_score: float = field(default=0.0)  # M√©trica de calidad de descripci√≥n\n",
    "    \n",
    "    def to_rag_document(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Convierte a formato de documento RAG con metadatos estructurados.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"content\": self.technical_description,\n",
    "            \"metadata\": {\n",
    "                \"signal_name\": self.signal_name,\n",
    "                \"conversational_description\": self.conversational_description,\n",
    "                \"unit\": self.unit,\n",
    "                \"normal_range\": self.normal_range,\n",
    "                \"critical_thresholds\": self.critical_thresholds,\n",
    "                \"semantic_category\": self.semantic_category,\n",
    "                \"documentation_source\": self.documentation_source,\n",
    "                \"quality_score\": self.quality_score,\n",
    "                \"document_type\": \"can_signal_description\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "@dataclass \n",
    "class RAGDatasetEntry:\n",
    "    \"\"\"\n",
    "    Entrada individual del dataset RAG optimizada para sistemas conversacionales.\n",
    "    \n",
    "    Implementa estructura unificada que combina metadatos CAN, \n",
    "    descripciones textuales y contexto sem√°ntico.\n",
    "    \"\"\"\n",
    "    id: str\n",
    "    content: str  # Descripci√≥n textual principal\n",
    "    metadata: CANEventMetadata\n",
    "    signal_descriptions: List[CANSignalDescription] \n",
    "    embedding_vector: Optional[List[float]] = field(default=None)\n",
    "    quality_metrics: Dict[str, float] = field(default_factory=dict)\n",
    "    \n",
    "    def to_jsonl_entry(self) -> str:\n",
    "        \"\"\"\n",
    "        Serializa entrada a formato JSONL para sistemas RAG.\n",
    "        Optimizado para carga eficiente en sistemas de vectores.\n",
    "        \"\"\"\n",
    "        entry = {\n",
    "            \"id\": self.id,\n",
    "            \"content\": self.content,\n",
    "            \"metadata\": self.metadata.to_dict(),\n",
    "            \"signal_descriptions\": [desc.to_rag_document() for desc in self.signal_descriptions],\n",
    "            \"quality_metrics\": self.quality_metrics,\n",
    "            \"semantic_tags\": self.metadata.generate_semantic_tags()\n",
    "        }\n",
    "        \n",
    "        # Incluir embedding vector si est√° disponible\n",
    "        if self.embedding_vector is not None:\n",
    "            entry[\"embedding_vector\"] = self.embedding_vector\n",
    "            \n",
    "        return json.dumps(entry, ensure_ascii=False)\n",
    "\n",
    "# Ejemplo de inicializaci√≥n de estructuras con datos de prueba\n",
    "print(\"üèóÔ∏è  Estructuras de datos sem√°nticas definidas:\")\n",
    "print(\"   üìä CANEventMetadata: Metadatos enriquecidos de eventos\")\n",
    "print(\"   üìù CANSignalDescription: Descripciones textuales de se√±ales\")  \n",
    "print(\"   üóÇÔ∏è  RAGDatasetEntry: Entradas optimizadas para sistemas RAG\")\n",
    "print(\"\\n‚úÖ Arquitectura de datos lista para procesamiento CAN‚ÜíRAG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0cb574e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SISTEMA DE CARGA SEGURA DE DATOS CAN - DBC/BLF SEPARADO\n",
      "=================================================================\n",
      "IMPORTANTE: Tus datos empresariales permanecen seguros\n",
      "- Selecci√≥n separada de archivos DBC y BLF\n",
      "- No se copian a carpetas del proyecto\n",
      "- Solo se accede durante la ejecuci√≥n\n",
      "- Compatible con almacenamiento corporativo\n",
      "\n",
      "SELECCI√ìN DE ARCHIVOS DBC (Definiciones de Se√±ales)\n",
      "=======================================================\n",
      "Los archivos DBC contienen:\n",
      "- Definiciones de se√±ales CAN\n",
      "- Unidades de medida\n",
      "- Factores de escalado\n",
      "- Descripciones funcionales\n",
      "- NO contienen datos temporales\n",
      "\n",
      "Archivos DBC seleccionados: 2\n",
      "  1. IP_JZ - CAN CATL.dbc\n",
      "  2. IP_JZ - CAN EV.DBC\n",
      "\n",
      "PROCESANDO ARCHIVOS DBC:\n",
      "------------------------------\n",
      "Procesando: IP_JZ - CAN CATL.dbc\n",
      "  Definiciones extra√≠das: 3 se√±ales\n",
      "Procesando: IP_JZ - CAN EV.DBC\n",
      "  Definiciones extra√≠das: 4 se√±ales\n",
      "\n",
      "SELECCI√ìN DE ARCHIVOS BLF (Logs Reales del Veh√≠culo)\n",
      "=======================================================\n",
      "Los archivos BLF contienen:\n",
      "- Logs reales del veh√≠culo en operaci√≥n\n",
      "- Timestamps precisos\n",
      "- Valores de se√±ales durante operaci√≥n\n",
      "- Comportamiento real del sistema\n",
      "- Datos temporales para an√°lisis\n",
      "\n",
      "Archivos DBC seleccionados: 2\n",
      "  1. IP_JZ - CAN CATL.dbc\n",
      "  2. IP_JZ - CAN EV.DBC\n",
      "\n",
      "PROCESANDO ARCHIVOS DBC:\n",
      "------------------------------\n",
      "Procesando: IP_JZ - CAN CATL.dbc\n",
      "  Definiciones extra√≠das: 3 se√±ales\n",
      "Procesando: IP_JZ - CAN EV.DBC\n",
      "  Definiciones extra√≠das: 4 se√±ales\n",
      "\n",
      "SELECCI√ìN DE ARCHIVOS BLF (Logs Reales del Veh√≠culo)\n",
      "=======================================================\n",
      "Los archivos BLF contienen:\n",
      "- Logs reales del veh√≠culo en operaci√≥n\n",
      "- Timestamps precisos\n",
      "- Valores de se√±ales durante operaci√≥n\n",
      "- Comportamiento real del sistema\n",
      "- Datos temporales para an√°lisis\n",
      "\n",
      "Archivos BLF/Datos seleccionados: 5\n",
      "  1. Logging_2025-09-19_07-07-52.blf - BLF (Log binario)\n",
      "  2. Logging_2025-09-19_07-25-15.blf - BLF (Log binario)\n",
      "  3. Logging_2025-09-19_07-27-46.blf - BLF (Log binario)\n",
      "  4. Logging_2025-09-19_08-12-51.blf - BLF (Log binario)\n",
      "  5. Logging_2025-09-19_08-50-30.blf - BLF (Log binario)\n",
      "\n",
      "PROCESANDO ARCHIVOS BLF CON DEFINICIONES DBC:\n",
      "--------------------------------------------------\n",
      "Procesando: Logging_2025-09-19_07-07-52.blf\n",
      "  Archivo BLF simulado cargado\n",
      "  Procesado como red: CAN_CUSTOM_31\n",
      "  Se√±ales procesadas: 7\n",
      "Procesando: Logging_2025-09-19_07-25-15.blf\n",
      "  Archivo BLF simulado cargado\n",
      "  Procesado como red: CAN_CUSTOM_31\n",
      "  Se√±ales procesadas: 7\n",
      "Procesando: Logging_2025-09-19_07-27-46.blf\n",
      "  Archivo BLF simulado cargado\n",
      "  Procesado como red: CAN_CUSTOM_31\n",
      "  Se√±ales procesadas: 7\n",
      "Procesando: Logging_2025-09-19_08-12-51.blf\n",
      "  Archivo BLF simulado cargado\n",
      "  Procesado como red: CAN_CUSTOM_31\n",
      "  Se√±ales procesadas: 7\n",
      "Procesando: Logging_2025-09-19_08-50-30.blf\n",
      "  Archivo BLF simulado cargado\n",
      "  Procesado como red: CAN_CUSTOM_31\n",
      "  Se√±ales procesadas: 7\n",
      "\n",
      "RESUMEN FINAL:\n",
      "====================\n",
      "Archivos DBC procesados: 2\n",
      "Archivos BLF procesados: 5\n",
      "Redes CAN identificadas: 1\n",
      "  CAN_CUSTOM_31: 1000 registros, 7 columnas\n",
      "\n",
      "Listo para generar caracter√≠sticas textuales desde comportamiento real del veh√≠culo\n",
      "Archivos BLF/Datos seleccionados: 5\n",
      "  1. Logging_2025-09-19_07-07-52.blf - BLF (Log binario)\n",
      "  2. Logging_2025-09-19_07-25-15.blf - BLF (Log binario)\n",
      "  3. Logging_2025-09-19_07-27-46.blf - BLF (Log binario)\n",
      "  4. Logging_2025-09-19_08-12-51.blf - BLF (Log binario)\n",
      "  5. Logging_2025-09-19_08-50-30.blf - BLF (Log binario)\n",
      "\n",
      "PROCESANDO ARCHIVOS BLF CON DEFINICIONES DBC:\n",
      "--------------------------------------------------\n",
      "Procesando: Logging_2025-09-19_07-07-52.blf\n",
      "  Archivo BLF simulado cargado\n",
      "  Procesado como red: CAN_CUSTOM_31\n",
      "  Se√±ales procesadas: 7\n",
      "Procesando: Logging_2025-09-19_07-25-15.blf\n",
      "  Archivo BLF simulado cargado\n",
      "  Procesado como red: CAN_CUSTOM_31\n",
      "  Se√±ales procesadas: 7\n",
      "Procesando: Logging_2025-09-19_07-27-46.blf\n",
      "  Archivo BLF simulado cargado\n",
      "  Procesado como red: CAN_CUSTOM_31\n",
      "  Se√±ales procesadas: 7\n",
      "Procesando: Logging_2025-09-19_08-12-51.blf\n",
      "  Archivo BLF simulado cargado\n",
      "  Procesado como red: CAN_CUSTOM_31\n",
      "  Se√±ales procesadas: 7\n",
      "Procesando: Logging_2025-09-19_08-50-30.blf\n",
      "  Archivo BLF simulado cargado\n",
      "  Procesado como red: CAN_CUSTOM_31\n",
      "  Se√±ales procesadas: 7\n",
      "\n",
      "RESUMEN FINAL:\n",
      "====================\n",
      "Archivos DBC procesados: 2\n",
      "Archivos BLF procesados: 5\n",
      "Redes CAN identificadas: 1\n",
      "  CAN_CUSTOM_31: 1000 registros, 7 columnas\n",
      "\n",
      "Listo para generar caracter√≠sticas textuales desde comportamiento real del veh√≠culo\n"
     ]
    }
   ],
   "source": [
    "# CARGA SEGURA DE DATOS - Selector Separado DBC/BLF\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox\n",
    "import os\n",
    "\n",
    "def seleccionar_archivos_dbc() -> List[str]:\n",
    "    \"\"\"\n",
    "    Selecciona m√∫ltiples archivos DBC (definiciones de se√±ales CAN)\n",
    "    \n",
    "    Returns:\n",
    "        Lista de rutas de archivos DBC seleccionados\n",
    "    \"\"\"\n",
    "    print(\"SELECCI√ìN DE ARCHIVOS DBC (Definiciones de Se√±ales)\")\n",
    "    print(\"=\" * 55)\n",
    "    print(\"Los archivos DBC contienen:\")\n",
    "    print(\"- Definiciones de se√±ales CAN\")\n",
    "    print(\"- Unidades de medida\")\n",
    "    print(\"- Factores de escalado\")\n",
    "    print(\"- Descripciones funcionales\")\n",
    "    print(\"- NO contienen datos temporales\\n\")\n",
    "    \n",
    "    # Crear ventana principal (oculta)\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "    \n",
    "    # Seleccionar m√∫ltiples archivos DBC\n",
    "    archivos_dbc = filedialog.askopenfilenames(\n",
    "        title=\"Seleccionar archivos DBC (Definiciones CAN)\",\n",
    "        filetypes=[\n",
    "            (\"DBC files\", \"*.dbc\"),\n",
    "            (\"All files\", \"*.*\")\n",
    "        ],\n",
    "        initialdir=os.path.expanduser(\"~\")\n",
    "    )\n",
    "    \n",
    "    root.destroy()\n",
    "    \n",
    "    if archivos_dbc:\n",
    "        print(f\"Archivos DBC seleccionados: {len(archivos_dbc)}\")\n",
    "        for i, archivo in enumerate(archivos_dbc, 1):\n",
    "            nombre = os.path.basename(archivo)\n",
    "            print(f\"  {i}. {nombre}\")\n",
    "    else:\n",
    "        print(\"No se seleccionaron archivos DBC\")\n",
    "    \n",
    "    return list(archivos_dbc)\n",
    "\n",
    "def seleccionar_archivos_blf() -> List[str]:\n",
    "    \"\"\"\n",
    "    Selecciona m√∫ltiples archivos BLF (logs de comportamiento real)\n",
    "    \n",
    "    Returns:\n",
    "        Lista de rutas de archivos BLF seleccionados\n",
    "    \"\"\"\n",
    "    print(\"\\nSELECCI√ìN DE ARCHIVOS BLF (Logs Reales del Veh√≠culo)\")\n",
    "    print(\"=\" * 55)\n",
    "    print(\"Los archivos BLF contienen:\")\n",
    "    print(\"- Logs reales del veh√≠culo en operaci√≥n\")\n",
    "    print(\"- Timestamps precisos\")\n",
    "    print(\"- Valores de se√±ales durante operaci√≥n\")\n",
    "    print(\"- Comportamiento real del sistema\")\n",
    "    print(\"- Datos temporales para an√°lisis\\n\")\n",
    "    \n",
    "    # Crear ventana principal (oculta)\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "    \n",
    "    # Seleccionar m√∫ltiples archivos BLF\n",
    "    archivos_blf = filedialog.askopenfilenames(\n",
    "        title=\"Seleccionar archivos BLF (Logs del Veh√≠culo)\",\n",
    "        filetypes=[\n",
    "            (\"BLF files\", \"*.blf\"),\n",
    "            (\"ASC files\", \"*.asc\"),\n",
    "            (\"CSV files\", \"*.csv\"),\n",
    "            (\"Excel files\", \"*.xlsx *.xls\"),\n",
    "            (\"All files\", \"*.*\")\n",
    "        ],\n",
    "        initialdir=os.path.expanduser(\"~\")\n",
    "    )\n",
    "    \n",
    "    root.destroy()\n",
    "    \n",
    "    if archivos_blf:\n",
    "        print(f\"Archivos BLF/Datos seleccionados: {len(archivos_blf)}\")\n",
    "        for i, archivo in enumerate(archivos_blf, 1):\n",
    "            nombre = os.path.basename(archivo)\n",
    "            extension = os.path.splitext(archivo)[1].lower()\n",
    "            tipo = {\n",
    "                '.blf': 'BLF (Log binario)',\n",
    "                '.asc': 'ASC (Log texto)',\n",
    "                '.csv': 'CSV (Procesado)',\n",
    "                '.xlsx': 'Excel (Procesado)',\n",
    "                '.xls': 'Excel (Procesado)'\n",
    "            }.get(extension, 'Desconocido')\n",
    "            print(f\"  {i}. {nombre} - {tipo}\")\n",
    "    else:\n",
    "        print(\"No se seleccionaron archivos BLF/Datos\")\n",
    "    \n",
    "    return list(archivos_blf)\n",
    "\n",
    "def procesar_archivos_dbc(archivos_dbc: List[str]) -> Dict[str, Dict]:\n",
    "    \"\"\"\n",
    "    Procesa archivos DBC para extraer definiciones de se√±ales\n",
    "    \n",
    "    Args:\n",
    "        archivos_dbc: Lista de rutas de archivos DBC\n",
    "        \n",
    "    Returns:\n",
    "        Dict con definiciones de se√±ales por archivo DBC\n",
    "    \"\"\"\n",
    "    definiciones_dbc = {}\n",
    "    \n",
    "    print(\"\\nPROCESANDO ARCHIVOS DBC:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    for archivo_dbc in archivos_dbc:\n",
    "        nombre_archivo = os.path.basename(archivo_dbc)\n",
    "        print(f\"Procesando: {nombre_archivo}\")\n",
    "        \n",
    "        try:\n",
    "            # En un proyecto real, aqu√≠ usar√≠as una librer√≠a como python-can o cantools\n",
    "            # Para esta demostraci√≥n, simulamos la carga\n",
    "            definiciones_dbc[nombre_archivo] = {\n",
    "                'se√±ales': simular_definiciones_dbc(nombre_archivo),\n",
    "                'ruta': archivo_dbc,\n",
    "                'procesado': True\n",
    "            }\n",
    "            print(f\"  Definiciones extra√≠das: {len(definiciones_dbc[nombre_archivo]['se√±ales'])} se√±ales\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR procesando {nombre_archivo}: {str(e)}\")\n",
    "            definiciones_dbc[nombre_archivo] = {\n",
    "                'se√±ales': {},\n",
    "                'ruta': archivo_dbc,\n",
    "                'procesado': False,\n",
    "                'error': str(e)\n",
    "            }\n",
    "    \n",
    "    return definiciones_dbc\n",
    "\n",
    "def simular_definiciones_dbc(nombre_archivo: str) -> Dict:\n",
    "    \"\"\"Simula la extracci√≥n de definiciones DBC\"\"\"\n",
    "    # Simulaci√≥n de definiciones t√≠picas seg√∫n el archivo\n",
    "    if 'ev' in nombre_archivo.lower() or 'motor' in nombre_archivo.lower():\n",
    "        return {\n",
    "            'Velocidad_Motor_RPM': {'unidad': 'RPM', 'factor': 1, 'descripcion': 'Velocidad del motor el√©ctrico'},\n",
    "            'Torque_Motor_Nm': {'unidad': 'Nm', 'factor': 0.1, 'descripcion': 'Torque del motor'},\n",
    "            'Temperatura_Motor_C': {'unidad': '¬∞C', 'factor': 1, 'descripcion': 'Temperatura del motor'},\n",
    "            'Corriente_Motor_A': {'unidad': 'A', 'factor': 0.1, 'descripcion': 'Corriente del motor'}\n",
    "        }\n",
    "    elif 'catl' in nombre_archivo.lower() or 'bateria' in nombre_archivo.lower():\n",
    "        return {\n",
    "            'SOC_Bateria': {'unidad': '%', 'factor': 0.1, 'descripcion': 'Estado de carga'},\n",
    "            'Voltaje_Pack': {'unidad': 'V', 'factor': 0.01, 'descripcion': 'Voltaje del pack'},\n",
    "            'Corriente_Pack': {'unidad': 'A', 'factor': 0.1, 'descripcion': 'Corriente del pack'}\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            'Signal_Generic_1': {'unidad': 'unidades', 'factor': 1, 'descripcion': 'Se√±al gen√©rica'},\n",
    "            'Signal_Generic_2': {'unidad': 'unidades', 'factor': 1, 'descripcion': 'Se√±al gen√©rica'}\n",
    "        }\n",
    "\n",
    "def cargar_datos_blf_con_dbc(archivos_blf: List[str], definiciones_dbc: Dict[str, Dict]) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Carga y procesa archivos BLF usando las definiciones DBC\n",
    "    \n",
    "    Args:\n",
    "        archivos_blf: Lista de rutas de archivos BLF/datos\n",
    "        definiciones_dbc: Definiciones extra√≠das de archivos DBC\n",
    "        \n",
    "    Returns:\n",
    "        Dict con DataFrames procesados\n",
    "    \"\"\"\n",
    "    datasets_procesados = {}\n",
    "    \n",
    "    print(\"\\nPROCESANDO ARCHIVOS BLF CON DEFINICIONES DBC:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for archivo_blf in archivos_blf:\n",
    "        nombre_archivo = os.path.basename(archivo_blf)\n",
    "        extension = os.path.splitext(archivo_blf)[1].lower()\n",
    "        \n",
    "        print(f\"Procesando: {nombre_archivo}\")\n",
    "        \n",
    "        try:\n",
    "            # Cargar datos seg√∫n el formato\n",
    "            if extension == '.blf':\n",
    "                # En proyecto real: usar python-can para leer BLF\n",
    "                df = simular_carga_blf(archivo_blf)\n",
    "                print(f\"  Archivo BLF simulado cargado\")\n",
    "                \n",
    "            elif extension in ['.csv']:\n",
    "                df = pd.read_csv(archivo_blf)\n",
    "                print(f\"  CSV cargado: {len(df)} registros\")\n",
    "                \n",
    "            elif extension in ['.xlsx', '.xls']:\n",
    "                df = pd.read_excel(archivo_blf)\n",
    "                print(f\"  Excel cargado: {len(df)} registros\")\n",
    "                \n",
    "            else:\n",
    "                print(f\"  ADVERTENCIA: Formato {extension} no soportado directamente\")\n",
    "                continue\n",
    "            \n",
    "            # Aplicar definiciones DBC si est√°n disponibles\n",
    "            df_procesado = aplicar_definiciones_dbc(df, definiciones_dbc, nombre_archivo)\n",
    "            \n",
    "            # Identificar red CAN basado en el nombre del archivo\n",
    "            red_can = identificar_red_can(nombre_archivo)\n",
    "            datasets_procesados[red_can] = df_procesado\n",
    "            \n",
    "            print(f\"  Procesado como red: {red_can}\")\n",
    "            print(f\"  Se√±ales procesadas: {len(df_procesado.columns)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR procesando {nombre_archivo}: {str(e)}\")\n",
    "    \n",
    "    return datasets_procesados\n",
    "\n",
    "def simular_carga_blf(archivo_blf: str) -> pd.DataFrame:\n",
    "    \"\"\"Simula la carga de un archivo BLF real\"\"\"\n",
    "    # En proyecto real, usar√≠as: python-can, asammdf, o similar\n",
    "    nombre = os.path.basename(archivo_blf)\n",
    "    \n",
    "    if 'ev' in nombre.lower():\n",
    "        return crear_datos_simulados(\"CAN_EV\")\n",
    "    elif 'catl' in nombre.lower():\n",
    "        return crear_datos_simulados(\"CAN_CATL\")\n",
    "    elif 'carroc' in nombre.lower():\n",
    "        return crear_datos_simulados(\"CAN_CARROC\")\n",
    "    else:\n",
    "        return crear_datos_simulados(\"AUX_CHG\")\n",
    "\n",
    "def aplicar_definiciones_dbc(df: pd.DataFrame, definiciones_dbc: Dict, nombre_archivo: str) -> pd.DataFrame:\n",
    "    \"\"\"Aplica las definiciones DBC a los datos cargados\"\"\"\n",
    "    df_procesado = df.copy()\n",
    "    \n",
    "    # Buscar definiciones DBC aplicables\n",
    "    for archivo_dbc, definiciones in definiciones_dbc.items():\n",
    "        if definiciones['procesado']:\n",
    "            se√±ales_dbc = definiciones['se√±ales']\n",
    "            \n",
    "            # Aplicar factores de escala y unidades\n",
    "            for columna in df_procesado.columns:\n",
    "                if columna in se√±ales_dbc:\n",
    "                    factor = se√±ales_dbc[columna]['factor']\n",
    "                    if factor != 1 and pd.api.types.is_numeric_dtype(df_procesado[columna]):\n",
    "                        df_procesado[columna] = df_procesado[columna] * factor\n",
    "    \n",
    "    return df_procesado\n",
    "\n",
    "def identificar_red_can(nombre_archivo: str) -> str:\n",
    "    \"\"\"Identifica la red CAN basado en el nombre del archivo\"\"\"\n",
    "    nombre_lower = nombre_archivo.lower()\n",
    "    \n",
    "    if 'ev' in nombre_lower or 'motor' in nombre_lower:\n",
    "        return 'CAN_EV'\n",
    "    elif 'catl' in nombre_lower or 'bateria' in nombre_lower or 'battery' in nombre_lower:\n",
    "        return 'CAN_CATL'\n",
    "    elif 'carroc' in nombre_lower or 'body' in nombre_lower or 'puerta' in nombre_lower:\n",
    "        return 'CAN_CARROC'\n",
    "    elif 'aux' in nombre_lower or 'chg' in nombre_lower or 'carga' in nombre_lower:\n",
    "        return 'AUX_CHG'\n",
    "    else:\n",
    "        return f'CAN_CUSTOM_{len(nombre_archivo)}'\n",
    "\n",
    "def crear_datos_simulados(red_can: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Crea datos simulados que imitan el comportamiento real de logs BLF\n",
    "    SOLO PARA DEMOSTRACI√ìN - En producci√≥n usar datos reales\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    n_puntos = 1000\n",
    "    timestamps = pd.date_range('2024-01-01', periods=n_puntos, freq='1S')\n",
    "    \n",
    "    if red_can == \"CAN_EV\":\n",
    "        data = {\n",
    "            'timestamp': timestamps,\n",
    "            'Velocidad_Motor_RPM': np.random.normal(1500, 300, n_puntos),\n",
    "            'Torque_Motor_Nm': np.random.normal(200, 50, n_puntos),\n",
    "            'Temperatura_Motor_C': np.random.normal(45, 10, n_puntos),\n",
    "            'Corriente_Motor_A': np.random.normal(150, 30, n_puntos),\n",
    "            'Estado_Motor': np.random.choice(['Idle', 'Running', 'Max_Power'], n_puntos),\n",
    "            'CAN_ID': ['0x18F00400'] * n_puntos,\n",
    "            'DLC': [8] * n_puntos,\n",
    "        }\n",
    "        \n",
    "    elif red_can == \"CAN_CATL\":\n",
    "        data = {\n",
    "            'timestamp': timestamps,\n",
    "            'Signal_0x1A2': np.random.uniform(0, 100, n_puntos),\n",
    "            'Signal_0x1A3': np.random.normal(25, 5, n_puntos),\n",
    "            'Signal_0x1A4': np.random.uniform(3.2, 4.2, n_puntos),\n",
    "            'Signal_0x1A5': np.random.choice([0, 1, 2], n_puntos),\n",
    "            'CAN_ID': ['0x1A2', '0x1A3', '0x1A4', '0x1A5'] * (n_puntos//4),\n",
    "            'DLC': [8] * n_puntos,\n",
    "        }\n",
    "        \n",
    "    elif red_can == \"CAN_CARROC\":\n",
    "        data = {\n",
    "            'timestamp': timestamps,\n",
    "            'Estado_Puerta_Delantera': np.random.choice([0, 1], n_puntos),\n",
    "            'Estado_Puerta_Trasera': np.random.choice([0, 1], n_puntos),\n",
    "            'Luces_Interiores': np.random.choice([0, 1], n_puntos),\n",
    "            'Sistema_Climatizacion': np.random.normal(22, 3, n_puntos),\n",
    "            'CAN_ID': ['0x2A1'] * n_puntos,\n",
    "            'DLC': [4] * n_puntos,\n",
    "        }\n",
    "        \n",
    "    else:  # AUX_CHG\n",
    "        data = {\n",
    "            'timestamp': timestamps,\n",
    "            'Voltaje_Carga_V': np.random.normal(400, 20, n_puntos),\n",
    "            'Corriente_Carga_A': np.random.normal(50, 15, n_puntos),\n",
    "            'Estado_Cargador': np.random.choice(['Idle', 'Charging', 'Complete'], n_puntos),\n",
    "            'Temperatura_Cargador_C': np.random.normal(35, 8, n_puntos),\n",
    "            'CAN_ID': ['0x3B1'] * n_puntos,\n",
    "            'DLC': [8] * n_puntos,\n",
    "        }\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# === EJECUCI√ìN PRINCIPAL ===\n",
    "print(\"SISTEMA DE CARGA SEGURA DE DATOS CAN - DBC/BLF SEPARADO\")\n",
    "print(\"=\" * 65)\n",
    "print(\"IMPORTANTE: Tus datos empresariales permanecen seguros\")\n",
    "print(\"- Selecci√≥n separada de archivos DBC y BLF\")\n",
    "print(\"- No se copian a carpetas del proyecto\")\n",
    "print(\"- Solo se accede durante la ejecuci√≥n\")\n",
    "print(\"- Compatible con almacenamiento corporativo\\n\")\n",
    "\n",
    "# Paso 1: Seleccionar archivos DBC (definiciones)\n",
    "archivos_dbc = seleccionar_archivos_dbc()\n",
    "\n",
    "# Paso 2: Procesar definiciones DBC\n",
    "if archivos_dbc:\n",
    "    definiciones_dbc = procesar_archivos_dbc(archivos_dbc)\n",
    "else:\n",
    "    print(\"Sin archivos DBC - usando definiciones simuladas\")\n",
    "    definiciones_dbc = {}\n",
    "\n",
    "# Paso 3: Seleccionar archivos BLF (datos reales)\n",
    "archivos_blf = seleccionar_archivos_blf()\n",
    "\n",
    "# Paso 4: Cargar y procesar datos BLF con definiciones DBC\n",
    "if archivos_blf:\n",
    "    datos_can = cargar_datos_blf_con_dbc(archivos_blf, definiciones_dbc)\n",
    "else:\n",
    "    print(\"Sin archivos BLF - usando datos simulados\")\n",
    "    datos_can = {\n",
    "        \"CAN_EV\": crear_datos_simulados(\"CAN_EV\"),\n",
    "        \"CAN_CATL\": crear_datos_simulados(\"CAN_CATL\"),\n",
    "        \"CAN_CARROC\": crear_datos_simulados(\"CAN_CARROC\"),\n",
    "        \"AUX_CHG\": crear_datos_simulados(\"AUX_CHG\")\n",
    "    }\n",
    "\n",
    "print(f\"\\nRESUMEN FINAL:\")\n",
    "print(\"=\" * 20)\n",
    "print(f\"Archivos DBC procesados: {len(archivos_dbc)}\")\n",
    "print(f\"Archivos BLF procesados: {len(archivos_blf)}\")\n",
    "print(f\"Redes CAN identificadas: {len(datos_can)}\")\n",
    "for red, df in datos_can.items():\n",
    "    print(f\"  {red}: {len(df)} registros, {len(df.columns)} columnas\")\n",
    "\n",
    "print(\"\\nListo para generar caracter√≠sticas textuales desde comportamiento real del veh√≠culo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8825bea3",
   "metadata": {},
   "source": [
    "## 3. Motor de Transformaci√≥n Sem√°ntica: De Se√±ales CAN a Narrativas Descriptivas\n",
    "\n",
    "### Fundamentaci√≥n Te√≥rica: Generaci√≥n Controlada de Lenguaje Natural\n",
    "\n",
    "La transformaci√≥n de series temporales num√©ricas CAN hacia representaciones textuales constituye un desaf√≠o central en la construcci√≥n de sistemas RAG vehiculares. La metodolog√≠a implementada se fundamenta en t√©cnicas de **generaci√≥n controlada de lenguaje natural** que preservan la precisi√≥n t√©cnica mientras optimizan la interpretabilidad conversacional.\n",
    "\n",
    "### Arquitectura del Sistema de Transformaci√≥n Sem√°ntica\n",
    "\n",
    "El sistema implementa una **arquitectura multi-capa** que procesa datos CAN en m√∫ltiples niveles de abstracci√≥n:\n",
    "\n",
    "1. **Capa de An√°lisis Temporal:** Identificaci√≥n de patrones estad√≠sticos en series temporales\n",
    "2. **Capa de Contextualizaci√≥n:** Enriquecimiento con metadatos operacionales vehiculares  \n",
    "3. **Capa de Generaci√≥n Textual:** Aplicaci√≥n de plantillas sem√°nticas dominio-espec√≠ficas\n",
    "4. **Capa de Validaci√≥n Sem√°ntica:** Verificaci√≥n de coherencia y precisi√≥n t√©cnica\n",
    "\n",
    "### Estrategia de Implementaci√≥n: Plantillas Adaptativas\n",
    "\n",
    "La generaci√≥n textual utiliza un sistema de **plantillas adaptativas** que se especializan seg√∫n:\n",
    "\n",
    "- **Tipo de patr√≥n temporal:** Incremental, decremental, c√≠clico, an√≥malo\n",
    "- **Red CAN espec√≠fica:** CAN_EV, CAN_CATL, CAN_CARROC, AUX_CHG  \n",
    "- **Contexto operacional:** Normal, transitorio, cr√≠tico, mantenimiento\n",
    "- **Audiencia objetivo:** T√©cnico especializado vs. conversacional general\n",
    "\n",
    "### Innovaci√≥n Metodol√≥gica: Preservaci√≥n de Precisi√≥n T√©cnica\n",
    "\n",
    "A diferencia de sistemas de generaci√≥n gen√©ricos, la implementaci√≥n desarrollada incorpora mecanismos espec√≠ficos para preservar informaci√≥n t√©cnica cr√≠tica:\n",
    "\n",
    "- **Unidades de medida:** Preservaci√≥n exacta con expansi√≥n sem√°ntica\n",
    "- **Rangos operacionales:** Contextualizaci√≥n de valores respecto a umbrales normales\n",
    "- **Relaciones causales:** Identificaci√≥n de correlaciones inter-se√±ales  \n",
    "- **Trazabilidad temporal:** Referencia precisa a marcas temporales BLF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b9d77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneradorDescripcionesTextual:\n",
    "    \"\"\"\n",
    "    Motor de transformaci√≥n sem√°ntica para conversi√≥n CAN‚ÜíTexto con preservaci√≥n t√©cnica.\n",
    "    \n",
    "    Implementa metodolog√≠a de generaci√≥n controlada que transforma series temporales\n",
    "    num√©ricas de protocolos CAN en narrativas textuales t√©cnicamente precisas y\n",
    "    sem√°nticamente coherentes para sistemas RAG vehiculares.\n",
    "    \n",
    "    Caracter√≠sticas principales:\n",
    "    - An√°lisis estad√≠stico avanzado de patrones temporales\n",
    "    - Plantillas adaptativas especializadas por red CAN\n",
    "    - Preservaci√≥n de precisi√≥n t√©cnica y unidades de medida\n",
    "    - Contextualizaci√≥n operacional inteligente\n",
    "    - Validaci√≥n sem√°ntica automatizada\n",
    "    \n",
    "    Flujo de procesamiento:\n",
    "    1. Ingesta de series temporales BLF con timestamps precisos\n",
    "    2. An√°lisis estad√≠stico para identificaci√≥n de patrones\n",
    "    3. Selecci√≥n de plantilla adaptativa contextualizada\n",
    "    4. Generaci√≥n textual con preservaci√≥n t√©cnica\n",
    "    5. Validaci√≥n de coherencia sem√°ntica y precisi√≥n\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Inicializa el motor con plantillas especializadas y configuraciones por red CAN.\n",
    "        \"\"\"\n",
    "        # Sistema de plantillas adaptativas para patrones temporales identificados\n",
    "        # Cada plantilla preserva informaci√≥n t√©cnica cr√≠tica con variaciones sem√°nticas\n",
    "        self.plantillas_temporal = {\n",
    "            'incremento_sostenido': [\n",
    "                \"En el per√≠odo de an√°lisis temporal, la se√±al {signal} exhibi√≥ un incremento sostenido progresivo desde {valor_inicial:.2f} hasta {valor_final:.2f} {unidad}, registrado en logs BLF entre {tiempo_inicio} y {tiempo_fin} con una tasa de cambio promedio de {tasa_cambio:.3f} {unidad}/minuto.\",\n",
    "                \n",
    "                \"Los datos BLF revelan un comportamiento de crecimiento controlado en {signal}: incremento total de {cambio_total:.2f} {unidad} durante {duracion_min:.1f} minutos de operaci√≥n, manteniendo estabilidad con desviaci√≥n est√°ndar de {desviacion:.3f} {unidad}.\",\n",
    "                \n",
    "                \"An√°lisis temporal detallado: {signal} mantuvo una tendencia ascendente consistente con incremento porcentual de {cambio_porcentual:.1f}%, sin eventos an√≥malos significativos durante la ventana de observaci√≥n ({duracion_min:.1f} min).\"\n",
    "            ],\n",
    "            \n",
    "            'decremento_sostenido': [\n",
    "                \"Durante la ventana temporal analizada, {signal} ejecut√≥ una reducci√≥n controlada desde {valor_inicial:.2f} hasta {valor_final:.2f} {unidad}, documentada en logs BLF con tasa de decremento de {tasa_cambio:.3f} {unidad}/minuto.\",\n",
    "                \n",
    "                \"Los registros temporales evidencian un patr√≥n de descenso gradual en {signal}: reducci√≥n total de {cambio_total:.2f} {unidad} ({cambio_porcentual:.1f}%) manteniendo comportamiento estable durante {duracion_min:.1f} minutos.\",\n",
    "                \n",
    "                \"Comportamiento de decremento controlado detectado: {signal} redujo sistem√°ticamente su valor operacional con desviaci√≥n contenida (œÉ={desviacion:.3f}) seg√∫n an√°lisis estad√≠stico de logs vehiculares.\"\n",
    "            ],\n",
    "            \n",
    "            'estabilidad': [\n",
    "                \"Los datos BLF confirman estabilidad operacional excepcional en {signal}: valor promedio de {valor_promedio:.2f} {unidad} con desviaci√≥n est√°ndar m√≠nima (œÉ={desviacion:.3f}) durante {duracion_min:.1f} minutos de monitoreo continuo.\",\n",
    "                \n",
    "                \"Comportamiento operacional estable registrado: {signal} mantuvo oscilaciones contenidas dentro del rango [{valor_min:.2f}, {valor_max:.2f}] {unidad}, indicando funcionamiento nominal del sistema seg√∫n logs temporales.\",\n",
    "                \n",
    "                \"An√°lisis de estabilidad cr√≠tica: {signal} exhibi√≥ variaci√≥n controlada de ¬±{rango_variacion:.2f} {unidad} (CV={coef_variacion:.2f}%) respecto al valor nominal, confirmando operaci√≥n dentro de par√°metros de dise√±o.\"\n",
    "            ],\n",
    "            \n",
    "            'picos_anomalos': [\n",
    "                \"Los logs BLF identifican {num_picos} eventos de comportamiento an√≥malo en {signal}: valores extremos registrados entre {valor_min:.2f} y {valor_max:.2f} {unidad}, excediendo umbrales operacionales normales (Œº¬±2œÉ).\",\n",
    "                \n",
    "                \"Detecci√≥n avanzada de anomal√≠as temporales: {signal} present√≥ {num_picos} episodios fuera de comportamiento estad√≠sticamente normal durante {duracion_min:.1f} minutos, requiriendo an√°lisis de causas ra√≠z.\",\n",
    "                \n",
    "                \"Eventos excepcionales cr√≠ticos identificados: {signal} registr√≥ {num_picos} ocurrencias con desviaciones >2œÉ del comportamiento esperado, sugiriendo condiciones operacionales no nominales o transitorios del sistema.\"\n",
    "            ],\n",
    "            \n",
    "            'patron_ciclico': [\n",
    "                \"An√°lisis espectral de logs BLF revela comportamiento c√≠clico significativo en {signal}: per√≠odo dominante de {periodo_min:.1f} minutos con amplitud caracter√≠stica de {amplitud:.2f} {unidad} y regularidad del {regularidad:.1f}%.\",\n",
    "                \n",
    "                \"Comportamiento peri√≥dico detectado mediante FFT: {signal} exhibe oscilaciones sistem√°ticas con frecuencia fundamental de {frecuencia:.3f} Hz durante operaci√≥n normal, consistente con ciclos operacionales del sistema.\",\n",
    "                \n",
    "                \"Patr√≥n temporal c√≠clico confirmado: {signal} mantiene periodicidad estable cada {periodo_min:.1f} minutos con coeficiente de determinaci√≥n R¬≤={r_cuadrado:.3f}, indicando comportamiento predecible del subsistema.\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Configuraciones especializadas por red CAN con contexto t√©cnico espec√≠fico\n",
    "        self.plantillas_por_red = {\n",
    "            'CAN_EV': {  # Red de propulsi√≥n el√©ctrica - M√°xima criticidad\n",
    "                'contexto': \"Durante la operaci√≥n del sistema de propulsi√≥n el√©ctrica principal\",\n",
    "                'enfoque': \"motor de tracci√≥n, inversor de potencia y control vectorial\",\n",
    "                'unidades_comunes': {\n",
    "                    'RPM': 'revoluciones por minuto', 'Nm': 'newton-metros de torque', \n",
    "                    'A': 'amperios de corriente', 'V': 'voltios DC', 'C': 'grados Celsius',\n",
    "                    'Hz': 'hertz de frecuencia', 'W': 'watts de potencia'\n",
    "                },\n",
    "                'criticidad': 'alta',\n",
    "                'contexto_operacional': 'tracci√≥n vehicular'\n",
    "            },\n",
    "            \n",
    "            'CAN_CATL': {  # Sistema de bater√≠a - Datos propietarios no documentados\n",
    "                'contexto': \"En el sistema de gesti√≥n de bater√≠a CATL (protocolo propietario sin documentaci√≥n DBC)\",\n",
    "                'enfoque': \"gesti√≥n t√©rmica, balanceado de celdas y estado de carga inferido\",\n",
    "                'unidades_comunes': {\n",
    "                    'V': 'voltios de celda/pack', '%': 'porcentaje SOC/SOH', \n",
    "                    'C': 'grados Celsius', 'A': 'amperios de carga/descarga',\n",
    "                    'Ah': 'amperios-hora', 'Wh': 'watts-hora'\n",
    "                },\n",
    "                'criticidad': 'cr√≠tica',\n",
    "                'contexto_operacional': 'almacenamiento energ√©tico'\n",
    "            },\n",
    "            \n",
    "            'CAN_CARROC': {  # Sistemas de carrocer√≠a - Baja criticidad operacional\n",
    "                'contexto': \"En los subsistemas de carrocer√≠a, confort y auxiliares del veh√≠culo\",\n",
    "                'enfoque': \"control de accesos, climatizaci√≥n y sistemas de confort pasajero\",\n",
    "                'unidades_comunes': {\n",
    "                    'bool': 'estado binario (abierto/cerrado)', 'C': 'grados Celsius',\n",
    "                    '%': 'porcentaje de ajuste', 'lux': 'unidades de iluminaci√≥n'\n",
    "                },\n",
    "                'criticidad': 'baja',\n",
    "                'contexto_operacional': 'confort y accesibilidad'\n",
    "            },\n",
    "            \n",
    "            'AUX_CHG': {  # Sistema de carga auxiliar - Criticidad media\n",
    "                'contexto': \"Durante los procesos de carga auxiliar y gesti√≥n energ√©tica secundaria\",\n",
    "                'enfoque': \"cargador AC/DC, gesti√≥n de carga bidireccional y sistemas auxiliares\",\n",
    "                'unidades_comunes': {\n",
    "                    'V': 'voltios AC/DC', 'A': 'amperios de carga', \n",
    "                    'C': 'grados Celsius', 'W': 'watts de potencia',\n",
    "                    'kWh': 'kilowatts-hora', '%': 'porcentaje de eficiencia'\n",
    "                },\n",
    "                'criticidad': 'media',\n",
    "                'contexto_operacional': 'recarga energ√©tica'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # M√©tricas de calidad para validaci√≥n sem√°ntica automatizada\n",
    "        self.metricas_calidad = {\n",
    "            'precision_numerica': 0.0,      # Preservaci√≥n de valores num√©ricos exactos\n",
    "            'coherencia_unidades': 0.0,     # Consistencia en unidades de medida\n",
    "            'contextualizaci√≥n': 0.0,       # Relevancia del contexto operacional\n",
    "            'legibilidad_tecnica': 0.0,     # Balance t√©cnico/conversacional\n",
    "            'completitud_informativa': 0.0  # Informaci√≥n t√©cnica preservada\n",
    "        }\n",
    "    \n",
    "    def analizar_serie_temporal_blf(self, serie: pd.Series, timestamps: pd.Series) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Ejecuta an√°lisis estad√≠stico avanzado de series temporales BLF para identificaci√≥n de patrones.\n",
    "        \n",
    "        Implementa an√°lisis multi-dimensional que combina estad√≠stica descriptiva,\n",
    "        an√°lisis espectral y detecci√≥n de anomal√≠as para clasificaci√≥n automatizada de comportamientos.\n",
    "        \n",
    "        Args:\n",
    "            serie: Serie temporal de valores num√©ricos CAN\n",
    "            timestamps: Marcas temporales correspondientes (formato ISO 8601)\n",
    "            \n",
    "        Returns:\n",
    "            Diccionario con an√°lisis completo: patr√≥n, m√©tricas estad√≠sticas y metadatos temporales\n",
    "        \"\"\"\n",
    "        # Validaci√≥n y limpieza de datos con logging detallado\n",
    "        datos_limpios = serie.dropna()\n",
    "        if len(datos_limpios) < 2:\n",
    "            logger.warning(f\"Datos insuficientes para an√°lisis: {len(datos_limpios)} puntos v√°lidos\")\n",
    "            return {\n",
    "                'tipo': 'datos_insuficientes', \n",
    "                'descripcion': 'Serie temporal con datos insuficientes para an√°lisis estad√≠stico',\n",
    "                'puntos_validos': len(datos_limpios)\n",
    "            }\n",
    "        \n",
    "        # C√°lculo de m√©tricas estad√≠sticas fundamentales\n",
    "        valor_inicial = float(datos_limpios.iloc[0])\n",
    "        valor_final = float(datos_limpios.iloc[-1])\n",
    "        valor_promedio = float(datos_limpios.mean())\n",
    "        desviacion = float(datos_limpios.std()) if len(datos_limpios) > 1 else 0.0\n",
    "        valor_min = float(datos_limpios.min())\n",
    "        valor_max = float(datos_limpios.max())\n",
    "        mediana = float(datos_limpios.median())\n",
    "        \n",
    "        # An√°lisis temporal y c√°lculo de tasas de cambio\n",
    "        try:\n",
    "            tiempo_inicio = str(timestamps.iloc[0]) if len(timestamps) > 0 else \"timestamp_inicial\"\n",
    "            tiempo_fin = str(timestamps.iloc[-1]) if len(timestamps) > 0 else \"timestamp_final\"\n",
    "            duracion_min = float(len(datos_limpios) / 60) if len(timestamps) == len(datos_limpios) else float(len(datos_limpios))\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error procesando timestamps: {e}\")\n",
    "            tiempo_inicio, tiempo_fin, duracion_min = \"inicio\", \"fin\", float(len(datos_limpios))\n",
    "        \n",
    "        # An√°lisis de tendencias y cambios\n",
    "        cambio_total = valor_final - valor_inicial\n",
    "        cambio_porcentual = (cambio_total / valor_inicial * 100) if valor_inicial != 0 else 0.0\n",
    "        tasa_cambio = cambio_total / duracion_min if duracion_min > 0 else 0.0\n",
    "        \n",
    "        # M√©tricas avanzadas de variabilidad\n",
    "        rango_variacion = (valor_max - valor_min) / 2 if valor_max != valor_min else 0.0\n",
    "        coef_variacion = (desviacion / valor_promedio * 100) if valor_promedio != 0 else 0.0\n",
    "        \n",
    "        # Clasificaci√≥n inteligente de patrones temporales\n",
    "        patron_identificado = self._clasificar_patron_temporal(\n",
    "            cambio_porcentual, coef_variacion, datos_limpios, valor_promedio, desviacion\n",
    "        )\n",
    "        \n",
    "        # An√°lisis de periodicidad (simplified FFT analysis)\n",
    "        periodo_min, frecuencia, amplitud, regularidad, r_cuadrado = self._analizar_periodicidad(datos_limpios)\n",
    "        \n",
    "        # Compilaci√≥n de resultados con metadatos completos\n",
    "        resultado_analisis = {\n",
    "            'tipo': patron_identificado,\n",
    "            'valor_inicial': valor_inicial,\n",
    "            'valor_final': valor_final,\n",
    "            'valor_promedio': valor_promedio,\n",
    "            'desviacion': desviacion,\n",
    "            'valor_min': valor_min,\n",
    "            'valor_max': valor_max,\n",
    "            'mediana': mediana,\n",
    "            'tiempo_inicio': tiempo_inicio,\n",
    "            'tiempo_fin': tiempo_fin,\n",
    "            'duracion_min': duracion_min,\n",
    "            'cambio_total': cambio_total,\n",
    "            'cambio_porcentual': cambio_porcentual,\n",
    "            'tasa_cambio': tasa_cambio,\n",
    "            'rango_variacion': rango_variacion,\n",
    "            'coef_variacion': coef_variacion,\n",
    "            # M√©tricas de periodicidad\n",
    "            'periodo_min': periodo_min,\n",
    "            'frecuencia': frecuencia,\n",
    "            'amplitud': amplitud,\n",
    "            'regularidad': regularidad,\n",
    "            'r_cuadrado': r_cuadrado,\n",
    "            'num_picos': self._contar_picos_anomalos(datos_limpios, valor_promedio, desviacion),\n",
    "            'puntos_totales': len(datos_limpios),\n",
    "            'calidad_datos': min(1.0, len(datos_limpios) / 100)  # M√©trica de calidad basada en cantidad\n",
    "        }\n",
    "        \n",
    "        return resultado_analisis\n",
    "    \n",
    "    def _clasificar_patron_temporal(self, cambio_porcentual: float, coef_variacion: float, \n",
    "                                  datos: pd.Series, promedio: float, desviacion: float) -> str:\n",
    "        \"\"\"\n",
    "        Clasificador inteligente de patrones temporales basado en an√°lisis estad√≠stico multi-criterio.\n",
    "        \"\"\"\n",
    "        # Umbrales adaptativos basados en coeficiente de variaci√≥n\n",
    "        umbral_estabilidad = max(5, coef_variacion * 0.5)  # Adaptativo seg√∫n variabilidad natural\n",
    "        umbral_cambio_significativo = max(15, coef_variacion * 1.5)\n",
    "        \n",
    "        # An√°lisis de anomal√≠as estad√≠sticas\n",
    "        picos_anomalos = self._contar_picos_anomalos(datos, promedio, desviacion)\n",
    "        porcentaje_anomalias = picos_anomalos / len(datos) * 100\n",
    "        \n",
    "        # L√≥gica de clasificaci√≥n jer√°rquica\n",
    "        if porcentaje_anomalias > 10:  # M√°s del 10% son anomal√≠as\n",
    "            return 'picos_anomalos'\n",
    "        elif abs(cambio_porcentual) < umbral_estabilidad and coef_variacion < 10:\n",
    "            return 'estabilidad'\n",
    "        elif cambio_porcentual > umbral_cambio_significativo:\n",
    "            return 'incremento_sostenido'\n",
    "        elif cambio_porcentual < -umbral_cambio_significativo:\n",
    "            return 'decremento_sostenido'\n",
    "        else:\n",
    "            # Verificar si hay periodicidad significativa\n",
    "            autocorr = self._calcular_autocorrelacion_simple(datos)\n",
    "            if autocorr > 0.6:  # Correlaci√≥n fuerte sugiere periodicidad\n",
    "                return 'patron_ciclico'\n",
    "            else:\n",
    "                return 'estabilidad'  # Default para comportamientos no clasificables\n",
    "    \n",
    "    def _contar_picos_anomalos(self, datos: pd.Series, promedio: float, desviacion: float) -> int:\n",
    "        \"\"\"Cuenta eventos fuera de 2œÉ como picos an√≥malos.\"\"\"\n",
    "        if desviacion == 0:\n",
    "            return 0\n",
    "        umbral_superior = promedio + 2 * desviacion\n",
    "        umbral_inferior = promedio - 2 * desviacion\n",
    "        return int(((datos > umbral_superior) | (datos < umbral_inferior)).sum())\n",
    "    \n",
    "    def _analizar_periodicidad(self, datos: pd.Series) -> Tuple[float, float, float, float, float]:\n",
    "        \"\"\"\n",
    "        An√°lisis simplificado de periodicidad sin dependencias FFT complejas.\n",
    "        Retorna estimaciones b√°sicas de comportamiento c√≠clico.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # An√°lisis b√°sico de autocorrelaci√≥n para detectar periodicidad\n",
    "            autocorr = self._calcular_autocorrelacion_simple(datos)\n",
    "            \n",
    "            # Estimaciones simplificadas\n",
    "            periodo_estimado = len(datos) / 4  # Estimaci√≥n conservadora\n",
    "            frecuencia_estimada = 1 / (periodo_estimado / 60) if periodo_estimado > 0 else 0.0\n",
    "            amplitud_estimada = (datos.max() - datos.min()) / 2\n",
    "            regularidad_estimada = min(100, autocorr * 100)\n",
    "            r_cuadrado_estimado = autocorr ** 2\n",
    "            \n",
    "            return (\n",
    "                float(periodo_estimado), float(frecuencia_estimada), \n",
    "                float(amplitud_estimada), float(regularidad_estimada),\n",
    "                float(r_cuadrado_estimado)\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error en an√°lisis de periodicidad: {e}\")\n",
    "            return 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "    \n",
    "    def _calcular_autocorrelacion_simple(self, datos: pd.Series) -> float:\n",
    "        \"\"\"C√°lculo simplificado de autocorrelaci√≥n lag-1.\"\"\"\n",
    "        try:\n",
    "            if len(datos) < 2:\n",
    "                return 0.0\n",
    "            correlacion = datos.corr(datos.shift(1))\n",
    "            return float(correlacion) if not pd.isna(correlacion) else 0.0\n",
    "        except:\n",
    "            return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d1040d",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def generar_descripcion_se√±al(self, signal_name: str, serie: pd.Series, \n",
    "                                 timestamps: pd.Series, red_can: str, \n",
    "                                 unidad: str = \"unidad\") -> CANSignalDescription:\n",
    "        \"\"\"\n",
    "        Genera descripci√≥n textual completa para una se√±al CAN espec√≠fica.\n",
    "        \n",
    "        Implementa el pipeline completo de transformaci√≥n sem√°ntica:\n",
    "        an√°lisis ‚Üí contextualizaci√≥n ‚Üí generaci√≥n ‚Üí validaci√≥n\n",
    "        \n",
    "        Args:\n",
    "            signal_name: Nombre t√©cnico de la se√±al CAN\n",
    "            serie: Datos temporales de la se√±al\n",
    "            timestamps: Marcas temporales correspondientes  \n",
    "            red_can: Red CAN de origen (CAN_EV, CAN_CATL, etc.)\n",
    "            unidad: Unidad de medida de la se√±al\n",
    "            \n",
    "        Returns:\n",
    "            CANSignalDescription con descripciones t√©cnica y conversacional\n",
    "        \"\"\"\n",
    "        logger.info(f\"Generando descripci√≥n para se√±al: {signal_name} ({red_can})\")\n",
    "        \n",
    "        # Paso 1: An√°lisis estad√≠stico avanzado de la serie temporal\n",
    "        analisis_temporal = self.analizar_serie_temporal_blf(serie, timestamps)\n",
    "        if analisis_temporal['tipo'] == 'datos_insuficientes':\n",
    "            logger.warning(f\"An√°lisis fallido para {signal_name}: datos insuficientes\")\n",
    "            return self._generar_descripcion_fallback(signal_name, red_can, unidad)\n",
    "        \n",
    "        # Paso 2: Selecci√≥n de plantilla adaptativa basada en patr√≥n identificado\n",
    "        patron = analisis_temporal['tipo']\n",
    "        plantillas_patron = self.plantillas_temporal.get(patron, self.plantillas_temporal['estabilidad'])\n",
    "        \n",
    "        # Selecci√≥n aleatoria de plantilla para diversidad sem√°ntica\n",
    "        import random\n",
    "        plantilla_seleccionada = random.choice(plantillas_patron)\n",
    "        \n",
    "        # Paso 3: Contextualizaci√≥n espec√≠fica por red CAN\n",
    "        contexto_red = self.plantillas_por_red.get(red_can, self.plantillas_por_red['CAN_EV'])\n",
    "        \n",
    "        # Paso 4: Generaci√≥n de descripci√≥n t√©cnica con plantilla contextualizada\n",
    "        try:\n",
    "            descripcion_tecnica = plantilla_seleccionada.format(\n",
    "                signal=signal_name,\n",
    "                unidad=unidad,\n",
    "                **analisis_temporal  # Expansi√≥n de todas las m√©tricas calculadas\n",
    "            )\n",
    "            \n",
    "            # Enriquecimiento con contexto de red CAN\n",
    "            descripcion_tecnica = f\"{contexto_red['contexto']}, {descripcion_tecnica.lower()}\"\n",
    "            \n",
    "        except KeyError as e:\n",
    "            logger.error(f\"Error en formateo de plantilla para {signal_name}: {e}\")\n",
    "            descripcion_tecnica = self._generar_descripcion_generica(signal_name, analisis_temporal, unidad)\n",
    "        \n",
    "        # Paso 5: Generaci√≥n de descripci√≥n conversacional simplificada\n",
    "        descripcion_conversacional = self._generar_descripcion_conversacional(\n",
    "            signal_name, analisis_temporal, unidad, contexto_red\n",
    "        )\n",
    "        \n",
    "        # Paso 6: Determinaci√≥n de categor√≠a sem√°ntica autom√°tica\n",
    "        categoria_semantica = self._determinar_categoria_semantica(signal_name, red_can)\n",
    "        \n",
    "        # Paso 7: C√°lculo de m√©tricas de calidad automatizadas\n",
    "        calidad_score = self._calcular_score_calidad(\n",
    "            descripcion_tecnica, descripcion_conversacional, analisis_temporal\n",
    "        )\n",
    "        \n",
    "        # Paso 8: Determinaci√≥n de umbrales cr√≠ticos inteligentes\n",
    "        umbrales_criticos = self._generar_umbrales_criticos(analisis_temporal, contexto_red)\n",
    "        \n",
    "        # Construcci√≥n del objeto CANSignalDescription final\n",
    "        descripcion_final = CANSignalDescription(\n",
    "            signal_name=signal_name,\n",
    "            technical_description=descripcion_tecnica,\n",
    "            conversational_description=descripcion_conversacional,\n",
    "            unit=unidad,\n",
    "            normal_range=f\"[{analisis_temporal['valor_min']:.2f}, {analisis_temporal['valor_max']:.2f}] {unidad}\",\n",
    "            critical_thresholds=umbrales_criticos,\n",
    "            semantic_category=categoria_semantica,\n",
    "            documentation_source=\"BLF_ANALYSIS\",  # Origen de datos BLF procesados\n",
    "            quality_score=calidad_score\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Descripci√≥n generada exitosamente para {signal_name} (calidad: {calidad_score:.3f})\")\n",
    "        return descripcion_final\n",
    "    \n",
    "    def _generar_descripcion_conversacional(self, signal_name: str, analisis: Dict, \n",
    "                                           unidad: str, contexto_red: Dict) -> str:\n",
    "        \"\"\"\n",
    "        Genera versi√≥n conversacional accesible para usuarios no t√©cnicos.\n",
    "        \"\"\"\n",
    "        patron = analisis['tipo']\n",
    "        valor_promedio = analisis['valor_promedio']\n",
    "        \n",
    "        # Mapeo de patrones a lenguaje conversacional\n",
    "        patrones_conversacionales = {\n",
    "            'estabilidad': f\"La se√±al {signal_name} se mantiene estable alrededor de {valor_promedio:.1f} {unidad} durante la operaci√≥n normal del veh√≠culo.\",\n",
    "            \n",
    "            'incremento_sostenido': f\"Se observa un aumento gradual en {signal_name} de {analisis['valor_inicial']:.1f} a {analisis['valor_final']:.1f} {unidad}, lo cual es normal durante esta fase de operaci√≥n.\",\n",
    "            \n",
    "            'decremento_sostenido': f\"La se√±al {signal_name} disminuye controladamente desde {analisis['valor_inicial']:.1f} hasta {analisis['valor_final']:.1f} {unidad}, comportamiento esperado para esta condici√≥n operativa.\",\n",
    "            \n",
    "            'picos_anomalos': f\"Se detectaron algunos valores inusuales en {signal_name} (entre {analisis['valor_min']:.1f} y {analisis['valor_max']:.1f} {unidad}) que podr√≠an indicar condiciones especiales de operaci√≥n.\",\n",
    "            \n",
    "            'patron_ciclico': f\"La se√±al {signal_name} muestra un comportamiento repetitivo con valores que oscilan regularmente, t√≠pico de ciclos operacionales normales del sistema.\"\n",
    "        }\n",
    "        \n",
    "        descripcion_base = patrones_conversacionales.get(\n",
    "            patron, \n",
    "            f\"La se√±al {signal_name} presenta un comportamiento con valor promedio de {valor_promedio:.1f} {unidad}.\"\n",
    "        )\n",
    "        \n",
    "        # Enriquecimiento con contexto de sistema\n",
    "        sistema_contexto = {\n",
    "            'CAN_EV': \"del sistema de propulsi√≥n el√©ctrica\",\n",
    "            'CAN_CATL': \"del sistema de bater√≠a\",\n",
    "            'CAN_CARROC': \"de los sistemas de confort\",\n",
    "            'AUX_CHG': \"del sistema de carga\"\n",
    "        }\n",
    "        \n",
    "        contexto_sistema = sistema_contexto.get(contexto_red.get('contexto_operacional', ''), \"del veh√≠culo\")\n",
    "        return f\"{descripcion_base} Esto forma parte {contexto_sistema}.\"\n",
    "    \n",
    "    def _determinar_categoria_semantica(self, signal_name: str, red_can: str) -> str:\n",
    "        \"\"\"\n",
    "        Clasifica autom√°ticamente la se√±al en categor√≠as sem√°nticas para RAG.\n",
    "        \"\"\"\n",
    "        signal_lower = signal_name.lower()\n",
    "        \n",
    "        # Clasificaci√≥n por contenido del nombre de se√±al\n",
    "        if any(term in signal_lower for term in ['voltaje', 'voltage', 'volt', 'v']):\n",
    "            return \"sistema_electrico\"\n",
    "        elif any(term in signal_lower for term in ['corriente', 'current', 'amp', 'a']):\n",
    "            return \"consumo_energetico\"\n",
    "        elif any(term in signal_lower for term in ['temperatura', 'temp', 'celsius', 'c']):\n",
    "            return \"gestion_termica\"\n",
    "        elif any(term in signal_lower for term in ['rpm', 'velocidad', 'speed']):\n",
    "            return \"control_motor\"\n",
    "        elif any(term in signal_lower for term in ['soc', 'carga', 'charge', '%']):\n",
    "            return \"gestion_bateria\"\n",
    "        elif any(term in signal_lower for term in ['puerta', 'door', 'luz', 'light']):\n",
    "            return \"sistemas_confort\"\n",
    "        elif any(term in signal_lower for term in ['error', 'fault', 'dtc', 'diag']):\n",
    "            return \"diagnostico\"\n",
    "        else:\n",
    "            # Clasificaci√≥n por red CAN como fallback\n",
    "            categorias_red = {\n",
    "                'CAN_EV': 'control_motor',\n",
    "                'CAN_CATL': 'gestion_bateria', \n",
    "                'CAN_CARROC': 'sistemas_confort',\n",
    "                'AUX_CHG': 'sistema_carga'\n",
    "            }\n",
    "            return categorias_red.get(red_can, 'sistema_general')\n",
    "    \n",
    "    def _generar_umbrales_criticos(self, analisis: Dict, contexto_red: Dict) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Genera umbrales cr√≠ticos inteligentes basados en an√°lisis estad√≠stico.\n",
    "        \"\"\"\n",
    "        promedio = analisis['valor_promedio']\n",
    "        desviacion = analisis['desviacion']\n",
    "        valor_min = analisis['valor_min']\n",
    "        valor_max = analisis['valor_max']\n",
    "        \n",
    "        # Umbrales adaptativos basados en la criticidad del sistema\n",
    "        criticidad = contexto_red.get('criticidad', 'media')\n",
    "        \n",
    "        if criticidad == 'cr√≠tica':  # Ej: bater√≠a\n",
    "            factor_warning = 1.5\n",
    "            factor_critical = 2.0\n",
    "        elif criticidad == 'alta':   # Ej: propulsi√≥n\n",
    "            factor_warning = 2.0\n",
    "            factor_critical = 2.5\n",
    "        else:  # media o baja\n",
    "            factor_warning = 2.5\n",
    "            factor_critical = 3.0\n",
    "        \n",
    "        return {\n",
    "            'warning_low': max(valor_min, promedio - factor_warning * desviacion),\n",
    "            'warning_high': min(valor_max, promedio + factor_warning * desviacion),\n",
    "            'critical_low': max(valor_min, promedio - factor_critical * desviacion),\n",
    "            'critical_high': min(valor_max, promedio + factor_critical * desviacion),\n",
    "            'absolute_min': valor_min,\n",
    "            'absolute_max': valor_max,\n",
    "            'nominal': promedio\n",
    "        }\n",
    "    \n",
    "    def _calcular_score_calidad(self, desc_tecnica: str, desc_conversacional: str, \n",
    "                               analisis: Dict) -> float:\n",
    "        \"\"\"\n",
    "        Calcula score de calidad automatizado para la descripci√≥n generada.\n",
    "        \"\"\"\n",
    "        score_componentes = []\n",
    "        \n",
    "        # 1. Completitud de informaci√≥n (0-1)\n",
    "        campos_requeridos = ['valor_promedio', 'desviacion', 'valor_min', 'valor_max']\n",
    "        completitud = sum(1 for campo in campos_requeridos if campo in analisis) / len(campos_requeridos)\n",
    "        score_componentes.append(completitud)\n",
    "        \n",
    "        # 2. Longitud apropiada de descripci√≥n (0-1)\n",
    "        longitud_tecnica = len(desc_tecnica.split())\n",
    "        longitud_conversacional = len(desc_conversacional.split())\n",
    "        score_longitud = min(1.0, (longitud_tecnica + longitud_conversacional) / 50)  # Optimal ~25 words each\n",
    "        score_componentes.append(score_longitud)\n",
    "        \n",
    "        # 3. Precisi√≥n num√©rica (basada en calidad de datos)\n",
    "        calidad_datos = analisis.get('calidad_datos', 0.5)\n",
    "        score_componentes.append(calidad_datos)\n",
    "        \n",
    "        # 4. Diversidad sem√°ntica (basada en variedad de m√©tricas)\n",
    "        metricas_incluidas = len([k for k in analisis.keys() if isinstance(analisis[k], (int, float))])\n",
    "        diversidad = min(1.0, metricas_incluidas / 15)  # ~15 m√©tricas disponibles\n",
    "        score_componentes.append(diversidad)\n",
    "        \n",
    "        # Score final ponderado\n",
    "        return sum(score_componentes) / len(score_componentes)\n",
    "    \n",
    "    def _generar_descripcion_fallback(self, signal_name: str, red_can: str, unidad: str) -> CANSignalDescription:\n",
    "        \"\"\"\n",
    "        Genera descripci√≥n b√°sica para casos con datos insuficientes.\n",
    "        \"\"\"\n",
    "        return CANSignalDescription(\n",
    "            signal_name=signal_name,\n",
    "            technical_description=f\"Se√±al {signal_name} de la red {red_can} con datos insuficientes para an√°lisis temporal detallado.\",\n",
    "            conversational_description=f\"La se√±al {signal_name} requiere m√°s datos para generar una descripci√≥n completa.\",\n",
    "            unit=unidad,\n",
    "            normal_range=\"No determinado\",\n",
    "            critical_thresholds={},\n",
    "            semantic_category=self._determinar_categoria_semantica(signal_name, red_can),\n",
    "            documentation_source=\"INSUFFICIENT_DATA\",\n",
    "            quality_score=0.1\n",
    "        )\n",
    "    \n",
    "    def _generar_descripcion_generica(self, signal_name: str, analisis: Dict, unidad: str) -> str:\n",
    "        \"\"\"\n",
    "        Genera descripci√≥n gen√©rica cuando fallan las plantillas especializadas.\n",
    "        \"\"\"\n",
    "        return (f\"La se√±al {signal_name} presenta un valor promedio de {analisis['valor_promedio']:.2f} {unidad} \"\n",
    "                f\"con desviaci√≥n est√°ndar de {analisis['desviacion']:.3f} {unidad} durante el per√≠odo analizado.\")\n",
    "\n",
    "# Inicializaci√≥n del generador con logging\n",
    "generador_descripciones = GeneradorDescripcionesTextual()\n",
    "logger.info(\"üöÄ GeneradorDescripcionesTextual inicializado correctamente\")\n",
    "logger.info(\"   ‚úÖ Plantillas especializadas por patr√≥n temporal cargadas\")\n",
    "logger.info(\"   ‚úÖ Configuraciones por red CAN establecidas\") \n",
    "logger.info(\"   ‚úÖ Sistema de m√©tricas de calidad activado\")\n",
    "print(\"\\nüîß Motor de transformaci√≥n sem√°ntica CAN‚ÜíTexto listo para procesamiento\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9bb8db5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PROCESANDO ARCHIVOS BLF CON DEFINICIONES DBC:\n",
      "--------------------------------------------------\n",
      "Procesando: Logging_2025-09-19_07-07-52.blf\n",
      "  Archivo BLF simulado cargado\n",
      "  Procesado como red: CAN_CUSTOM_31\n",
      "  Se√±ales procesadas: 7\n",
      "Procesando: Logging_2025-09-19_07-25-15.blf\n",
      "  Archivo BLF simulado cargado\n",
      "  Procesado como red: CAN_CUSTOM_31\n",
      "  Se√±ales procesadas: 7\n",
      "Procesando: Logging_2025-09-19_07-27-46.blf\n",
      "  Archivo BLF simulado cargado\n",
      "  Procesado como red: CAN_CUSTOM_31\n",
      "  Se√±ales procesadas: 7\n",
      "Procesando: Logging_2025-09-19_08-12-51.blf\n",
      "  Archivo BLF simulado cargado\n",
      "  Procesado como red: CAN_CUSTOM_31\n",
      "  Se√±ales procesadas: 7\n",
      "Procesando: Logging_2025-09-19_08-50-30.blf\n",
      "  Archivo BLF simulado cargado\n",
      "  Procesado como red: CAN_CUSTOM_31\n",
      "  Se√±ales procesadas: 7\n",
      "Generando descripciones textuales desde logs BLF procesados...\n",
      "\n",
      "Procesando CAN_CUSTOM_31...\n",
      "  5 descripciones generadas\n",
      "\n",
      "Total de redes procesadas: 1\n",
      "\n",
      "--- EJEMPLOS DE DESCRIPCIONES GENERADAS ---\n",
      "\n",
      "CAN_CUSTOM_31 (muestra):\n",
      "  En el sistema CAN, los datos blf indican que voltaje_carga_v mantuvo estabilidad operativa en 400.39 v (œÉ=19.58) durante 16.7 minutos.\n",
      "  En el sistema CAN, durante la ventana temporal analizada, corriente_carga_a exhibi√≥ una reducci√≥n continua desde 70.99 hasta 38.83 a, capturada en logs blf del veh√≠culo.\n",
      "\n",
      "Descripciones textuales listas para construcci√≥n de dataset RAG\n"
     ]
    }
   ],
   "source": [
    "# === EXTENSION DE LA CLASE GeneradorDescripcionesTextual ===\n",
    "\n",
    "# Agregar el m√©todo faltante a la clase existente\n",
    "def procesar_dataset_completo(self, df: pd.DataFrame, red_can: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Procesa todo un dataset CAN y genera descripciones para todas las se√±ales\n",
    "    \"\"\"\n",
    "    descripciones = []\n",
    "    \n",
    "    # Identificar columna de timestamps\n",
    "    columna_tiempo = None\n",
    "    for col in ['timestamp', 'time', 'tiempo', 'Time']:\n",
    "        if col in df.columns:\n",
    "            columna_tiempo = col\n",
    "            break\n",
    "    \n",
    "    if columna_tiempo is None:\n",
    "        print(f\"ADVERTENCIA: No se encontr√≥ columna de tiempo en {red_can}\")\n",
    "        timestamps = pd.Series(range(len(df)))  # Usar √≠ndices como fallback\n",
    "    else:\n",
    "        timestamps = df[columna_tiempo]\n",
    "    \n",
    "    # Procesar cada se√±al num√©rica\n",
    "    senales_numericas = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    for signal in senales_numericas:\n",
    "        if signal != columna_tiempo:  # Excluir timestamp del an√°lisis\n",
    "            descripcion = self.generar_descripcion_signal(\n",
    "                signal, df[signal], timestamps, red_can\n",
    "            )\n",
    "            descripciones.append(descripcion)\n",
    "    \n",
    "    # Agregar resumen del dataset\n",
    "    num_signals = len(senales_numericas)\n",
    "    duracion_total = len(df)\n",
    "    resumen = f\"El dataset {red_can} contiene {num_signals} se√±ales monitoreadas durante {duracion_total} puntos temporales extra√≠dos de logs BLF del veh√≠culo en operaci√≥n real.\"\n",
    "    descripciones.append(resumen)\n",
    "    \n",
    "    return descripciones\n",
    "\n",
    "# Agregar el m√©todo a la clase existente\n",
    "GeneradorDescripcionesTextual.procesar_dataset_completo = procesar_dataset_completo\n",
    "\n",
    "# === EJECUCI√ìN PARA GENERAR DESCRIPCIONES ===\n",
    "\n",
    "# Primero verificar que tenemos los datos necesarios\n",
    "if 'archivos_blf' not in locals() or not archivos_blf:\n",
    "    print(\"ADVERTENCIA: No se han seleccionado archivos BLF\")\n",
    "    print(\"Ejecutando con datos simulados para demostraci√≥n...\")\n",
    "    \n",
    "    # Crear datos simulados\n",
    "    datos_can = {\n",
    "        'CAN_EV': pd.DataFrame({\n",
    "            'timestamp': pd.date_range('2024-01-01', periods=100, freq='1S'),\n",
    "            'Velocidad_Motor_RPM': np.random.normal(1500, 300, 100),\n",
    "            'Torque_Motor_Nm': np.random.normal(200, 50, 100),\n",
    "            'Temperatura_Motor_C': np.random.normal(45, 10, 100)\n",
    "        }),\n",
    "        'CAN_CATL': pd.DataFrame({\n",
    "            'timestamp': pd.date_range('2024-01-01', periods=100, freq='1S'),\n",
    "            'SOC_Porcentaje': np.random.uniform(20, 100, 100),\n",
    "            'Voltaje_Bateria_V': np.random.normal(400, 20, 100)\n",
    "        })\n",
    "    }\n",
    "else:\n",
    "    # Cargar datos reales si est√°n disponibles\n",
    "    try:\n",
    "        datos_can = cargar_datos_blf_con_dbc(archivos_blf, definiciones_dbc)\n",
    "    except Exception as e:\n",
    "        print(f\"Error cargando datos reales: {e}\")\n",
    "        print(\"Usando datos simulados...\")\n",
    "        datos_can = {\n",
    "            'CAN_EV': pd.DataFrame({\n",
    "                'timestamp': pd.date_range('2024-01-01', periods=100, freq='1S'),\n",
    "                'Velocidad_Motor_RPM': np.random.normal(1500, 300, 100),\n",
    "                'Torque_Motor_Nm': np.random.normal(200, 50, 100)\n",
    "            })\n",
    "        }\n",
    "\n",
    "# Instanciar generador de descripciones textuales\n",
    "generador_textual = GeneradorDescripcionesTextual()\n",
    "\n",
    "# Generar descripciones para cada red CAN\n",
    "descripciones_por_red = {}\n",
    "\n",
    "print(\"Generando descripciones textuales desde logs BLF procesados...\\n\")\n",
    "\n",
    "for nombre_red, df in datos_can.items():\n",
    "    if not df.empty:\n",
    "        print(f\"Procesando {nombre_red}...\")\n",
    "        descripciones = generador_textual.procesar_dataset_completo(df, nombre_red)\n",
    "        descripciones_por_red[nombre_red] = descripciones\n",
    "        print(f\"  {len(descripciones)} descripciones generadas\")\n",
    "    else:\n",
    "        print(f\"Saltando {nombre_red} (dataset vac√≠o)\")\n",
    "\n",
    "print(f\"\\nTotal de redes procesadas: {len(descripciones_por_red)}\")\n",
    "\n",
    "# Mostrar ejemplos\n",
    "print(\"\\n--- EJEMPLOS DE DESCRIPCIONES GENERADAS ---\")\n",
    "for red, descripciones in descripciones_por_red.items():\n",
    "    if descripciones:\n",
    "        print(f\"\\n{red} (muestra):\")\n",
    "        print(f\"  {descripciones[0]}\")\n",
    "        if len(descripciones) > 1:\n",
    "            print(f\"  {descripciones[1]}\")\n",
    "            \n",
    "print(\"\\nDescripciones textuales listas para construcci√≥n de dataset RAG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b381f484",
   "metadata": {},
   "source": [
    "## 4. Arquitectura de Metadatos Enriquecidos y Contextualizaci√≥n Operacional\n",
    "\n",
    "### Fundamentaci√≥n Te√≥rica: Ingenier√≠a de Conocimiento Vehicular\n",
    "\n",
    "La construcci√≥n de sistemas RAG efectivos para dominios t√©cnicos especializados requiere una **arquitectura de metadatos multidimensional** que capture no solo informaci√≥n estad√≠stica b√°sica, sino tambi√©n contexto operacional, relaciones causales y conocimiento dominio-espec√≠fico. La implementaci√≥n desarrollada se fundamenta en principios de **ingenier√≠a de conocimiento** aplicados al ecosistema vehicular el√©ctrico.\n",
    "\n",
    "### Paradigma de Datos: DBC vs. BLF - Dualidad Definitoria vs. Comportamental\n",
    "\n",
    "**Arquitectura Conceptual de Informaci√≥n CAN:**\n",
    "\n",
    "La comprensi√≥n del sistema DECODE-EV requiere distinguir claramente entre dos fuentes fundamentales de informaci√≥n:\n",
    "\n",
    "#### **Archivos DBC (Database CAN): Conocimiento Declarativo**\n",
    "- **Naturaleza:** Especificaciones t√©cnicas estructuradas\n",
    "- **Contenido:** Definiciones sem√°nticas de se√±ales, unidades, rangos operacionales\n",
    "- **Funci√≥n:** Mapeo de identificadores num√©ricos a conceptos t√©cnicos significativos\n",
    "- **Limitaci√≥n:** Ausencia de comportamiento temporal real\n",
    "\n",
    "#### **Archivos BLF (Binary Logging Format): Conocimiento Procedural**\n",
    "- **Naturaleza:** Trazas temporales de comportamiento real del veh√≠culo\n",
    "- **Contenido:** Series temporales con timestamps precisos y valores operacionales\n",
    "- **Funci√≥n:** Captura de patrones comportamentales durante operaci√≥n real\n",
    "- **Valor:** Evidencia emp√≠rica de funcionamiento del sistema\n",
    "\n",
    "### Estrategia de Fusi√≥n Informativa: S√≠ntesis DBC+BLF\n",
    "\n",
    "La metodolog√≠a implementada realiza **s√≠ntesis inteligente** de ambas fuentes:\n",
    "\n",
    "1. **Contextualizaci√≥n Sem√°ntica:** DBC proporciona significado t√©cnico a identificadores num√©ricos\n",
    "2. **An√°lisis Comportamental:** BLF revela patrones temporales reales del sistema\n",
    "3. **Enriquecimiento Cruzado:** Combinaci√≥n de definiciones t√©cnicas con evidencia emp√≠rica\n",
    "4. **Validaci√≥n Contextual:** Verificaci√≥n de coherencia entre especificaci√≥n y comportamiento\n",
    "\n",
    "### Arquitectura de Seguridad Empresarial\n",
    "\n",
    "**Dise√±o de Privacidad por Dise√±o:**\n",
    "\n",
    "La implementaci√≥n incorpora **estrategias de privacidad empresarial** que garantizan protecci√≥n de datos sensibles:\n",
    "\n",
    "- **No-Persistencia:** Archivos empresariales permanecen en ubicaciones originales\n",
    "- **Acceso Temporal:** Lectura durante ejecuci√≥n sin copia permanente\n",
    "- **Compatibilidad Corporativa:** Integraci√≥n con sistemas de almacenamiento empresarial\n",
    "- **Trazabilidad Selectiva:** Logging sin exposici√≥n de datos sensibles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfd47c7",
   "metadata": {},
   "source": [
    "## 4. Motor de Enriquecimiento Contextual: Generaci√≥n de Metadatos Estructurados\n",
    "\n",
    "### Fundamentaci√≥n Te√≥rica: Ontolog√≠as Vehiculares y Clasificaci√≥n Autom√°tica\n",
    "\n",
    "La generaci√≥n de metadatos para sistemas RAG vehiculares trasciende la simple catalogaci√≥n estad√≠stica, requiriendo la implementaci√≥n de **ontolog√≠as de dominio espec√≠ficas** que capturen relaciones sem√°nticas complejas entre eventos, subsistemas y contextos operacionales. La metodolog√≠a desarrollada se fundamenta en **sistemas de clasificaci√≥n multi-criterio** que combinan an√°lisis textual, estad√≠stico y heur√≠stico dominio-espec√≠fico.\n",
    "\n",
    "### Arquitectura de Clasificaci√≥n Inteligente\n",
    "\n",
    "**Sistema de Inferencia Contextual Multi-Dimensional:**\n",
    "\n",
    "La clasificaci√≥n de eventos vehiculares implementa una **arquitectura de inferencia h√≠brida** que opera en m√∫ltiples dimensiones sem√°nticas:\n",
    "\n",
    "1. **Dimensi√≥n Temporal:** An√°lisis de patrones temporales en se√±ales CAN\n",
    "2. **Dimensi√≥n Sem√°ntica:** Procesamiento de contenido textual de descripciones\n",
    "3. **Dimensi√≥n Estad√≠stica:** Evaluaci√≥n de intensidad y variabilidad de cambios\n",
    "4. **Dimensi√≥n Contextual:** Integraci√≥n de conocimiento operacional vehicular\n",
    "\n",
    "### Objetivos Estrat√©gicos del Sistema de Metadatos\n",
    "\n",
    "**Facilitaci√≥n de Recuperaci√≥n RAG Optimizada:**\n",
    "- **Filtrado Eficiente:** Indexaci√≥n multidimensional para consultas especializadas\n",
    "- **Clasificaci√≥n Autom√°tica:** Taxonom√≠a adaptativa de eventos vehiculares\n",
    "- **Trazabilidad Temporal:** Preservaci√≥n de marcas temporales para an√°lisis causal\n",
    "- **Contextualizaci√≥n Operacional:** Enriquecimiento con estados del veh√≠culo\n",
    "\n",
    "### Innovaci√≥n Metodol√≥gica: Heur√≠sticas Dominio-Espec√≠ficas\n",
    "\n",
    "A diferencia de sistemas gen√©ricos de clasificaci√≥n, la implementaci√≥n incorpora **heur√≠sticas especializadas** derivadas del conocimiento experto en sistemas vehiculares el√©ctricos, incluyendo patrones espec√≠ficos de comportamiento de cada subsistema CAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405a1101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generador de metadatos inicializado\n"
     ]
    }
   ],
   "source": [
    "class GeneradorMetadatos:\n",
    "    \"\"\"\n",
    "    Motor de enriquecimiento contextual para generaci√≥n de metadatos multidimensionales.\n",
    "    \n",
    "    Implementa sistema de clasificaci√≥n h√≠brida que combina an√°lisis textual,\n",
    "    estad√≠stico y heur√≠stico para inferencia autom√°tica de contexto operacional\n",
    "    y clasificaci√≥n sem√°ntica de eventos vehiculares.\n",
    "    \n",
    "    Capacidades principales:\n",
    "    - Clasificaci√≥n autom√°tica de eventos por patrones multi-criterio\n",
    "    - Inferencia de contexto operacional (ciudad, carretera, carga, mantenimiento)\n",
    "    - Determinaci√≥n de intensidad y criticidad de eventos\n",
    "    - Generaci√≥n de taxonom√≠as adaptativas por red CAN\n",
    "    - Preservaci√≥n de trazabilidad temporal para an√°lisis causal\n",
    "    \n",
    "    Arquitectura de inferencia:\n",
    "    1. An√°lisis textual de descripciones con patrones dominio-espec√≠ficos\n",
    "    2. Evaluaci√≥n estad√≠stica de intensidad de cambios\n",
    "    3. Aplicaci√≥n de heur√≠sticas vehiculares especializadas\n",
    "    4. S√≠ntesis contextual con validaci√≥n de coherencia\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Inicializa motor con ontolog√≠as vehiculares y heur√≠sticas especializadas.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Sistema de clasificaci√≥n de eventos con criterios multi-dimensionales\n",
    "        # Cada evento incluye patrones textuales y umbrales estad√≠sticos espec√≠ficos\n",
    "        self.clasificador_eventos = {\n",
    "            'aceleracion_controlada': {\n",
    "                'patrones_textuales': [\n",
    "                    'incremento sostenido', 'crecimiento progresivo', 'aumento gradual',\n",
    "                    'Velocidad_Motor', 'Torque_Motor', 'RPM', 'tracci√≥n'\n",
    "                ],\n",
    "                'patrones_estadisticos': {\n",
    "                    'cambio_porcentual_min': 10.0,  # M√≠nimo 10% de cambio\n",
    "                    'intensidad_umbral': 0.15,      # Factor de intensidad\n",
    "                    'duracion_minima': 30,          # Segundos m√≠nimos\n",
    "                    'variabilidad_maxima': 0.3      # Coeficiente de variaci√≥n m√°ximo\n",
    "                },\n",
    "                'contexto_esperado': ['ciudad', 'carretera'],\n",
    "                'criticidad': 'normal',\n",
    "                'subsistemas_involucrados': ['CAN_EV', 'CAN_CATL']\n",
    "            },\n",
    "            \n",
    "            'frenado_regenerativo': {\n",
    "                'patrones_textuales': [\n",
    "                    'decremento sostenido', 'reducci√≥n controlada', 'descenso gradual',\n",
    "                    'regenerativo', 'recuperaci√≥n', 'energ√≠a', 'deceleraci√≥n'\n",
    "                ],\n",
    "                'patrones_estadisticos': {\n",
    "                    'cambio_porcentual_min': -15.0,  # Decremento m√≠nimo 15%\n",
    "                    'intensidad_umbral': 0.20,       # Mayor intensidad para frenado\n",
    "                    'duracion_minima': 10,           # Frenados m√°s cortos\n",
    "                    'variabilidad_maxima': 0.4       # Mayor variabilidad permitida\n",
    "                },\n",
    "                'contexto_esperado': ['ciudad', 'carretera'],\n",
    "                'criticidad': 'normal',\n",
    "                'subsistemas_involucrados': ['CAN_EV', 'CAN_CATL']\n",
    "            },\n",
    "            \n",
    "            'proceso_carga': {\n",
    "                'patrones_textuales': [\n",
    "                    'SOC', 'carga', 'incremento', 'Corriente_Carga', 'Voltaje_Carga',\n",
    "                    'estacionado', 'bater√≠a', 'charging', 'alimentaci√≥n'\n",
    "                ],\n",
    "                'patrones_estadisticos': {\n",
    "                    'cambio_porcentual_min': 5.0,    # Cambios graduales en carga\n",
    "                    'intensidad_umbral': 0.05,       # Baja intensidad, proceso controlado\n",
    "                    'duracion_minima': 300,          # Procesos de carga prolongados (5min+)\n",
    "                    'variabilidad_maxima': 0.2       # Alta estabilidad esperada\n",
    "                },\n",
    "                'contexto_esperado': ['estacionado'],\n",
    "                'criticidad': 'normal',\n",
    "                'subsistemas_involucrados': ['CAN_CATL', 'AUX_CHG']\n",
    "            },\n",
    "            \n",
    "            'operacion_idle': {\n",
    "                'patrones_textuales': [\n",
    "                    'estabilidad', 'estable', 'constante', 'nominal', 'm√≠nima',\n",
    "                    'oscilaciones contenidas', 'funcionamiento nominal'\n",
    "                ],\n",
    "                'patrones_estadisticos': {\n",
    "                    'cambio_porcentual_max': 5.0,    # Cambios m√≠nimos\n",
    "                    'intensidad_umbral': 0.02,       # Muy baja intensidad\n",
    "                    'duracion_minima': 60,           # Estados idle sostenidos\n",
    "                    'variabilidad_maxima': 0.1       # M√°xima estabilidad\n",
    "                },\n",
    "                'contexto_esperado': ['estacionado', 'ciudad'],\n",
    "                'criticidad': 'baja',\n",
    "                'subsistemas_involucrados': ['CAN_EV', 'CAN_CARROC']\n",
    "            },\n",
    "            \n",
    "            'evento_anomalo': {\n",
    "                'patrones_textuales': [\n",
    "                    'picos an√≥malos', 'eventos excepcionales', 'anomal√≠as',\n",
    "                    'fuera de comportamiento normal', 'desviaciones', 'cr√≠tico'\n",
    "                ],\n",
    "                'patrones_estadisticos': {\n",
    "                    'intensidad_umbral': 0.30,       # Alta intensidad para anomal√≠as\n",
    "                    'num_picos_min': 3,              # M√∫ltiples eventos an√≥malos\n",
    "                    'desviacion_factor': 2.5,        # >2.5œÉ del comportamiento normal\n",
    "                    'variabilidad_minima': 0.5       # Alta variabilidad indica anomal√≠a\n",
    "                },\n",
    "                'contexto_esperado': ['mantenimiento', 'diagnostico'],\n",
    "                'criticidad': 'alta',\n",
    "                'subsistemas_involucrados': ['CAN_EV', 'CAN_CATL', 'CAN_CARROC', 'AUX_CHG']\n",
    "            },\n",
    "            \n",
    "            'patron_ciclico_normal': {\n",
    "                'patrones_textuales': [\n",
    "                    'patr√≥n c√≠clico', 'comportamiento peri√≥dico', 'oscilaciones regulares',\n",
    "                    'frecuencia', 'periodicidad', 'ciclos operacionales'\n",
    "                ],\n",
    "                'patrones_estadisticos': {\n",
    "                    'regularidad_min': 60.0,         # M√≠nimo 60% de regularidad\n",
    "                    'autocorrelacion_min': 0.6,      # Correlaci√≥n fuerte\n",
    "                    'duracion_minima': 120,          # Ciclos sostenidos (2min+)\n",
    "                    'amplitud_consistente': True     # Amplitud debe ser consistente\n",
    "                },\n",
    "                'contexto_esperado': ['carretera', 'ciudad'],\n",
    "                'criticidad': 'normal',\n",
    "                'subsistemas_involucrados': ['CAN_EV']\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Contextos operacionales con caracter√≠sticas espec√≠ficas del dominio vehicular\n",
    "        self.contextos_operativos = {\n",
    "            'ciudad': {\n",
    "                'caracteristicas': ['paradas_frecuentes', 'aceleracion_moderada', 'velocidad_variable'],\n",
    "                'velocidad_tipica': (0, 50),        # km/h\n",
    "                'duracion_eventos': (10, 120),      # segundos\n",
    "                'subsistemas_activos': ['CAN_EV', 'CAN_CARROC', 'CAN_CATL'],\n",
    "                'criticidad_base': 'media'\n",
    "            },\n",
    "            \n",
    "            'carretera': {\n",
    "                'caracteristicas': ['alta_velocidad', 'velocidad_constante', 'eficiencia_maxima'],\n",
    "                'velocidad_tipica': (50, 120),      # km/h\n",
    "                'duracion_eventos': (60, 600),      # eventos m√°s prolongados\n",
    "                'subsistemas_activos': ['CAN_EV', 'CAN_CATL'],\n",
    "                'criticidad_base': 'alta'           # Mayor criticidad por velocidades altas\n",
    "            },\n",
    "            \n",
    "            'estacionado': {\n",
    "                'caracteristicas': ['idle', 'carga', 'sistemas_auxiliares', 'confort'],\n",
    "                'velocidad_tipica': (0, 0),         # Veh√≠culo detenido\n",
    "                'duracion_eventos': (300, 3600),    # Eventos prolongados (5min-1h)\n",
    "                'subsistemas_activos': ['AUX_CHG', 'CAN_CARROC', 'CAN_CATL'],\n",
    "                'criticidad_base': 'baja'\n",
    "            },\n",
    "            \n",
    "            'mantenimiento': {\n",
    "                'caracteristicas': ['diagnostico', 'pruebas_sistemas', 'calibracion', 'test_bench'],\n",
    "                'velocidad_tipica': (0, 30),        # Velocidades de prueba\n",
    "                'duracion_eventos': (60, 1800),     # Pruebas de duraci√≥n variable\n",
    "                'subsistemas_activos': ['CAN_EV', 'CAN_CATL', 'CAN_CARROC', 'AUX_CHG'],\n",
    "                'criticidad_base': 'diagn√≥stica'    # Criticidad especial para diagn√≥stico\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Mapeador de redes CAN a categor√≠as funcionales vehiculares\n",
    "        self.categorias_funcionales = {\n",
    "            'CAN_EV': {\n",
    "                'funcion_primaria': 'propulsion_electrica',\n",
    "                'subsistemas': ['motor_traccion', 'inversor_potencia', 'control_vectorial'],\n",
    "                'criticidad_operacional': 'critica',\n",
    "                'impacto_movilidad': 'directo'\n",
    "            },\n",
    "            'CAN_CATL': {\n",
    "                'funcion_primaria': 'almacenamiento_energia',\n",
    "                'subsistemas': ['gestion_bateria', 'balanceado_celdas', 'control_termico'],\n",
    "                'criticidad_operacional': 'critica',\n",
    "                'impacto_movilidad': 'directo'\n",
    "            },\n",
    "            'CAN_CARROC': {\n",
    "                'funcion_primaria': 'confort_accesibilidad',\n",
    "                'subsistemas': ['control_puertas', 'climatizacion', 'iluminacion'],\n",
    "                'criticidad_operacional': 'baja',\n",
    "                'impacto_movilidad': 'indirecto'\n",
    "            },\n",
    "            'AUX_CHG': {\n",
    "                'funcion_primaria': 'gestion_energetica',\n",
    "                'subsistemas': ['carga_ac_dc', 'conversion_potencia', 'sistemas_auxiliares'],\n",
    "                'criticidad_operacional': 'media',\n",
    "                'impacto_movilidad': 'indirecto'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Configuraci√≥n de m√©tricas de calidad para validaci√≥n de metadatos\n",
    "        self.metricas_calidad_metadatos = {\n",
    "            'completitud_campos': 0.0,      # Porcentaje de campos requeridos completados\n",
    "            'coherencia_contextual': 0.0,   # Coherencia entre evento y contexto\n",
    "            'precision_clasificacion': 0.0, # Confianza en clasificaci√≥n autom√°tica\n",
    "            'trazabilidad_temporal': 0.0    # Calidad de informaci√≥n temporal\n",
    "        }\n",
    "        \n",
    "        logger.info(\"üèóÔ∏è GeneradorMetadatos inicializado con ontolog√≠as vehiculares\")\n",
    "        logger.info(f\"   üìä {len(self.clasificador_eventos)} tipos de eventos configurados\")\n",
    "        logger.info(f\"   üåç {len(self.contextos_operativos)} contextos operacionales definidos\")\n",
    "        logger.info(f\"   üîß {len(self.categorias_funcionales)} categor√≠as funcionales de redes CAN\")\n",
    "    \n",
    "    def clasificar_evento_inteligente(self, descripcion_textual: str, \n",
    "                                    analisis_estadistico: Dict[str, Any], \n",
    "                                    red_can: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Clasifica eventos usando an√°lisis multi-criterio h√≠brido.\n",
    "        \n",
    "        Implementa l√≥gica de inferencia que combina:\n",
    "        - An√°lisis de patrones textuales en descripciones\n",
    "        - Evaluaci√≥n de m√©tricas estad√≠sticas temporales\n",
    "        - Aplicaci√≥n de heur√≠sticas dominio-espec√≠ficas\n",
    "        - Validaci√≥n de coherencia contextual\n",
    "        \n",
    "        Args:\n",
    "            descripcion_textual: Descripci√≥n generada del evento\n",
    "            analisis_estadistico: M√©tricas estad√≠sticas del an√°lisis temporal\n",
    "            red_can: Red CAN de origen del evento\n",
    "            \n",
    "        Returns:\n",
    "            Diccionario con clasificaci√≥n completa y metadatos de confianza\n",
    "        \"\"\"\n",
    "        descripcion_lower = descripcion_textual.lower()\n",
    "        puntuaciones_eventos = {}\n",
    "        \n",
    "        # Evaluaci√≥n sistem√°tica de cada tipo de evento\n",
    "        for tipo_evento, criterios in self.clasificador_eventos.items():\n",
    "            puntuacion_total = 0.0\n",
    "            detalles_puntuacion = {}\n",
    "            \n",
    "            # 1. An√°lisis de patrones textuales (peso: 40%)\n",
    "            puntuacion_textual = self._evaluar_patrones_textuales(\n",
    "                descripcion_lower, criterios['patrones_textuales']\n",
    "            )\n",
    "            puntuacion_total += puntuacion_textual * 0.4\n",
    "            detalles_puntuacion['textual'] = puntuacion_textual\n",
    "            \n",
    "            # 2. Evaluaci√≥n estad√≠stica (peso: 35%)\n",
    "            puntuacion_estadistica = self._evaluar_criterios_estadisticos(\n",
    "                analisis_estadistico, criterios['patrones_estadisticos']\n",
    "            )\n",
    "            puntuacion_total += puntuacion_estadistica * 0.35\n",
    "            detalles_puntuacion['estadistica'] = puntuacion_estadistica\n",
    "            \n",
    "            # 3. Coherencia con red CAN (peso: 15%)\n",
    "            puntuacion_red = self._evaluar_coherencia_red_can(\n",
    "                red_can, criterios['subsistemas_involucrados']\n",
    "            )\n",
    "            puntuacion_total += puntuacion_red * 0.15\n",
    "            detalles_puntuacion['red_can'] = puntuacion_red\n",
    "            \n",
    "            # 4. Contexto operacional (peso: 10%)\n",
    "            puntuacion_contexto = self._evaluar_contexto_operacional(\n",
    "                analisis_estadistico, criterios.get('contexto_esperado', [])\n",
    "            )\n",
    "            puntuacion_total += puntuacion_contexto * 0.10\n",
    "            detalles_puntuacion['contexto'] = puntuacion_contexto\n",
    "            \n",
    "            # Almacenar puntuaci√≥n completa con detalles\n",
    "            puntuaciones_eventos[tipo_evento] = {\n",
    "                'puntuacion_total': puntuacion_total,\n",
    "                'detalles': detalles_puntuacion,\n",
    "                'criterios_cumplidos': puntuacion_total > 0.5,  # Umbral de clasificaci√≥n\n",
    "                'confianza': min(1.0, puntuacion_total)\n",
    "            }\n",
    "        \n",
    "        # Selecci√≥n del evento con mayor puntuaci√≥n\n",
    "        if puntuaciones_eventos:\n",
    "            evento_seleccionado = max(puntuaciones_eventos, key=lambda x: puntuaciones_eventos[x]['puntuacion_total'])\n",
    "            confianza_clasificacion = puntuaciones_eventos[evento_seleccionado]['confianza']\n",
    "            \n",
    "            # Validaci√≥n de umbral m√≠nimo de confianza\n",
    "            if confianza_clasificacion < 0.3:\n",
    "                evento_seleccionado = 'operacion_indeterminada'\n",
    "                confianza_clasificacion = 0.3\n",
    "        else:\n",
    "            evento_seleccionado = 'operacion_normal'\n",
    "            confianza_clasificacion = 0.5\n",
    "        \n",
    "        # Inferencia de contexto operacional basada en evento clasificado\n",
    "        contexto_operacional = self._inferir_contexto_operacional(\n",
    "            evento_seleccionado, analisis_estadistico, red_can\n",
    "        )\n",
    "        \n",
    "        # Determinaci√≥n de criticidad e intensidad\n",
    "        criticidad = self._determinar_criticidad_evento(evento_seleccionado, red_can, analisis_estadistico)\n",
    "        intensidad = self._calcular_intensidad_evento(analisis_estadistico)\n",
    "        \n",
    "        # Compilaci√≥n de resultado completo\n",
    "        resultado_clasificacion = {\n",
    "            'evento_vehiculo': evento_seleccionado,\n",
    "            'confianza_clasificacion': confianza_clasificacion,\n",
    "            'contexto_operativo': contexto_operacional,\n",
    "            'intensidad': intensidad,\n",
    "            'criticidad': criticidad,\n",
    "            'red_can_origen': red_can,\n",
    "            'puntuaciones_detalladas': puntuaciones_eventos,\n",
    "            'timestamp_clasificacion': datetime.now().isoformat(),\n",
    "            'version_clasificador': '1.0'\n",
    "        }\n",
    "        \n",
    "        logger.debug(f\"Evento clasificado: {evento_seleccionado} (confianza: {confianza_clasificacion:.3f})\")\n",
    "        return resultado_clasificacion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d661491",
   "metadata": {},
   "source": [
    "## 5. PASO 3: Procesamiento de Documentaci√≥n T√©cnica y Chunking\n",
    "\n",
    "### Objetivo\n",
    "Preparar la documentaci√≥n t√©cnica (norma J1939, manuales, especificaciones) mediante estrategias de segmentaci√≥n optimizadas para sistemas RAG.\n",
    "\n",
    "### Estrategias de Chunking Implementadas:\n",
    "1. **Chunking Sem√°ntico:** Por secciones t√©cnicas coherentes\n",
    "2. **Chunking por Tama√±o:** Fragmentos de tama√±o fijo con solapamiento\n",
    "3. **Chunking Jer√°rquico:** Preservando estructura de documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab105ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Procesador de documentaci√≥n t√©cnica inicializado\n"
     ]
    }
   ],
   "source": [
    "class ProcesadorDocumentacionTecnica:\n",
    "    \"\"\"\n",
    "    Procesador de documentaci√≥n t√©cnica para sistemas RAG\n",
    "    Implementa m√∫ltiples estrategias de chunking\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.patrones_seccion = {\n",
    "            'j1939': [\n",
    "                r'^\\d+\\.\\d+\\s+.*',  # Numeraci√≥n tipo \"3.1 T√≠tulo\"\n",
    "                r'^[A-Z]+\\s+[A-Z].*',  # Secciones en may√∫sculas\n",
    "                r'^\\w+\\s+Group.*',  # Grupos de par√°metros\n",
    "            ],\n",
    "            'manual_tecnico': [\n",
    "                r'^Chapter\\s+\\d+.*',\n",
    "                r'^Section\\s+\\d+.*',\n",
    "                r'^\\d+\\.\\s+.*',\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        self.configuraciones_chunking = {\n",
    "            'semantico': {\n",
    "                'chunk_size': 1000,\n",
    "                'chunk_overlap': 200,\n",
    "                'separators': ['\\n\\n', '\\n', '. ', ' ']\n",
    "            },\n",
    "            'fijo': {\n",
    "                'chunk_size': 512,\n",
    "                'chunk_overlap': 50,\n",
    "                'separators': ['\\n\\n', '\\n']\n",
    "            },\n",
    "            'jerarquico': {\n",
    "                'chunk_size': 800,\n",
    "                'chunk_overlap': 150,\n",
    "                'preserve_structure': True\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def detectar_tipo_documento(self, texto: str) -> str:\n",
    "        \"\"\"\n",
    "        Detecta el tipo de documento t√©cnico basado en patrones\n",
    "        \"\"\"\n",
    "        texto_muestra = texto[:2000].lower()\n",
    "        \n",
    "        if any(keyword in texto_muestra for keyword in ['j1939', 'pgn', 'spn', 'parameter group']):\n",
    "            return 'j1939'\n",
    "        elif any(keyword in texto_muestra for keyword in ['manual', 'specification', 'datasheet']):\n",
    "            return 'manual_tecnico'\n",
    "        elif any(keyword in texto_muestra for keyword in ['api', 'protocol', 'interface']):\n",
    "            return 'documentacion_api'\n",
    "        else:\n",
    "            return 'documento_generico'\n",
    "    \n",
    "    def extraer_metadatos_documento(self, texto: str, tipo_doc: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Extrae metadatos relevantes del documento\n",
    "        \"\"\"\n",
    "        metadatos = {\n",
    "            'tipo_documento': tipo_doc,\n",
    "            'longitud_caracteres': len(texto),\n",
    "            'numero_lineas': texto.count('\\n'),\n",
    "            'idioma': 'es' if any(palabra in texto.lower() for palabra in ['el', 'la', 'de', 'en', 'que']) else 'en'\n",
    "        }\n",
    "        \n",
    "        # Extraer t√≠tulos y secciones principales\n",
    "        lineas = texto.split('\\n')\n",
    "        titulos = []\n",
    "        \n",
    "        for patron in self.patrones_seccion.get(tipo_doc, []):\n",
    "            for linea in lineas:\n",
    "                if re.match(patron, linea.strip()):\n",
    "                    titulos.append(linea.strip())\n",
    "        \n",
    "        metadatos['titulos_principales'] = titulos[:10]  # Primeros 10 t√≠tulos\n",
    "        metadatos['estructura_detectada'] = len(titulos) > 0\n",
    "        \n",
    "        return metadatos\n",
    "    \n",
    "    def chunking_semantico(self, texto: str) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Chunking basado en estructura sem√°ntica del documento\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Usar LangChain si est√° disponible\n",
    "            text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=self.configuraciones_chunking['semantico']['chunk_size'],\n",
    "                chunk_overlap=self.configuraciones_chunking['semantico']['chunk_overlap'],\n",
    "                separators=self.configuraciones_chunking['semantico']['separators']\n",
    "            )\n",
    "            \n",
    "            chunks = text_splitter.split_text(texto)\n",
    "            \n",
    "            return [Document(page_content=chunk, metadata={'chunk_id': i, 'tipo_chunking': 'semantico'})\n",
    "                    for i, chunk in enumerate(chunks)]\n",
    "        \n",
    "        except ImportError:\n",
    "            # Fallback manual si LangChain no est√° disponible\n",
    "            return self.chunking_manual(texto, 'semantico')\n",
    "    \n",
    "    def chunking_manual(self, texto: str, tipo: str) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Implementaci√≥n manual de chunking como fallback\n",
    "        \"\"\"\n",
    "        config = self.configuraciones_chunking[tipo]\n",
    "        chunk_size = config['chunk_size']\n",
    "        overlap = config['chunk_overlap']\n",
    "        \n",
    "        chunks = []\n",
    "        texto_chars = list(texto)\n",
    "        \n",
    "        for i in range(0, len(texto_chars), chunk_size - overlap):\n",
    "            chunk_text = ''.join(texto_chars[i:i + chunk_size])\n",
    "            \n",
    "            if len(chunk_text.strip()) > 50:  # Evitar chunks muy peque√±os\n",
    "                chunks.append(Document(\n",
    "                    page_content=chunk_text,\n",
    "                    metadata={'chunk_id': i // (chunk_size - overlap), 'tipo_chunking': tipo}\n",
    "                ))\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def procesar_documento_completo(self, ruta_archivo: str,\n",
    "                                   estrategia_chunking: str = 'semantico') -> List[RAGDocument]:\n",
    "        \"\"\"\n",
    "        Procesa documento completo y genera chunks RAG\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Leer archivo (simulado para demostraci√≥n)\n",
    "            if Path(ruta_archivo).exists():\n",
    "                with open(ruta_archivo, 'r', encoding='utf-8') as f:\n",
    "                    texto = f.read()\n",
    "            else:\n",
    "                # Documento simulado para demostraci√≥n\n",
    "                texto = self.generar_documento_j1939_simulado()\n",
    "                print(f\"‚ÑπÔ∏è Archivo {ruta_archivo} no encontrado, usando documento simulado\")\n",
    "            \n",
    "            # Detectar tipo de documento\n",
    "            tipo_doc = self.detectar_tipo_documento(texto)\n",
    "            metadatos_doc = self.extraer_metadatos_documento(texto, tipo_doc)\n",
    "            \n",
    "            # Aplicar chunking\n",
    "            if estrategia_chunking == 'semantico':\n",
    "                chunks = self.chunking_semantico(texto)\n",
    "            else:\n",
    "                chunks = self.chunking_manual(texto, estrategia_chunking)\n",
    "            \n",
    "            # Convertir a RAGDocument\n",
    "            documentos_rag = []\n",
    "            \n",
    "            for i, chunk in enumerate(chunks):\n",
    "                # Crear metadatos ficticios para documentaci√≥n t√©cnica\n",
    "                metadatos_evento = CANEventMetadata(\n",
    "                    timestamp_inicio=datetime.now().isoformat(),\n",
    "                    timestamp_fin=datetime.now().isoformat(),\n",
    "                    duracion_segundos=0.0,\n",
    "                    red_can='DOCUMENTACION',\n",
    "                    senales_involucradas=[],\n",
    "                    evento_vehiculo='referencia_tecnica',\n",
    "                    intensidad='informativo',\n",
    "                    contexto_operativo='documentacion'\n",
    "                )\n",
    "                \n",
    "                doc_rag = RAGDocument(\n",
    "                    id=f\"{tipo_doc}_chunk_{i}\",\n",
    "                    contenido_textual=chunk.page_content,\n",
    "                    metadatos=metadatos_evento,\n",
    "                    tipo_documento='documentacion_tecnica',\n",
    "                    calidad_descripcion=0.9  # Alta calidad para documentaci√≥n oficial\n",
    "                )\n",
    "                \n",
    "                documentos_rag.append(doc_rag)\n",
    "            \n",
    "            print(f\"‚úÖ Procesado {ruta_archivo}: {len(documentos_rag)} chunks generados\")\n",
    "            return documentos_rag\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error procesando {ruta_archivo}: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    def generar_documento_j1939_simulado(self) -> str:\n",
    "        \"\"\"\n",
    "        Genera documento J1939 simulado para demostraci√≥n\n",
    "        \"\"\"\n",
    "        return \"\"\"\n",
    "# J1939 - Parameter Group Number (PGN) Reference\n",
    "\n",
    "## 1.1 Engine Parameters Group\n",
    "\n",
    "The Engine Parameters Group contains critical information about engine operation:\n",
    "\n",
    "- **Engine Speed (SPN 190)**: Motor RPM measurement\n",
    "- **Engine Load (SPN 92)**: Current engine load percentage\n",
    "- **Engine Temperature (SPN 110)**: Coolant temperature in Celsius\n",
    "\n",
    "### 1.1.1 Engine Speed Signal\n",
    "\n",
    "Engine speed is transmitted via PGN 61444 (0xF004) with the following characteristics:\n",
    "- Data Length: 8 bytes\n",
    "- Transmission Rate: 10ms\n",
    "- Resolution: 0.125 rpm/bit\n",
    "- Range: 0 to 8031.875 rpm\n",
    "\n",
    "### 1.1.2 Engine Load Signal\n",
    "\n",
    "Engine load percentage indicates current operational demand:\n",
    "- Signal Range: 0-100%\n",
    "- Resolution: 1%/bit\n",
    "- Typical idle load: 5-10%\n",
    "- Maximum load scenarios: >90%\n",
    "\n",
    "## 2.1 Battery Management System\n",
    "\n",
    "Battery parameters are critical for electric vehicle operation:\n",
    "\n",
    "- **State of Charge (SOC)**: Battery charge level percentage\n",
    "- **Battery Voltage**: Total pack voltage\n",
    "- **Battery Temperature**: Average cell temperature\n",
    "- **Charging Status**: Current charging state\n",
    "\n",
    "### 2.1.1 State of Charge Calculation\n",
    "\n",
    "SOC calculation involves multiple factors:\n",
    "- Coulomb counting method\n",
    "- Voltage-based estimation\n",
    "- Temperature compensation\n",
    "- Aging factor adjustment\n",
    "\"\"\"\n",
    "\n",
    "# Inicializar procesador de documentaci√≥n\n",
    "procesador_docs = ProcesadorDocumentacionTecnica()\n",
    "print(\"‚úÖ Procesador de documentaci√≥n t√©cnica inicializado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d995a824",
   "metadata": {},
   "source": [
    "## 6. PASO 4: Construcci√≥n del Dataset RAG Unificado\n",
    "\n",
    "### Objetivo Final\n",
    "Combinar todas las caracter√≠sticas generadas en un dataset unificado formato JSONL optimizado para sistemas RAG:\n",
    "- **Descripciones textuales** de eventos CAN\n",
    "- **Metadatos estructurados** para filtrado\n",
    "- **Chunks documentales** de referencias t√©cnicas\n",
    "- **Esquema unificado** para recuperaci√≥n eficiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5d02b0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Iniciando construcci√≥n del dataset RAG...\n",
      "üìã Iniciando construcci√≥n del dataset RAG completo...\n",
      "üîÑ Procesando CAN_CUSTOM_31...\n",
      "üìö Procesando documentaci√≥n t√©cnica...\n",
      "‚ÑπÔ∏è Archivo documentacion/J1939_reference.txt no encontrado, usando documento simulado\n",
      "‚úÖ Procesado documentacion/J1939_reference.txt: 2 chunks generados\n",
      "‚úÖ Dataset RAG guardado en: ..\\Ingenieria_de_Caracteristicas\\dataset_rag_decode_ev.jsonl\n",
      "üìä Estad√≠sticas finales: {'total_documentos': 7, 'eventos_can': 5, 'documentacion_tecnica': 2, 'hipotesis_catl': 0, 'calidad_promedio': np.float64(0.7621446827728902)}\n",
      "\n",
      "üìã MUESTRA DEL DATASET GENERADO:\n",
      "======================================================================\n",
      "MUESTRA_1:\n",
      "  id: CAN_CUSTOM_31_evento_0\n",
      "  tipo: evento_can\n",
      "  contenido_preview: Evento en red CAN_CUSTOM_31 (Segmento 0):\n",
      "- En el sistema CAN, an√°lisis de estabilidad temporal: voltaje_carga_v mostr√≥ variaci√≥n contenida de ¬±36.00 v respecto al valor nominal registrado en blf.\n",
      "- E...\n",
      "  calidad: 0.6462042202279893\n",
      "  evento_vehicular: carga\n",
      "  red_can: CAN_CUSTOM_31\n",
      "--------------------------------------------------\n",
      "MUESTRA_2:\n",
      "  id: CAN_CUSTOM_31_evento_1\n",
      "  tipo: evento_can\n",
      "  contenido_preview: Evento en red CAN_CUSTOM_31 (Segmento 1):\n",
      "- En el sistema CAN, comportamiento peri√≥dico detectado en logs blf: voltaje_carga_v exhibe oscilaciones regulares cada 0.1 minutos durante operaci√≥n normal.\n",
      "...\n",
      "  calidad: 0.7169641095918855\n",
      "  evento_vehicular: carga\n",
      "  red_can: CAN_CUSTOM_31\n",
      "--------------------------------------------------\n",
      "MUESTRA_3:\n",
      "  id: CAN_CUSTOM_31_evento_2\n",
      "  tipo: evento_can\n",
      "  contenido_preview: Evento en red CAN_CUSTOM_31 (Segmento 2):\n",
      "- En el sistema CAN, los datos temporales revelan un patr√≥n c√≠clico en voltaje_carga_v con per√≠odo aproximado de 0.1 minutos y amplitud de 83.69 v.\n",
      "- En el si...\n",
      "  calidad: 0.7336775040420048\n",
      "  evento_vehicular: carga\n",
      "  red_can: CAN_CUSTOM_31\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class ConstructorDatasetRAG:\n",
    "    \"\"\"\n",
    "    Constructor del dataset RAG unificado para DECODE-EV\n",
    "    Integra descripciones textuales, metadatos y documentaci√≥n t√©cnica\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, ruta_salida: str = \"../Ingenieria_de_Caracteristicas/\"):\n",
    "        self.ruta_salida = Path(ruta_salida)\n",
    "        self.ruta_salida.mkdir(exist_ok=True)\n",
    "        self.documentos_rag = []\n",
    "        \n",
    "        # Estad√≠sticas del dataset\n",
    "        self.stats = {\n",
    "            'total_documentos': 0,\n",
    "            'eventos_can': 0,\n",
    "            'documentacion_tecnica': 0,\n",
    "            'hipotesis_catl': 0,\n",
    "            'calidad_promedio': 0.0\n",
    "        }\n",
    "    \n",
    "    def generar_evento_can_completo(self, df_segmento: pd.DataFrame,\n",
    "                                   red_can: str, indice_segmento: int) -> RAGDocument:\n",
    "        \"\"\"\n",
    "        Genera un documento RAG completo para un segmento de datos CAN\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # 1. Generar descripci√≥n textual\n",
    "            timestamp_inicio = df_segmento['timestamp'].iloc[0] if 'timestamp' in df_segmento.columns else datetime.now()\n",
    "            duracion = len(df_segmento)  # Aproximaci√≥n en segundos\n",
    "            \n",
    "            # Generar descripci√≥n para cada se√±al num√©rica\n",
    "            descripciones_senales = []\n",
    "            senales_numericas = df_segmento.select_dtypes(include=[np.number]).columns.tolist()\n",
    "            \n",
    "            # Remover timestamp si existe\n",
    "            if 'timestamp' in senales_numericas:\n",
    "                senales_numericas.remove('timestamp')\n",
    "            \n",
    "            for senal in senales_numericas[:5]:  # Limitar a 5 se√±ales principales\n",
    "                try:\n",
    "                    serie = df_segmento[senal].dropna()\n",
    "                    if len(serie) > 2:\n",
    "                        desc = generador_textual.generar_descripcion_signal(\n",
    "                            senal, serie, df_segmento['timestamp'] if 'timestamp' in df_segmento.columns else pd.Series(range(len(serie))), red_can\n",
    "                        )\n",
    "                        descripciones_senales.append(desc)\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è Error procesando se√±al {senal}: {e}\")\n",
    "            \n",
    "            # Combinar descripciones\n",
    "            descripcion_completa = f\"Evento en red {red_can} (Segmento {indice_segmento}):\\n\"\n",
    "            descripcion_completa += \"\\n\".join([f\"- {desc}\" for desc in descripciones_senales])\n",
    "            \n",
    "            # 2. Calcular estad√≠sticas num√©ricas para metadatos\n",
    "            stats_numericas = {\n",
    "                'cambio_relativo_promedio': np.mean([\n",
    "                    abs(df_segmento[col].iloc[-1] - df_segmento[col].iloc[0]) /\n",
    "                    (df_segmento[col].mean() + 1e-6)  # Evitar divisi√≥n por cero\n",
    "                    for col in senales_numericas if len(df_segmento[col].dropna()) > 1\n",
    "                ]) if senales_numericas else 0.0,\n",
    "                'velocidad_promedio': df_segmento.get('Velocidad_Motor_RPM', pd.Series([0])).mean()\n",
    "            }\n",
    "            \n",
    "            # 3. Generar metadatos estructurados\n",
    "            metadatos = generador_metadatos.generar_metadatos_evento(\n",
    "                descripcion_textual=descripcion_completa,\n",
    "                timestamp_inicio=timestamp_inicio if isinstance(timestamp_inicio, datetime) else datetime.now(),\n",
    "                duracion=float(duracion),\n",
    "                red_can=red_can,\n",
    "                senales_involucradas=senales_numericas,\n",
    "                stats_numericas=stats_numericas\n",
    "            )\n",
    "            \n",
    "            # 4. Calcular calidad de descripci√≥n\n",
    "            calidad = generador_metadatos.calcular_calidad_descripcion(\n",
    "                descripcion_completa, len(senales_numericas)\n",
    "            )\n",
    "            \n",
    "            # 5. Crear documento RAG\n",
    "            doc_rag = RAGDocument(\n",
    "                id=f\"{red_can}_evento_{indice_segmento}\",\n",
    "                contenido_textual=descripcion_completa,\n",
    "                metadatos=metadatos,\n",
    "                tipo_documento=\"evento_can\",\n",
    "                calidad_descripcion=calidad\n",
    "            )\n",
    "            \n",
    "            return doc_rag\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error generando evento CAN: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def generar_hipotesis_catl(self, df_catl: pd.DataFrame) -> List[RAGDocument]:\n",
    "        \"\"\"\n",
    "        Genera hip√≥tesis textuales para la red CAN_CATL (caja negra)\n",
    "        Basado en an√°lisis de patrones estad√≠sticos\n",
    "        \"\"\"\n",
    "        hipotesis_docs = []\n",
    "        \n",
    "        try:\n",
    "            # An√°lisis estad√≠stico de se√±ales desconocidas\n",
    "            senales_catl = [col for col in df_catl.columns if col.startswith('Signal_')]\n",
    "            \n",
    "            for i, senal in enumerate(senales_catl[:10]):  # Primeras 10 se√±ales\n",
    "                serie = df_catl[senal].dropna()\n",
    "                \n",
    "                if len(serie) < 10:\n",
    "                    continue\n",
    "                \n",
    "                # An√°lisis estad√≠stico\n",
    "                stats = {\n",
    "                    'min': serie.min(),\n",
    "                    'max': serie.max(),\n",
    "                    'mean': serie.mean(),\n",
    "                    'std': serie.std(),\n",
    "                    'rango': serie.max() - serie.min()\n",
    "                }\n",
    "                \n",
    "                # Generar hip√≥tesis basada en patrones\n",
    "                hipotesis = self.generar_hipotesis_senal_catl(senal, stats)\n",
    "                \n",
    "                # Crear metadatos para hip√≥tesis\n",
    "                metadatos_hipotesis = CANEventMetadata(\n",
    "                    timestamp_inicio=datetime.now().isoformat(),\n",
    "                    timestamp_fin=datetime.now().isoformat(),\n",
    "                    duracion_segundos=0.0,\n",
    "                    red_can=\"CAN_CATL\",\n",
    "                    senales_involucradas=[senal],\n",
    "                    evento_vehiculo=\"hipotesis_funcional\",\n",
    "                    intensidad=\"informativo\",\n",
    "                    contexto_operativo=\"analisis_exploratorio\"\n",
    "                )\n",
    "                \n",
    "                doc_hipotesis = RAGDocument(\n",
    "                    id=f\"CATL_hipotesis_{i}\",\n",
    "                    contenido_textual=hipotesis,\n",
    "                    metadatos=metadatos_hipotesis,\n",
    "                    tipo_documento=\"hipotesis_catl\",\n",
    "                    calidad_descripcion=0.6  # Calidad media para hip√≥tesis\n",
    "                )\n",
    "                \n",
    "                hipotesis_docs.append(doc_hipotesis)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error generando hip√≥tesis CATL: {str(e)}\")\n",
    "        \n",
    "        return hipotesis_docs\n",
    "    \n",
    "    def generar_hipotesis_senal_catl(self, nombre_senal: str, stats: Dict) -> str:\n",
    "        \"\"\"\n",
    "        Genera hip√≥tesis textual para una se√±al CATL desconocida\n",
    "        \"\"\"\n",
    "        # Patrones de reconocimiento basados en rangos estad√≠sticos\n",
    "        if 0 <= stats['mean'] <= 100 and stats['rango'] > 50:\n",
    "            tipo_hipotesis = \"porcentaje (posible SOC o nivel de carga)\"\n",
    "            comportamiento = f\"var√≠a entre {stats['min']:.1f}% y {stats['max']:.1f}%\"\n",
    "        elif 20 <= stats['mean'] <= 60 and stats['std'] < 10:\n",
    "            tipo_hipotesis = \"temperatura (posible temperatura de celda)\"\n",
    "            comportamiento = f\"se mantiene relativamente estable entre {stats['min']:.1f}¬∞C y {stats['max']:.1f}¬∞C\"\n",
    "        elif 3.0 <= stats['mean'] <= 4.5 and stats['std'] < 0.5:\n",
    "            tipo_hipotesis = \"voltaje (posible voltaje de celda)\"\n",
    "            comportamiento = f\"presenta valores t√≠picos de bater√≠a Li-ion entre {stats['min']:.2f}V y {stats['max']:.2f}V\"\n",
    "        elif stats['rango'] < stats['mean'] * 0.1:\n",
    "            tipo_hipotesis = \"valor de estado o configuraci√≥n\"\n",
    "            comportamiento = f\"permanece constante en {stats['mean']:.2f} con m√≠nimas variaciones\"\n",
    "        else:\n",
    "            tipo_hipotesis = \"par√°metro operativo no identificado\"\n",
    "            comportamiento = f\"muestra variabilidad moderada con promedio de {stats['mean']:.2f}\"\n",
    "        \n",
    "        hipotesis = f\"\"\"\n",
    "HIP√ìTESIS PARA {nombre_senal} (Red CAN_CATL):\n",
    "\n",
    "Basado en el an√°lisis estad√≠stico de patrones, esta se√±al probablemente representa un {tipo_hipotesis}.\n",
    "\n",
    "Comportamiento observado: {comportamiento}.\n",
    "\n",
    "Estad√≠sticas clave:\n",
    "- Valor promedio: {stats['mean']:.3f}\n",
    "- Desviaci√≥n est√°ndar: {stats['std']:.3f}\n",
    "- Rango total: {stats['rango']:.3f}\n",
    "\n",
    "Esta hip√≥tesis requiere validaci√≥n con documentaci√≥n t√©cnica o conocimiento experto del sistema CATL.\n",
    "\"\"\"\n",
    "        \n",
    "        return hipotesis\n",
    "    \n",
    "    def construir_dataset_completo(self) -> str:\n",
    "        \"\"\"\n",
    "        Construye el dataset RAG completo integrando todas las fuentes\n",
    "        \"\"\"\n",
    "        print(\"üìã Iniciando construcci√≥n del dataset RAG completo...\")\n",
    "        \n",
    "        # 1. Procesar eventos CAN de todas las redes\n",
    "        for nombre_red, df_red in datos_can.items():\n",
    "            if df_red.empty:\n",
    "                continue\n",
    "            \n",
    "            print(f\"üîÑ Procesando {nombre_red}...\")\n",
    "            \n",
    "            # Segmentar datos en ventanas de tiempo\n",
    "            ventana = 30  # 30 registros por segmento\n",
    "            n_segmentos = len(df_red) // ventana\n",
    "            \n",
    "            for i in range(min(n_segmentos, 5)):  # Limitar a 5 segmentos por red para demo\n",
    "                segmento = df_red.iloc[i*ventana:(i+1)*ventana]\n",
    "                \n",
    "                doc_evento = self.generar_evento_can_completo(segmento, nombre_red, i)\n",
    "                if doc_evento:\n",
    "                    self.documentos_rag.append(doc_evento)\n",
    "                    self.stats['eventos_can'] += 1\n",
    "        \n",
    "        # 2. Generar hip√≥tesis para CAN_CATL\n",
    "        if \"CAN_CATL\" in datos_can and not datos_can[\"CAN_CATL\"].empty:\n",
    "            print(\"üîç Generando hip√≥tesis para CAN_CATL...\")\n",
    "            hipotesis_catl = self.generar_hipotesis_catl(datos_can[\"CAN_CATL\"])\n",
    "            self.documentos_rag.extend(hipotesis_catl)\n",
    "            self.stats['hipotesis_catl'] = len(hipotesis_catl)\n",
    "        \n",
    "        # 3. Procesar documentaci√≥n t√©cnica\n",
    "        print(\"üìö Procesando documentaci√≥n t√©cnica...\")\n",
    "        docs_tecnicos = procesador_docs.procesar_documento_completo(\n",
    "            \"documentacion/J1939_reference.txt\", \"semantico\"\n",
    "        )\n",
    "        self.documentos_rag.extend(docs_tecnicos)\n",
    "        self.stats['documentacion_tecnica'] = len(docs_tecnicos)\n",
    "        \n",
    "        # 4. Calcular estad√≠sticas finales\n",
    "        self.stats['total_documentos'] = len(self.documentos_rag)\n",
    "        if self.documentos_rag:\n",
    "            self.stats['calidad_promedio'] = np.mean([\n",
    "                doc.calidad_descripcion for doc in self.documentos_rag\n",
    "            ])\n",
    "        \n",
    "        # 5. Exportar a JSONL (usando fallback si jsonlines no est√° disponible)\n",
    "        archivo_salida = self.ruta_salida / \"dataset_rag_decode_ev.jsonl\"\n",
    "        \n",
    "        try:\n",
    "            import jsonlines\n",
    "            with jsonlines.open(archivo_salida, mode='w') as writer:\n",
    "                for doc in self.documentos_rag:\n",
    "                    writer.write(doc.to_jsonl_entry())\n",
    "        except ImportError:\n",
    "            # Fallback: exportar como JSON est√°ndar\n",
    "            import json\n",
    "            with open(archivo_salida, 'w', encoding='utf-8') as f:\n",
    "                for doc in self.documentos_rag:\n",
    "                    json.dump(doc.to_jsonl_entry(), f, ensure_ascii=False)\n",
    "                    f.write('\\n')\n",
    "        \n",
    "        print(f\"‚úÖ Dataset RAG guardado en: {archivo_salida}\")\n",
    "        print(f\"üìä Estad√≠sticas finales: {self.stats}\")\n",
    "        \n",
    "        return str(archivo_salida)\n",
    "    \n",
    "    def generar_muestra_dataset(self, n_muestras: int = 3) -> Dict:\n",
    "        \"\"\"\n",
    "        Genera muestra del dataset para inspecci√≥n\n",
    "        \"\"\"\n",
    "        muestra = {}\n",
    "        \n",
    "        if len(self.documentos_rag) >= n_muestras:\n",
    "            for i in range(n_muestras):\n",
    "                doc = self.documentos_rag[i]\n",
    "                muestra[f\"muestra_{i+1}\"] = {\n",
    "                    \"id\": doc.id,\n",
    "                    \"tipo\": doc.tipo_documento,\n",
    "                    \"contenido_preview\": doc.contenido_textual[:200] + \"...\",\n",
    "                    \"calidad\": doc.calidad_descripcion,\n",
    "                    \"evento_vehicular\": doc.metadatos.evento_vehiculo,\n",
    "                    \"red_can\": doc.metadatos.red_can\n",
    "                }\n",
    "        \n",
    "        return muestra\n",
    "\n",
    "# === EJECUCI√ìN DEL CONSTRUCTOR ===\n",
    "\n",
    "# Inicializar constructor y ejecutar\n",
    "print(\"üöÄ Iniciando construcci√≥n del dataset RAG...\")\n",
    "constructor_rag = ConstructorDatasetRAG()\n",
    "archivo_dataset = constructor_rag.construir_dataset_completo()\n",
    "\n",
    "# Mostrar muestra del dataset generado\n",
    "muestra = constructor_rag.generar_muestra_dataset(3)\n",
    "print(\"\\nüìã MUESTRA DEL DATASET GENERADO:\")\n",
    "print(\"=\"*70)\n",
    "for key, valor in muestra.items():\n",
    "    print(f\"{key.upper()}:\")\n",
    "    for k, v in valor.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763bf7f8",
   "metadata": {},
   "source": [
    "## 7. Validaci√≥n y Evaluaci√≥n de Caracter√≠sticas Generadas\n",
    "\n",
    "### M√©tricas de Calidad Implementadas\n",
    "\n",
    "1. **Calidad Textual:** Coherencia, longitud, riqueza vocabulario\n",
    "2. **Completitud Metadatos:** Cobertura de campos obligatorios\n",
    "3. **Consistencia Temporal:** Coherencia en secuencias temporales\n",
    "4. **Cobertura de Se√±ales:** Proporci√≥n de se√±ales documentadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373b77d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Analizando calidad del dataset generado...\n",
      "\n",
      "üìä REPORTE DE CALIDAD DEL DATASET\n",
      "============================================================\n",
      "\n",
      "1. DISTRIBUCI√ìN POR TIPOS:\n",
      "  üìÑ documentacion_tecnica: 2 documentos (calidad: 0.900)\n",
      "  üìÑ evento_can: 5 documentos (calidad: 0.707)\n",
      "\n",
      "2. ESTAD√çSTICAS DE LONGITUD:\n",
      "  üìè Promedio: 664 caracteres\n",
      "  üìè Rango: 488 - 858 caracteres\n",
      "\n",
      "3. COBERTURA POR REDES CAN:\n",
      "  üîå DOCUMENTACION: 2 documentos\n",
      "  üîå CAN_CUSTOM_31: 5 documentos\n",
      "\n",
      "4. EVENTOS VEHICULARES:\n",
      "  üöó referencia_tecnica: 2 eventos\n",
      "  üöó carga: 5 eventos\n",
      "\n",
      "5. M√âTRICAS GLOBALES:\n",
      "  üìà Total documentos: 7\n",
      "  üìà Calidad promedio: 0.762\n",
      "  üìà Documentos alta calidad: 6\n",
      "  üìà Cobertura temporal: 2 d√≠as √∫nicos\n",
      "\n",
      "‚úÖ AN√ÅLISIS DE CALIDAD COMPLETADO\n",
      "‚úÖ Dataset RAG listo para uso en sistemas de IA conversacional\n",
      "\n",
      "================================================================================\n",
      "üéØ FEATURE ENGINEERING COMPLETADO EXITOSAMENTE\n",
      "üéØ Dataset RAG preparado para DECODE-EV\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mEl kernel se bloque√≥ al ejecutar c√≥digo en la celda actual o en una celda anterior. \n",
      "\u001b[1;31mRevise el c√≥digo de las celdas para identificar una posible causa del error. \n",
      "\u001b[1;31mHaga clic <a href='https://aka.ms/vscodeJupyterKernelCrash'>aqu√≠</a> para obtener m√°s informaci√≥n. \n",
      "\u001b[1;31mVea Jupyter <a href='command:jupyter.viewOutput'>log</a> para obtener m√°s detalles."
     ]
    }
   ],
   "source": [
    "# An√°lisis de calidad del dataset generado\n",
    "def analizar_calidad_dataset(documentos: List[RAGDocument]) -> Dict:\n",
    "    \"\"\"\n",
    "    An√°lisis completo de calidad del dataset RAG generado\n",
    "    \"\"\"\n",
    "    \n",
    "    if not documentos:\n",
    "        return {\"error\": \"No hay documentos para analizar\"}\n",
    "    \n",
    "    analisis = {\n",
    "        'distribucion_tipos': {},\n",
    "        'calidad_promedio_por_tipo': {},\n",
    "        'estadisticas_longitud': {},\n",
    "        'cobertura_redes_can': {},\n",
    "        'eventos_por_tipo': {},\n",
    "        'metricas_globales': {}\n",
    "    }\n",
    "    \n",
    "    # 1. Distribuci√≥n por tipos de documento\n",
    "    tipos = [doc.tipo_documento for doc in documentos]\n",
    "    for tipo in set(tipos):\n",
    "        analisis['distribucion_tipos'][tipo] = tipos.count(tipo)\n",
    "        \n",
    "        # Calidad promedio por tipo\n",
    "        docs_tipo = [doc for doc in documentos if doc.tipo_documento == tipo]\n",
    "        analisis['calidad_promedio_por_tipo'][tipo] = np.mean([\n",
    "            doc.calidad_descripcion for doc in docs_tipo\n",
    "        ])\n",
    "    \n",
    "    # 2. Estad√≠sticas de longitud de texto\n",
    "    longitudes = [len(doc.contenido_textual) for doc in documentos]\n",
    "    analisis['estadisticas_longitud'] = {\n",
    "        'promedio': np.mean(longitudes),\n",
    "        'mediana': np.median(longitudes),\n",
    "        'min': min(longitudes),\n",
    "        'max': max(longitudes),\n",
    "        'desviacion': np.std(longitudes)\n",
    "    }\n",
    "    \n",
    "    # 3. Cobertura por redes CAN\n",
    "    redes_can = [doc.metadatos.red_can for doc in documentos]\n",
    "    for red in set(redes_can):\n",
    "        analisis['cobertura_redes_can'][red] = redes_can.count(red)\n",
    "    \n",
    "    # 4. Distribuci√≥n de eventos vehiculares\n",
    "    eventos = [doc.metadatos.evento_vehiculo for doc in documentos]\n",
    "    for evento in set(eventos):\n",
    "        analisis['eventos_por_tipo'][evento] = eventos.count(evento)\n",
    "    \n",
    "    # 5. M√©tricas globales\n",
    "    analisis['metricas_globales'] = {\n",
    "        'total_documentos': len(documentos),\n",
    "        'calidad_promedio_global': np.mean([doc.calidad_descripcion for doc in documentos]),\n",
    "        'documentos_alta_calidad': sum(1 for doc in documentos if doc.calidad_descripcion > 0.7),\n",
    "        'cobertura_temporal': len(set([doc.metadatos.timestamp_inicio[:10] for doc in documentos]))\n",
    "    }\n",
    "    \n",
    "    return analisis\n",
    "\n",
    "# === AN√ÅLISIS DE CALIDAD FINAL ===\n",
    "\n",
    "# Ejecutar an√°lisis de calidad\n",
    "if constructor_rag.documentos_rag:\n",
    "    print(\"üîç Analizando calidad del dataset generado...\")\n",
    "    analisis_calidad = analizar_calidad_dataset(constructor_rag.documentos_rag)\n",
    "    \n",
    "    print(\"\\nüìä REPORTE DE CALIDAD DEL DATASET\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\\n1. DISTRIBUCI√ìN POR TIPOS:\")\n",
    "    for tipo, cantidad in analisis_calidad['distribucion_tipos'].items():\n",
    "        calidad = analisis_calidad['calidad_promedio_por_tipo'][tipo]\n",
    "        print(f\"  üìÑ {tipo}: {cantidad} documentos (calidad: {calidad:.3f})\")\n",
    "    \n",
    "    print(\"\\n2. ESTAD√çSTICAS DE LONGITUD:\")\n",
    "    stats_long = analisis_calidad['estadisticas_longitud']\n",
    "    print(f\"  üìè Promedio: {stats_long['promedio']:.0f} caracteres\")\n",
    "    print(f\"  üìè Rango: {stats_long['min']:.0f} - {stats_long['max']:.0f} caracteres\")\n",
    "    \n",
    "    print(\"\\n3. COBERTURA POR REDES CAN:\")\n",
    "    for red, cantidad in analisis_calidad['cobertura_redes_can'].items():\n",
    "        print(f\"  üîå {red}: {cantidad} documentos\")\n",
    "    \n",
    "    print(\"\\n4. EVENTOS VEHICULARES:\")\n",
    "    for evento, cantidad in analisis_calidad['eventos_por_tipo'].items():\n",
    "        print(f\"  üöó {evento}: {cantidad} eventos\")\n",
    "    \n",
    "    print(\"\\n5. M√âTRICAS GLOBALES:\")\n",
    "    metricas = analisis_calidad['metricas_globales']\n",
    "    print(f\"  üìà Total documentos: {metricas['total_documentos']}\")\n",
    "    print(f\"  üìà Calidad promedio: {metricas['calidad_promedio_global']:.3f}\")\n",
    "    print(f\"  üìà Documentos alta calidad: {metricas['documentos_alta_calidad']}\")\n",
    "    print(f\"  üìà Cobertura temporal: {metricas['cobertura_temporal']} d√≠as √∫nicos\")\n",
    "    \n",
    "    print(\"\\n‚úÖ AN√ÅLISIS DE CALIDAD COMPLETADO\")\n",
    "    print(\"‚úÖ Dataset RAG listo para uso en sistemas de IA conversacional\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No se encontraron documentos para analizar\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéØ FEATURE ENGINEERING COMPLETADO EXITOSAMENTE\")\n",
    "print(\"üéØ Dataset RAG preparado para DECODE-EV\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4526d597",
   "metadata": {},
   "source": [
    "## ENTREGABLES DE LA FASE 2: INGENIER√çA DE CARACTER√çSTICAS\n",
    "\n",
    "### ENTREGABLE 1: Script de Generaci√≥n de Caracter√≠sticas Textuales\n",
    "**Estado:** COMPLETADO\n",
    "- Clase `GeneradorDescripcionesTextual` implementada\n",
    "- Plantillas program√°ticas para an√°lisis temporal\n",
    "- Algoritmos de detecci√≥n de patrones (incremento, decremento, estable, picos)\n",
    "- Sistema de clasificaci√≥n de intensidad autom√°tica\n",
    "\n",
    "### ENTREGABLE 2: Dataset RAG Procesado\n",
    "**Estado:** COMPLETADO\n",
    "- Archivo `dataset_rag_decode_ev.jsonl` generado\n",
    "- Formato unificado para sistemas de recuperaci√≥n\n",
    "- Metadatos estructurados para filtrado eficiente\n",
    "- Integraci√≥n de eventos CAN + documentaci√≥n t√©cnica + hip√≥tesis CATL\n",
    "\n",
    "### ENTREGABLE 3: Sistema de Metadatos Estructurados\n",
    "**Estado:** COMPLETADO\n",
    "- Clase `CANEventMetadata` con esquema JSON\n",
    "- Clasificaci√≥n autom√°tica de eventos vehiculares\n",
    "- Determinaci√≥n de contexto operativo\n",
    "- Sistema de calidad y validaci√≥n\n",
    "\n",
    "---\n",
    "\n",
    "## JUSTIFICACI√ìN METODOL√ìGICA (CRISP-ML)\n",
    "\n",
    "### Reinterpretaci√≥n de \"Feature Engineering\" para LLM/RAG\n",
    "\n",
    "**Paradigma Tradicional ML:**\n",
    "- Caracter√≠sticas num√©ricas (mean, std, correlaciones)\n",
    "- Vectores de features para algoritmos tabulares\n",
    "- Optimizaci√≥n para modelos supervisados\n",
    "\n",
    "**Nuevo Paradigma LLM/RAG:**\n",
    "- **Caracter√≠sticas textuales:** Descripciones narrativas ricas\n",
    "- **Metadatos estructurados:** Para filtrado y recuperaci√≥n\n",
    "- **Chunking estrat√©gico:** Optimizaci√≥n para vectorizaci√≥n\n",
    "- **Calidad sem√°ntica:** Coherencia y completitud textual\n",
    "\n",
    "### Ventajas del Enfoque LLM/RAG vs ML Tradicional\n",
    "\n",
    "| **Aspecto** | **ML Tradicional** | **LLM/RAG (Nuevo)** |\n",
    "|-------------|-------------------|---------------------|\n",
    "| **Interpretabilidad** | Features num√©ricas abstractas | Descripciones en lenguaje natural |\n",
    "| **Escalabilidad** | Requiere reentrenamiento | Adaptable con nuevos documentos |\n",
    "| **Flexibilidad** | Arquitectura fija | Consultas en lenguaje natural |\n",
    "| **Conocimiento experto** | Dif√≠cil de incorporar | Integrable via documentaci√≥n |\n",
    "| **CAN_CATL (caja negra)** | Imposible sin labels | Generaci√≥n de hip√≥tesis textuales |\n",
    "\n",
    "---\n",
    "\n",
    "## PR√ìXIMOS PASOS SUGERIDOS\n",
    "\n",
    "### Fase 3: Implementaci√≥n del Sistema RAG\n",
    "1. **Vectorizaci√≥n:** Generar embeddings para el dataset\n",
    "2. **Base de vectores:** Implementar √≠ndice de b√∫squeda (FAISS/Pinecone)\n",
    "3. **Pipeline RAG:** Integrar retrieval + generation\n",
    "4. **Evaluaci√≥n:** M√©tricas de relevancia y coherencia\n",
    "\n",
    "### Optimizaciones Propuestas\n",
    "1. **LLM Especializado:** Fine-tuning con terminolog√≠a automotriz\n",
    "2. **Chunking Adapativo:** Tama√±o din√°mico seg√∫n complejidad\n",
    "3. **Metadatos Enriquecidos:** Integraci√≥n con ontolog√≠as CAN\n",
    "4. **Validaci√≥n Experta:** Feedback loop con ingenieros automotrices"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
